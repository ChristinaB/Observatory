{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to retrieve gridded climate time-series data sets\n",
    "\n",
    "### Case study: the Sauk-Suiattle river watershed, the Elwha river watershed, the Upper Rio Salado watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. HydroShare setup and preparation\n",
    "    2. Re-establish the paths to the mapping file\n",
    "    3. Download climate data\n",
    "    4. Summarize the file availability from each watershed mapping file\n",
    "    5. Save results back into HydroShare\n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import ogh\n",
    "import geopandas as gpd\n",
    "\n",
    "# data migration library\n",
    "from utilities import hydroshare\n",
    "\n",
    "# plotting and shape libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)\n",
    "print('Data will be loaded from and save to:'+homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Re-establish the paths to the mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the mapping files generated for Sauk-Suiattle, Elwha, and Upper Rio Salado from usecase1\n",
    "mappingfile1 = os.path.join(homedir,'Sauk_mappingfile.csv')\n",
    "mappingfile2 = os.path.join(homedir,'Elwha_mappingfile.csv')\n",
    "mappingfile3 = os.path.join(homedir,'RioSalado_mappingfile.csv')\n",
    "\n",
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1, mappingfile2, mappingfile3], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river','Elwha river','Upper Rio Salado'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download climate data \n",
    "\n",
    "### Call each get Climate data function\n",
    "\n",
    "Each function reads in the mapping file table, generates the destination folder, downloads and unzips the files, then catalogs the downloaded files within the mapping file.\n",
    "\n",
    "Meteorology - MET; Weather Research and Forecasting (WRF); Variable Infiltration Capacity - VIC\n",
    "\n",
    "    1. getDailyMET_livneh2013\n",
    "    2. getDailyMET_bcLivneh2013\n",
    "    3. getDailyMET_livneh2015\n",
    "    4. getDailyVIC_livneh2013\n",
    "    5. getDailyVIC_livneh2015\n",
    "    6. getDailyWRF_salathe2014\n",
    "    7. getDailyWRF_bcsalathe2014\n",
    "\n",
    "### View data extent:\n",
    "\n",
    "    1. Continental United States (CONUS)\n",
    "    Livneh, B. (2017). Gridded climatology locations (1/16th degree): Continental United States extent, HydroShare, http://www.hydroshare.org/resource/14f0a6619c6b45cc90d1f8cabc4129af\n",
    "\n",
    "    2. North America (NAmer)\n",
    "    Livneh, B. (2017). Gridded climatology locations (1/16th degree): North American extent, HydroShare, http://www.hydroshare.org/resource/ef2d82bf960144b4bfb1bae6242bcc7f\n",
    "\n",
    "    3. Pacific Northwest - Columbia River Basin\n",
    "    Bandaragoda, C. (2017). Sauk Suiattle HydroMeteorology (WRF), HydroShare, http://www.hydroshare.org/resource/0db969e4cfb54cb18b4e1a2014a26c82\n",
    "\n",
    "\n",
    "\n",
    "### Please cite:\n",
    "\n",
    "    1. Livneh B., E.A. Rosenberg, C. Lin, B. Nijssen, V. Mishra, K.M. Andreadis, E.P. Maurer, and D.P. Lettenmaier, 2013: A Long-Term Hydrologically Based Dataset of Land Surface Fluxes and States for the Conterminous United States: Update and Extensions, Journal of Climate, 26, 9384–9392.\n",
    "\n",
    "    2. Livneh B., T.J. Bohn, D.S. Pierce, F. Munoz-Ariola, B. Nijssen, R. Vose, D. Cayan, and L.D. Brekke, 2015: A spatially comprehensive, hydrometeorological data set for Mexico, the U.S., and southern Canada 1950-2013, Nature Scientific Data, 5:150042, doi:10.1038/sdata.2015.42.\n",
    "\n",
    "    3. Salathé, EP, AF Hamlet, CF Mass, M Stumbaugh, S-Y Lee, R Steed: 2017. Estimates of 21st Century Flood Risk in the Pacific Northwest Based on Regional Scale Climate Model Simulations.  J. Hydrometeorology. DOI: 10.1175/JHM-D-13-0137.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all available datasets for Sauk-Suiattle Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ogh.getDailyMET_livneh2013(homedir, mappingfile1)\n",
    "ogh.getDailyMET_bcLivneh2013(homedir, mappingfile1)\n",
    "ogh.getDailyMET_livneh2015(homedir, mappingfile1)\n",
    "ogh.getDailyVIC_livneh2013(homedir, mappingfile1)\n",
    "ogh.getDailyVIC_livneh2015(homedir, mappingfile1)\n",
    "ogh.getDailyWRF_salathe2014(homedir, mappingfile1)\n",
    "ogh.getDailyWRF_bcsalathe2014(homedir, mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all available datasets for Elwha Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ogh.getDailyMET_livneh2013(homedir, mappingfile2)\n",
    "ogh.getDailyMET_bcLivneh2013(homedir, mappingfile2)\n",
    "ogh.getDailyMET_livneh2015(homedir, mappingfile2)\n",
    "ogh.getDailyVIC_livneh2013(homedir, mappingfile2)\n",
    "ogh.getDailyVIC_livneh2015(homedir, mappingfile2)\n",
    "ogh.getDailyWRF_salathe2014(homedir, mappingfile2)\n",
    "ogh.getDailyWRF_bcsalathe2014(homedir, mappingfile2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all available datasets for Upper Rio Salado Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ogh.getDailyMET_livneh2013(homedir, mappingfile3)\n",
    "ogh.getDailyMET_bcLivneh2013(homedir, mappingfile3)\n",
    "ogh.getDailyMET_livneh2015(homedir, mappingfile3)\n",
    "ogh.getDailyVIC_livneh2013(homedir, mappingfile3)\n",
    "ogh.getDailyVIC_livneh2015(homedir, mappingfile3)\n",
    "ogh.getDailyWRF_salathe2014(homedir, mappingfile3)\n",
    "ogh.getDailyWRF_bcsalathe2014(homedir, mappingfile3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1, mappingfile2, mappingfile3], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river','Elwha river','Upper Rio Salado'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "watershed_table = os.path.join(homedir, 'watershed_table.txt')\n",
    "t1.to_csv(watershed_table, sep='\\t', header=True, index=True)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive the downloaded data files for collaborative use\n",
    "\n",
    "Create list of files to save to HydroShare. Verify location and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!tar -zcf livneh2013.tar.gz livneh2013\n",
    "!tar -zcf livneh2015.tar.gz livneh2015\n",
    "!tar -zcf salathe2014.tar.gz salathe2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the downloaded tar files\n",
    "climate2013_tar = os.path.join(homedir,'livneh2013.tar.gz')\n",
    "climate2015_tar = os.path.join(homedir,'livneh2015.tar.gz')\n",
    "wrf_tar = os.path.join(homedir,'salathe2014.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Downloaded data sets from each study site for each of seven gridded data products'\n",
    "abstract = 'This resource contains the downloaded data files for each study site and the gridded cell centroids that intersected within the study area. The file availability is described within the watershed_table file, which summarizes each of the three mapping file catalogs.'\n",
    "keywords = ['Sauk', 'Elwha','Rio Salado','climate','hydromet','watershed'] \n",
    "rtype = 'genericresource'\n",
    "\n",
    "# files to migrate\n",
    "files=[mappingfile1, # sauk\n",
    "       mappingfile2, # elwha\n",
    "       mappingfile3, # riosalado\n",
    "       watershed_table, # watershed summary table\n",
    "       climate2013_tar, # Livneh et al. 2013 raw MET, bc MET, and VIC\n",
    "       climate2015_tar, # Livneh et al. 2015 raw MET, and VIC\n",
    "       wrf_tar] # Salathe et al. 2014 raw WRF and bc WRF\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(title=title, \n",
    "                                          abstract=abstract,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
