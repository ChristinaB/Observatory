{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve NetCDF and model gridded climate time-series for a watershed\n",
    "\n",
    "### Case study:  the Sauk-Suiattle Watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. HydroShare setup and preparation\n",
    "    2. Re-establish the paths to the mapping file\n",
    "    3. Compute daily, monthly, and annual temperature and precipitation statistics\n",
    "    4. Visualize precipitation results relative to the forcing data\n",
    "    5. Visualize the time-series trends\n",
    "    6. Save results back into HydroShare\n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ogh/ogh.py:12: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n",
      "    self._current_handle = None\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c38b888d7e8f>\", line 4, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/__init__.py\", line 6, in <module>\n",
      "    from .rcmod import *\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/rcmod.py\", line 8, in <module>\n",
      "    from . import palettes, _orig_rc_params\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/palettes.py\", line 12, in <module>\n",
      "    from .utils import desaturate, set_hls_values, get_color_cycle\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/utils.py\", line 11, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  mpl.use('Agg')\n",
      "/opt/conda/lib/python3.6/site-packages/landlab/__init__.py:27: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n",
      "    self._current_handle = None\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c38b888d7e8f>\", line 4, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/__init__.py\", line 6, in <module>\n",
      "    from .rcmod import *\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/rcmod.py\", line 8, in <module>\n",
      "    from . import palettes, _orig_rc_params\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/palettes.py\", line 12, in <module>\n",
      "    from .utils import desaturate, set_hls_values, get_color_cycle\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/utils.py\", line 11, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import seaborn as sns\n",
    "\n",
    "# data migration library\n",
    "import ogh\n",
    "import ogh_xarray_landlab as oxl\n",
    "from utilities import hydroshare\n",
    "from ecohydrology_model_functions import run_ecohydrology_model, plot_results\n",
    "InputFile = 'ecohyd_inputs.yaml'\n",
    "\n",
    "# plotting and shape libraries\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dailymet_bclivneh2013',\n",
       " 'dailymet_livneh2013',\n",
       " 'dailymet_livneh2015',\n",
       " 'dailyvic_livneh2013',\n",
       " 'dailyvic_livneh2015',\n",
       " 'dailywrf_bcsalathe2014',\n",
       " 'dailywrf_salathe2014']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_steps',\n",
       " 'delimiter',\n",
       " 'domain',\n",
       " 'end_date',\n",
       " 'file_format',\n",
       " 'filename_structure',\n",
       " 'reference',\n",
       " 'spatial_resolution',\n",
       " 'start_date',\n",
       " 'subdomain',\n",
       " 'temporal_resolution',\n",
       " 'variable_info',\n",
       " 'variable_list',\n",
       " 'web_protocol']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(meta_file['dailymet_livneh2013'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following system variables:\n",
      "   HS_USR_NAME = jphuong\n",
      "   HS_RES_ID = 70b977e22af544f8a7e5a803935c329c\n",
      "   HS_RES_TYPE = genericresource\n",
      "   JUPYTER_HUB_IP = jupyter.cuahsi.org\n",
      "\n",
      "These can be accessed using the following command: \n",
      "   os.environ[key]\n",
      "\n",
      "   (e.g.)\n",
      "   os.environ[\"HS_USR_NAME\"]  => jphuong\n",
      "\n",
      "The hs_utils library requires a secure connection to your HydroShare account.\n",
      "Enter the HydroShare password for user 'jphuong': ········\n",
      "Successfully established a connection with HydroShare\n"
     ]
    }
   ],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of gridded climate points for the watershed\n",
    "\n",
    "For visualization purposes, we will also remap the study site shapefile, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. Since the shapefile was previously migrated, we can select 'N' for no overwriting.\n",
    "\n",
    "In the usecase1 notebook, the treatgeoself function identified the gridded cell centroid coordinates that overlap with our study site. These coordinates were documented within the mapping file, which will be remapped here. In the usecase2 notebook, the downloaded files were cataloged within the mapping file, so we will use the mappingfileSummary function to characterize the files available for Sauk-Suiattle for each gridded data product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|-- ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- NAmer_dem_list.cpg\n",
      "|   |   |   |-- NAmer_dem_list.dbf\n",
      "|   |   |   |-- NAmer_dem_list.prj\n",
      "|   |   |   |-- NAmer_dem_list.sbn\n",
      "|   |   |   |-- NAmer_dem_list.sbx\n",
      "|   |   |   |-- NAmer_dem_list.shp\n",
      "|   |   |   |-- NAmer_dem_list.shx\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "NAmer_dem_list.cpg<br>NAmer_dem_list.dbf<br>NAmer_dem_list.prj<br>NAmer_dem_list.sbn<br>NAmer_dem_list.sbx<br>NAmer_dem_list.shp<br>NAmer_dem_list.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "c532e0578e974201a0bc40a37ef2d284/\n",
      "|-- c532e0578e974201a0bc40a37ef2d284/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.cpg\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shp\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shx\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.dbf\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.prj\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wbdhub12_17110006_WGS84_Basin.cpg<br>wbdhub12_17110006_WGS84_Basin.shp<br>wbdhub12_17110006_WGS84_Basin.shx<br>wbdhub12_17110006_WGS84_Basin.dbf<br>wbdhub12_17110006_WGS84_Basin.prj"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 4)\n",
      "   FID       LAT      LONG_    ELEV\n",
      "0    0  48.53125 -121.59375  1113.0\n",
      "1    1  48.46875 -121.46875   646.0\n",
      "2    2  48.46875 -121.53125   321.0\n",
      "3    3  48.46875 -121.59375   164.0\n",
      "4    4  48.46875 -121.65625   369.0\n",
      "CPU times: user 22.3 s, sys: 493 ms, total: 22.7 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# map the mappingfiles from usecase1\n",
    "mappingfile1=ogh.treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                              mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Compare Hydrometeorology \n",
    "\n",
    "This section performs computations and generates plots of the Livneh 2013 and Salathe 2014 mean temperature and mean total monthly precipitation in order to compare them with each other. The generated plots are automatically downloaded and saved as .png files within the \"homedir\" directory.\n",
    "\n",
    "Let's compare the Livneh 2013 and Salathe 2014 using the period of overlapping history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function getDailyWRF_salathe2014 in module ogh.ogh:\n",
      "\n",
      "getDailyWRF_salathe2014(homedir, mappingfile, subdir='salathe2014/WWA_1950_2010/raw', catalog_label='dailywrf_salathe2014')\n",
      "    Get the Salathe el al., 2014 raw Daily WRF files of interest using the reference mapping file\n",
      "    \n",
      "    homedir: (dir) the home directory to be used for establishing subdirectories\n",
      "    mappingfile: (dir) the file path to the mappingfile, which contains LAT, LONG_, and ELEV coordinates of interest\n",
      "    subdir: (dir) the subdirectory to be established under homedir\n",
      "    catalog_label: (str) the preferred name for the series of catalogged filepaths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ogh.getDailyWRF_salathe2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_x_dailywrf_Salathe2014 in module ogh_xarray_landlab:\n",
      "\n",
      "get_x_dailywrf_Salathe2014(homedir, spatialbounds, subdir='salathe2014/Daily_WRF_1970_1999/noBC', nworkers=4, start_date='1970-01-01', end_date='1989-12-31', rename_timelatlong_names={'LAT': 'LAT', 'LON': 'LON'}, file_prefix='sp_', replace_file=True)\n",
      "    get Daily WRF data from Salathe et al. (2014) using xarray on netcdf files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(oxl.get_x_dailywrf_Salathe2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF retrieval and clipping to a spatial extent\n",
    "\n",
    "The function get_x_dailywrf_salathe2014 retrieves and clips NetCDF files archived within the UW Rocinante NNRP repository. This archive contains daily data from January 1970 through December 1999 (30 years). Each netcdf file is comprised of meteorologic and VIC hydrologic outputs for a calendar month. The expected number of files would be 360 files (12 months for 30 years). \n",
    "\n",
    "In the code chunk below, 40 parallel workers will be initialized to distribute file retrieval and spatial clipping tasks. For each worker, they will wget the requested file, clip the netcdf file to gridded cell centroids within the the provided bounding box, then return the location of the spatially clipped output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gridded data files:99\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1151.040404040404m\n",
      "Maximum elevation: 2216.0m\n",
      "[########################################] | 100% Completed |  7min 20.4s\n"
     ]
    }
   ],
   "source": [
    "maptable, nstations = ogh.mappingfileToDF(mappingfile1)\n",
    "spatialbounds = {'minx':maptable.LONG_.min(), 'maxx':maptable.LONG_.max(),\n",
    "                 'miny':maptable.LAT.min(), 'maxy':maptable.LAT.max()}\n",
    "\n",
    "outputfiles = oxl.get_x_dailywrf_Salathe2014(homedir=homedir,\n",
    "                                             subdir='salathe2014/Daily_WRF_1970_1979/noBC_netcdf',\n",
    "                                             spatialbounds=spatialbounds,\n",
    "                                             nworkers=40,\n",
    "                                             start_date='1970-01-01', end_date='1979-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed | 22.0s\n",
      "[########################################] | 100% Completed | 22.1s\n",
      "CPU times: user 49.9 s, sys: 4.77 s, total: 54.6 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outfilelist = oxl.netcdf_to_ascii(homedir=homedir, \n",
    "                                  subdir='salathe2014/Daily_WRF_1970_1979/noBC_ascii', \n",
    "                                  mappingfile=mappingfile1,\n",
    "                                  source_directory=os.path.join(homedir, 'salathe2014/Daily_WRF_1970_1979/noBC_netcdf'),\n",
    "                                  meta_file=meta_file,\n",
    "                                  catalog_label='sp_WRF_NNRP_noBC_1970_1979')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = os.path.join(homedir, 'salathe2014/Daily_WRF_1970_1979/noBC_netcdf')\n",
    "files = os.listdir(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "test = xarray.open_mfdataset(os.path.join(filedir, files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Of the three parameters: start, end, and periods, exactly two must be specified",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-42435aa5ea88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/timedeltas.py\u001b[0m in \u001b[0;36mtimedelta_range\u001b[0;34m(start, end, periods, freq, name, closed)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \"\"\"\n\u001b[1;32m   1017\u001b[0m     return TimedeltaIndex(start=start, end=end, periods=periods,\n\u001b[0;32m-> 1018\u001b[0;31m                           freq=freq, name=name, closed=closed)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/timedeltas.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, unit, freq, start, end, periods, copy, name, closed, verify_integrity, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             return cls._generate(start, end, periods, name, freq,\n\u001b[0;32m--> 219\u001b[0;31m                                  closed=closed)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/timedeltas.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(cls, start, end, periods, name, offset, closed)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_not_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             raise ValueError('Of the three parameters: start, end, and '\n\u001b[0m\u001b[1;32m    265\u001b[0m                              'periods, exactly two must be specified')\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Of the three parameters: start, end, and periods, exactly two must be specified"
     ]
    }
   ],
   "source": [
    "tmp = pd.Series(test.TIME)\n",
    "tdiff = (tmp-tmp.shift(1))[1:2]\n",
    "pd.timedelta_range(tdiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<m8[ns]')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdiff.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'projection': 'Geographic',\n",
       " 'contributors': 'Eric P. Salathe, A.F. Hamlet, C.F. Mass, S-Y. Lee, G.S. Mauger, M. Stumbaugh, R. Steed, B. Dotson',\n",
       " 'keywords': 'climate change, climate impacts, dynamical downscaling, hydrologic change, extremes, Western U.S., Western United States, PNW, Pacific Northwest',\n",
       " 'contributor_role': 'Principal Investigator',\n",
       " 'contributor': 'Eric P. Salathe Jr.',\n",
       " 'contact_name': 'Eric P. Salathe Jr.',\n",
       " 'contributor_email': 'salathe@uw.edu',\n",
       " 'lat_min': '38.09375',\n",
       " 'lon_resolution': '0.0625',\n",
       " 'lat_max': '52.40625',\n",
       " 'title': 'Dynamically Downscaled Hydroclimate Projections: WRF model',\n",
       " 'lat_resolution': '0.0625',\n",
       " 'lon_min': '-124.65625',\n",
       " 'creator_email': 'bdotson@uw.edu',\n",
       " 'surfsgnconvention': 'Traditional',\n",
       " 'contact_email': 'salathe@uw.edu',\n",
       " 'institution': 'University of Washington, Climate Impacts Group',\n",
       " 'creator_name': 'Bri Dotson',\n",
       " 'rights': 'freely available',\n",
       " 'acknowledgement': ' ',\n",
       " 'summary': 'Dynamically downscaled NCEP-NCAR Reanalysis (NNRP) data and VIC model output using WRF for the period 1970-1999 at 1/16 degree resolution over the Pacific Northwestern United States. This dataset includes an observational run as well as two bias-correction methods. ',\n",
       " 'cdm_data_type': 'grid',\n",
       " 'sources': 'http://cses.washington.edu/cig/data/wrf.shtml',\n",
       " 'history': ' ',\n",
       " 'lon_max': '-105.96875',\n",
       " 'file_name': '/home/disk/picea/mauger/WRF/DATA/wrf_nnrp_vic_16d/WRF_NNRP_noBC/netcdf_daily/WRF_NNRP_noBC.197207.nc',\n",
       " 'variable_list': ['MASK',\n",
       "  'PREC',\n",
       "  'TAVG',\n",
       "  'TMAX',\n",
       "  'TMIN',\n",
       "  'OLR',\n",
       "  'ISR',\n",
       "  'RH',\n",
       "  'VPD',\n",
       "  'ET',\n",
       "  'RUNOFF',\n",
       "  'BASEFLOW',\n",
       "  'SOILM1',\n",
       "  'SOILM2',\n",
       "  'SOILM3',\n",
       "  'SWE',\n",
       "  'SNODEP',\n",
       "  'PET1',\n",
       "  'PET2',\n",
       "  'PET3',\n",
       "  'PET4',\n",
       "  'PET5'],\n",
       " 'delimiter': '\\t',\n",
       " 'start_date': Timestamp('1972-07-01 00:00:00'),\n",
       " 'end_date': Timestamp('1976-04-30 00:00:00'),\n",
       " 'temporal_resolution': 'D',\n",
       " 'variable_info': {'LON': <xarray.IndexVariable 'LON' (LON: 14)>\n",
       "  array([-121.71875, -121.65625, -121.59375, -121.53125, -121.46875, -121.40625,\n",
       "         -121.34375, -121.28125, -121.21875, -121.15625, -121.09375, -121.03125,\n",
       "         -120.96875, -120.90625], dtype=float32)\n",
       "  Attributes:\n",
       "      units:      degrees_east\n",
       "      long_name:  longitude\n",
       "      valid_min:  -180.0\n",
       "      valid_max:  180.0, 'LAT': <xarray.IndexVariable 'LAT' (LAT: 11)>\n",
       "  array([ 47.90625,  47.96875,  48.03125,  48.09375,  48.15625,  48.21875,\n",
       "          48.28125,  48.34375,  48.40625,  48.46875,  48.53125], dtype=float32)\n",
       "  Attributes:\n",
       "      units:      degrees_north\n",
       "      long_name:  latitude\n",
       "      valid_min:  -90.0\n",
       "      valid_max:  90.0, 'TIME': <xarray.IndexVariable 'TIME' (TIME: 3652)>\n",
       "  array(['1972-07-01T00:00:00.000000000', '1972-07-02T00:00:00.000000000',\n",
       "         '1972-07-03T00:00:00.000000000', ..., '1976-04-28T00:00:00.000000000',\n",
       "         '1976-04-29T00:00:00.000000000', '1976-04-30T00:00:00.000000000'], dtype='datetime64[ns]')\n",
       "  Attributes:\n",
       "      long_name:  Time axis, 'MASK': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=int32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:      0=invalid, 1=valid\n",
       "      long_name:  binary_land_mask, 'YEAR': <xarray.Variable (TIME: 3652)>\n",
       "  dask.array<shape=(3652,), dtype=int16, chunksize=(31,)>\n",
       "  Attributes:\n",
       "      units:      Year\n",
       "      long_name:  Calendar Year, 'MONTH': <xarray.Variable (TIME: 3652)>\n",
       "  dask.array<shape=(3652,), dtype=int16, chunksize=(31,)>\n",
       "  Attributes:\n",
       "      units:      Month\n",
       "      long_name:  Calendar Month, 'DAY': <xarray.Variable (TIME: 3652)>\n",
       "  dask.array<shape=(3652,), dtype=int16, chunksize=(31,)>\n",
       "  Attributes:\n",
       "      units:      Day\n",
       "      long_name:  Calendar Day, 'PREC': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Precipitation\n",
       "      description:  , 'TAVG': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        C\n",
       "      long_name:    Average daily air temperature\n",
       "      description:  , 'TMAX': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        C\n",
       "      long_name:    Maximum daily air temperature\n",
       "      description:  , 'TMIN': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        C\n",
       "      long_name:    Minimum daily air temperature\n",
       "      description:  , 'OLR': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        W/m^2\n",
       "      long_name:    Outgoing longwave radiation\n",
       "      description:  , 'ISR': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        W/m^2\n",
       "      long_name:    Incoming shortwave radiation\n",
       "      description:  solar radiation, 'RH': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        %\n",
       "      long_name:    Relative humidity\n",
       "      description:  , 'VPD': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        Pa\n",
       "      long_name:    Vapor pressure deficit\n",
       "      description:  , 'ET': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Evapotranspiration\n",
       "      description:  , 'RUNOFF': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Daily runoff\n",
       "      description:  , 'BASEFLOW': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Daily baseflow\n",
       "      description:  , 'SOILM1': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Soil moisture, layer 1\n",
       "      description:  , 'SOILM2': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Soil moisture, layer 2\n",
       "      description:  , 'SOILM3': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Soil moisture, layer 3\n",
       "      description:  , 'SWE': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Snow water equivalent\n",
       "      description:  total snowpack water content, 'SNODEP': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        cm\n",
       "      long_name:    Snow depth\n",
       "      description:  , 'PET1': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Potential evapotranspiration 1\n",
       "      description:  natural vegetation, no water limit, 'PET2': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Potential evapotranspiration 2\n",
       "      description:  open water surface, fixed albedo, 'PET3': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Potential evapotranspiration 3\n",
       "      description:  natural vegetation, no water limit, no vegetation resistance, 'PET4': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Potential evapotranspiration 4\n",
       "      description:  tall reference crop (alfalfa), 'PET5': <xarray.Variable (TIME: 3652, LAT: 11, LON: 14)>\n",
       "  dask.array<shape=(3652, 11, 14), dtype=float32, chunksize=(31, 11, 14)>\n",
       "  Attributes:\n",
       "      units:        mm\n",
       "      long_name:    Potential evapotranspiration 5\n",
       "      description:  short reference crop (short grass)}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_file['sp_WRF_NNRP_noBC_1970_1979']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of climate variables for the long-term mean (ltm).\n",
    "#### INPUT: gridded meteorology ASCII files located from the Sauk-Suiattle Mapping file. The inputs to gridclim_dict() include the folder location and name of the hydrometeorology data, the file start and end, the analysis start and end, and the elevation band to be included in the analsyis (max and min elevation). <br/>OUTPUT: dictionary of dataframes where rows are temporal summaries and columns are spatial summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        dataset='sp_WRF_NNRP_noBC_1970_1979')\n",
    "\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total monthly and yearly precipitation, as well as the mean values across time and across stations\n",
    "#### INPUT: daily precipitation for each station from the long-term mean dictionary (ltm) <br/>OUTPUT: Append the computed dataframes and values into the ltm dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata\n",
    "dr = meta_file['sp_WRF_NNRP_noBC_1970_1979']\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='PREC_sp_WRF_NNRP_noBC_1970_1979',\n",
    "                                   start_date=dr['start_date'],\n",
    "                                   end_date=dr['end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the analytical dataframes and values within ltm\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of outputs\n",
    "files=[]\n",
    "\n",
    "# create the destination path for the dictionary of dataframes\n",
    "ltm_sauk=os.path.join(homedir, 'ltm_1970_1979_sauk.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=ltm, outfilepath=ltm_sauk)\n",
    "files.append(ltm_sauk)\n",
    "\n",
    "# append the mapping file for Sauk-Suiattle gridded cell centroids\n",
    "files.append(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitations\"\n",
    "\n",
    "#### INPUT: dataframe with each month as a row and each station as a column. <br/>OUTPUT: A png file that represents the distribution across stations (in Wateryear order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two lowest elevation locations\n",
    "lowE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=164)\n",
    "\n",
    "# one highest elevation location\n",
    "highE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=2216)\n",
    "\n",
    "# combine references together\n",
    "reference_lines = highE_ref + lowE_ref\n",
    "reference_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# #Higher resolution children gridded cells \n",
    "# #get data from Lower resolution parent grid cells to the children\n",
    "# \"\"\"\n",
    "# import landlab as L2\n",
    "\n",
    "# watershed_dem_sc = os.path.join(homedir, 'DEM_10m.asc')\n",
    "# (rmg_sc, z_sc) = L2.io.read_esri_ascii(watershed_dem_sc, name='topographic__elevation')\n",
    "# rmg_sc.set_watershed_boundary_condition(z_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test0=pd.read_table(watershed_dem_sc, nrows=5, sep='\\s+', header=None).set_index(0)[1].to_dict()\n",
    "# print(test0)\n",
    "\n",
    "# test1 = pd.read_table(watershed_dem_sc, \n",
    "#                       skiprows=6, \n",
    "#                       nrows=test0['nrows'],\n",
    "#                       sep='\\s+',\n",
    "#                       header=None)\n",
    "# print(test1.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.unstack().as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx2, miny2, maxx2, maxy2 = oxl.calculateUTMbounds(mappingfile=mappingfile1,\n",
    "                                                    mappingfile_crs={'init':'epsg:4326'},\n",
    "                                                    spatial_resolution=0.06250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx2, miny2, maxx2, maxy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a raster\n",
    "raster, t1, t2 = oxl.rasterDimensions (minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=100, dy=100)\n",
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeXmap, raster = oxl.mappingfileToRaster(mappingfile=mappingfile1,\n",
    "                                           spatial_resolution=0.06250,\n",
    "                                           approx_distance_m_x=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PREC_sp_WRF_NNRP_noBC_1970_1979'],\n",
    "                          vardf_dateindex=3,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(VegType_low, yrs_low, debug_low) = run_ecohydrology_model(raster, \n",
    "                                                           input_data=vector,\n",
    "                                                           input_file=InputFile,\n",
    "                                                           synthetic_storms=False,\n",
    "                                                           number_of_storms=50000,\n",
    "                                                           pet_method='PriestleyTaylor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbyyearsum_PREC_sp_WRF_NNRP_noBC_1970_1979'],\n",
    "                          vardf_dateindex=0,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total files and image to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Computed spatial-temporal summaries of two gridded data product data sets for Sauk-Suiattle'\n",
    "abstract = 'This resource contains the computed summaries for the Meteorology data from Livneh et al. 2013 and the WRF data from Salathe et al. 2014.'\n",
    "keywords = ['Sauk-Suiattle', 'Livneh 2013', 'Salathe 2014','climate','hydromet','watershed', 'visualizations and summaries'] \n",
    "rtype = 'genericresource'\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_bc45\n",
    "models=[model for model in df.columns if model not in ['Date','Year','Month','Day']]\n",
    "time=time1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t=pd.DataFrame({'test':['terrific']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=pd.DataFrame({'test':['terrific']})\n",
    "t2=pd.DataFrame({'test':['terrific']})\n",
    "t3=pd.DataFrame({'test':['terrific']})\n",
    "t4=pd.DataFrame({'test':['terrific']})\n",
    "\n",
    "for somedf in (t1, t2, t3, t4):\n",
    "    if somedf in locals():\n",
    "        print(locals())\n",
    "    list(somedf.to_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [t1,t2,t2,t4]:\n",
    "    if each in globals().keys():\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = eval(t)\n",
    "str(y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
