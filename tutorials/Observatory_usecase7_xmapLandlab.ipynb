{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve NetCDF and model gridded climate time-series for a watershed\n",
    "\n",
    "### Case study:  the Sauk-Suiattle Watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. HydroShare setup and preparation\n",
    "    2. Re-establish the paths to the mapping file\n",
    "    3. Compute daily, monthly, and annual temperature and precipitation statistics\n",
    "    4. Visualize precipitation results relative to the forcing data\n",
    "    5. Visualize the time-series trends\n",
    "    6. Save results back into HydroShare\n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import seaborn as sns\n",
    "\n",
    "# data migration library\n",
    "from utilities import hydroshare\n",
    "import ogh\n",
    "import ogh_xarray_landlab as oxl\n",
    "\n",
    "# plotting and shape libraries\n",
    "%matplotlib inline\n",
    "\n",
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dailymet_bclivneh2013',\n",
       " 'dailymet_livneh2013',\n",
       " 'dailymet_livneh2015',\n",
       " 'dailyvic_livneh2013',\n",
       " 'dailyvic_livneh2015',\n",
       " 'dailywrf_bcsalathe2014',\n",
       " 'dailywrf_salathe2014']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_steps',\n",
       " 'delimiter',\n",
       " 'domain',\n",
       " 'end_date',\n",
       " 'file_format',\n",
       " 'filename_structure',\n",
       " 'reference',\n",
       " 'spatial_resolution',\n",
       " 'start_date',\n",
       " 'subdomain',\n",
       " 'temporal_resolution',\n",
       " 'variable_info',\n",
       " 'variable_list',\n",
       " 'web_protocol']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(meta_file['dailymet_livneh2013'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following system variables:\n",
      "   HS_USR_NAME = jphuong\n",
      "   HS_RES_ID = 70b977e22af544f8a7e5a803935c329c\n",
      "   HS_RES_TYPE = genericresource\n",
      "   JUPYTER_HUB_IP = jupyter.cuahsi.org\n",
      "\n",
      "These can be accessed using the following command: \n",
      "   os.environ[key]\n",
      "\n",
      "   (e.g.)\n",
      "   os.environ[\"HS_USR_NAME\"]  => jphuong\n",
      "Successfully established a connection with HydroShare\n"
     ]
    }
   ],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of gridded climate points for the watershed\n",
    "\n",
    "For visualization purposes, we will also remap the study site shapefile, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. Since the shapefile was previously migrated, we can select 'N' for no overwriting.\n",
    "\n",
    "In the usecase1 notebook, the treatgeoself function identified the gridded cell centroid coordinates that overlap with our study site. These coordinates were documented within the mapping file, which will be remapped here. In the usecase2 notebook, the downloaded files were cataloged within the mapping file, so we will use the mappingfileSummary function to characterize the files available for Sauk-Suiattle for each gridded data product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|-- ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- NAmer_dem_list.cpg\n",
      "|   |   |   |-- NAmer_dem_list.dbf\n",
      "|   |   |   |-- NAmer_dem_list.prj\n",
      "|   |   |   |-- NAmer_dem_list.sbn\n",
      "|   |   |   |-- NAmer_dem_list.sbx\n",
      "|   |   |   |-- NAmer_dem_list.shp\n",
      "|   |   |   |-- NAmer_dem_list.shx\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? y\n",
      "Download Finished                               \n",
      "Successfully downloaded resource ef2d82bf960144b4bfb1bae6242bcc7f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "NAmer_dem_list.cpg<br>NAmer_dem_list.dbf<br>NAmer_dem_list.prj<br>NAmer_dem_list.sbn<br>NAmer_dem_list.sbx<br>NAmer_dem_list.shp<br>NAmer_dem_list.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "c532e0578e974201a0bc40a37ef2d284/\n",
      "|-- c532e0578e974201a0bc40a37ef2d284/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.cpg\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shp\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shx\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.dbf\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.prj\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? y\n",
      "Download Finished                               \n",
      "Successfully downloaded resource c532e0578e974201a0bc40a37ef2d284\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wbdhub12_17110006_WGS84_Basin.cpg<br>wbdhub12_17110006_WGS84_Basin.dbf<br>wbdhub12_17110006_WGS84_Basin.prj<br>wbdhub12_17110006_WGS84_Basin.sbn<br>wbdhub12_17110006_WGS84_Basin.sbx<br>wbdhub12_17110006_WGS84_Basin.shp<br>wbdhub12_17110006_WGS84_Basin.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatgeoself(shapefile, NAmer, mappingfile=os.path.join(os.getcwd(), 'mappingfile.csv'), buffer_distance=0.06):\n",
    "    \"\"\"\n",
    "    TreatGeoSelf to some [data] lovin'!\n",
    "    \n",
    "    shapefile: (dir) the path to an ESRI shapefile for the region of interest\n",
    "    Namer: (dir) the path to the ESRI shapefile, which has each 1/16th-degree gridded cell centroid and DEM elevation\n",
    "    mappingfile: (str) the name of the output file; default is 'mappingfile.csv'\n",
    "    buffer_distance: (float64) the multiplier for increasing the geodetic boundary area; default is 0.06\n",
    "    \"\"\"\n",
    "    # conform projections to longlat values in WGS84\n",
    "    reprojShapefile(shapefile, newprojdictionary={'proj': 'longlat', 'ellps': 'WGS84', 'datum': 'WGS84'}, outpath=None)\n",
    "    \n",
    "    # read shapefile into a multipolygon shape-object\n",
    "    shape_mp = getFullShape(shapefile)\n",
    "    \n",
    "    # read in the North American continental DEM points for the station elevations\n",
    "    NAmer_datapoints = readShapefileTable(NAmer).rename(columns={'Lat': 'LAT', 'Long': 'LONG_', 'Elev': 'ELEV'})\n",
    "    \n",
    "    # generate maptable\n",
    "    maptable = filterPointsinShape(shape_mp, \n",
    "                                   points_lat=NAmer_datapoints.LAT, \n",
    "                                   points_lon=NAmer_datapoints.LONG_,\n",
    "                                   points_elev=NAmer_datapoints.ELEV,\n",
    "                                   buffer_distance=buffer_distance, buffer_resolution=16, \n",
    "                                   labels=['LAT', 'LONG_', 'ELEV'])\n",
    "    maptable.reset_index(inplace=True)\n",
    "    maptable = maptable.rename(columns={'index': 'FID'})\n",
    "    print(maptable.shape)\n",
    "    print(maptable.head())\n",
    "    \n",
    "    # print the mappingfile\n",
    "    maptable.to_csv(mappingfile, sep=',', header=True, index=False)\n",
    "    return(mappingfile)\n",
    "\n",
    "\n",
    "def filterPointsinShape(shape, points_lat, points_lon, points_elev=None, buffer_distance=0.06, \n",
    "                        buffer_resolution=16, labels=['LAT', 'LONG_', 'ELEV']):\n",
    "    \"\"\"\n",
    "    Filter for datafiles that can be used\n",
    "    \n",
    "    shape: (geometry) a geometric polygon or MultiPolygon\n",
    "    points_lat: (series) a series of latitude points in WGS84 projection\n",
    "    points_lon: (series) a series of longitude points in WGS84 projection\n",
    "    points_elev: (series) a series of elevation points in meters; default is None\n",
    "    buffer_distance: (float64) a numerical multiplier to increase the geodetic boundary area\n",
    "    buffer_resolution: (float64) the increments between geodetic longlat degrees\n",
    "    labels: (list) a list of preferred labels for latitude, longitude, and elevation\n",
    "    \"\"\"\n",
    "    # add buffer region\n",
    "    region = shape.buffer(buffer_distance, resolution=buffer_resolution)\n",
    "    \n",
    "    # construct bounds\n",
    "    minx, miny, maxx, maxy = region.bounds\n",
    "    \n",
    "    # construct points_elev if null\n",
    "    if isinstance(points_elev, type(None)):\n",
    "        points_elev=np.repeat(np.nan, len(points_lon))\n",
    "        \n",
    "    # filter the points to the regional bounds\n",
    "    point_filter = points_lat.map(lambda y:(y>=miny)and(y<=maxy)) & points_lon.map(lambda x:(x>=minx)and(x<=maxx))\n",
    "    points_lon = points_lon.loc[point_filter]\n",
    "    points_lat = points_lat.loc[point_filter]\n",
    "    points_elev = points_elev.loc[point_filter]\n",
    "    \n",
    "    # Intersection each coordinate with the region\n",
    "    limited_list = []    \n",
    "    for lon, lat, elev in zip(points_lon, points_lat, points_elev):\n",
    "        gpoint = point.Point(lon, lat)\n",
    "        if gpoint.intersects(region):\n",
    "            limited_list.append([lat, lon, elev])\n",
    "\n",
    "    maptable = pd.DataFrame.from_records(limited_list, columns=labels)\n",
    "    return(maptable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_mp = getFullShape(sauk)\n",
    "\n",
    "# add buffer region\n",
    "region = shape_mp.buffer(0.06, resolution=16)\n",
    "\n",
    "# read in the North American continental DEM points for the station elevations\n",
    "NAmer_datapoints = readShapefileTable(NAmer).rename(columns={'Lat': 'LAT', 'Long': 'LONG_', 'Elev': 'ELEV'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-121.77964055218817 47.861448800342785 0.9690326485623046 0.7159806405065297\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,96.4388782411921)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.019380652971246092\" opacity=\"0.6\" d=\"M -121.74205930994844,48.14731376006673 L -121.74303688804333,48.1519959652796 L -121.7436677390791,48.15770158033659 L -121.74375045409327,48.163441369046495 L -121.7432842759674,48.16916279318414 L -121.74227347178734,48.174813482621495 L -121.74072729378526,48.18034171468914 L -121.73865989465075,48.18569688761133 L -121.73609019798654,48.190829983681134 L -121.73304172509432,48.19569401793608 L -121.72954237967637,48.200244468227396 L -121.7256241924235,48.20443968274626 L -121.72132302782732,48.20824126127689 L -121.71667825590049,48.21161440668668 L -121.71173239180962,48.21452824343613 L -121.7065307067196,48.21695610019307 L -121.7011208134114,48.218875753964404 L -121.69491295353433,48.22074941566922 L -121.68847902763653,48.222309783671164 L -121.68204224593951,48.22313515596599 L -121.6824388510909,48.22380152093705 L -121.68494091425052,48.22913506572785 L -121.68690785900775,48.23468827636785 L -121.68832072236505,48.24040761524781 L -121.68916588313436,48.246237943142894 L -121.68943519325688,48.25212305080105 L -121.68912605635708,48.25800620084736 L -121.68824145277401,48.26383067477992 L -121.68678991082832,48.26954031978378 L -121.6847854246021,48.2750800900912 L -121.68224731902414,48.280396577669045 L -121.68051464998449,48.283263429436126 L -121.6800798432205,48.2869762503523 L -121.67880338103085,48.29279586536371 L -121.67695604094556,48.29846019314481 L -121.67455603839194,48.30391338141821 L -121.67162703824492,48.3091016598168 L -121.66819792148296,48.31397387007861 L -121.66488373824906,48.31780931444028 L -121.66348942779693,48.32356382024972 L -121.662494382289,48.326326898986224 L -121.66110160786106,48.332076291241705 L -121.65890944435263,48.33816518209083 L -121.65613316603161,48.344740229337845 L -121.65683813089987,48.34590237477262 L -121.65936412431748,48.35115783388234 L -121.66136805005956,48.356633669418564 L -121.66283098188627,48.36227816440661 L -121.66310976873889,48.3640463965018 L -121.66314863071514,48.36415012996389 L -121.66440021664438,48.36886374704973 L -121.66441618904462,48.368874385161355 L -121.67284597598493,48.37515097832269 L -121.67816754118235,48.377846256554164 L -121.68255303371656,48.38030348854927 L -121.68671744049617,48.38311920338204 L -121.69063152908159,48.38627363588107 L -121.69553604191765,48.39057733171033 L -121.70007471575418,48.393795866858554 L -121.70447968333947,48.39761291359797 L -121.70849349738418,48.40183936388783 L -121.71207827909566,48.40643533226449 L -121.71520019850529,48.41135744608256 L -121.71782979372551,48.41655925482722 L -121.71994224898485,48.42199166847269 L -121.72151762881697,48.42760342075008 L -121.7225410661938,48.43334155295263 L -121.7230029028273,48.439151913712706 L -121.72289878031575,48.44497967003404 L -121.72222968127441,48.45076982475659 L -121.72100192006255,48.45646773557064 L -121.71922708319411,48.46201963068211 L -121.71692191999456,48.4673731162627 L -121.71410818453566,48.47247767089612 L -121.71081243034004,48.47728512235405 L -121.70706575979274,48.481750102202625 L -121.70290353062465,48.48583047394923 L -121.69836502223781,48.489487730689035 L -121.69349306502156,48.492687358498834 L -121.68833363615748,48.49539916214858 L -121.68293542572789,48.497597550057044 L -121.67734937722227,48.49926177580228 L -121.67375992246109,48.500143831865486 L -121.67291300562306,48.50068818852215 L -121.6711416359206,48.5047955690498 L -121.66834638555498,48.509867308812765 L -121.66507521565389,48.514645950492984 L -121.66135859895392,48.51908697842977 L -121.65723115777226,48.5231490220191 L -121.65273134148077,48.52679424110343 L -121.64790106832919,48.52998867847342 L -121.6437747265137,48.53243989445581 L -121.64377083241953,48.53243333919357 L -121.64102873352121,48.53412913032662 L -121.6354746724811,48.53681865602996 L -121.62967373577577,48.5389234780859 L -121.62368728626569,48.54042133148646 L -121.6175786491834,48.54129637179585 L -121.61141244227318,48.54153934275441 L -121.60938321440561,48.54151486021711 L -121.60938267482051,48.541514827882544 L -121.60938215611826,48.541514847439586 L -121.60735293097306,48.541490329135584 L -121.60735183692323,48.54149026330051 L -121.60735081439948,48.54149030352455 L -121.60126315533805,48.54141653401358 L -121.59559144409114,48.541078765187706 L -121.58997711238742,48.540206024809 L -121.58447050580486,48.538806139030456 L -121.57912100391448,48.53689166109301 L -121.57397657747734,48.53447975875649 L -121.56908335827474,48.5315920603506 L -121.56815964318906,48.530921572756185 L -121.56621450327734,48.52976546478097 L -121.56125423209525,48.526473733343494 L -121.55664457606359,48.52270665790942 L -121.55243091441112,48.51850132301969 L -121.5510371773357,48.51680271766774 L -121.55033838728828,48.51670132237744 L -121.54390154685254,48.51539286400609 L -121.54108980697113,48.51507421205275 L -121.53508878130707,48.513762022762755 L -121.5325543303962,48.512928603843136 L -121.52701641483725,48.512938814472 L -121.52045675543617,48.51222639422264 L -121.5108270234196,48.51064076578501 L -121.505376009682,48.50948089467689 L -121.50005612749796,48.50782026067059 L -121.49491327393196,48.5056731908176 L -121.48999181874134,48.503058208874684 L -121.48503448064768,48.50012112743661 L -121.47991434985302,48.49873947176501 L -121.47407403084571,48.4964880705508 L -121.46850003949314,48.49364048316217 L -121.46325303651128,48.490227699365505 L -121.46210383812719,48.489296404293924 L -121.46025263558363,48.48847889459784 L -121.45496879212331,48.48547307873825 L -121.45001621489303,48.4819480383877 L -121.44544574200269,48.47793995801754 L -121.44130428926854,48.473489980495586 L -121.4376343686232,48.46864378475546 L -121.43447365173137,48.463451116903116 L -121.43410468856523,48.46267829559049 L -121.43097640432524,48.460257473449225 L -121.41667626264176,48.46082596473966 L -121.41089053495212,48.460776776061024 L -121.4051364463968,48.4601705340942 L -121.39946750530727,48.459012876395136 L -121.39393642821298,48.457314568236434 L -121.38859464962069,48.4550914024991 L -121.38349184371539,48.452364052811646 L -121.37949155968472,48.449971137985486 L -121.3748198238956,48.44687133137977 L -121.37045782379668,48.44334899246745 L -121.3667610090251,48.439744328361755 L -121.3626408751089,48.439937575807576 L -121.36056001216222,48.44041303932174 L -121.354681783301,48.44145158859084 L -121.34872945020437,48.441900910119465 L -121.3427619280057,48.44175655660313 L -121.33683828217737,48.44101995682723 L -121.33101714391128,48.43969840152523 L -121.32535612979768,48.43780497121624 L -121.32438088979478,48.43742386324097 L -121.32412001959052,48.437381965979334 L -121.32186641200641,48.43823834804272 L -121.3197946730126,48.438751283398695 L -121.31678519277038,48.44073153113309 L -121.31113393444306,48.44364471885579 L -121.29884140568393,48.449177893962634 L -121.29342667147698,48.45130587975289 L -121.28783129567816,48.45289944171445 L -121.28210788674131,48.4539435969697 L -121.27631025690458,48.45442852823457 L -121.2704929162407,48.454349676122014 L -121.26471056014614,48.453707782009865 L -121.25901755508794,48.4525088810704 L -121.25346702173826,48.451055709801615 L -121.25273138721724,48.451052243379685 L -121.24759419534287,48.45058495620613 L -121.24251612613573,48.44967814267295 L -121.23753471371785,48.448338505416224 L -121.2291166502785,48.44568171827957 L -121.22355407236103,48.44361855183469 L -121.21822244720718,48.4410161102722 L -121.21317390467658,48.43789983891306 L -121.20845780679403,48.4343002070389 L -121.2041202651134,48.43025240997926 L -121.20020368986278,48.42579602498989 L -121.19674637527928,48.42097462428699 L -121.19454360419628,48.41715556496493 L -121.19357689229358,48.41726881390254 L -121.18357367911761,48.418322220914796 L -121.18136560456846,48.41909774094178 L -121.17619885942169,48.420655198479544 L -121.17091307602408,48.42174213144299 L -121.16555101185482,48.422349747471536 L -121.16015604143946,48.42247313146978 L -121.15477180548707,48.422111285366405 L -121.14828656839171,48.42138130297512 L -121.14371975313249,48.42288731236768 L -121.1403871696866,48.42381383582676 L -121.13750871040205,48.425104225391685 L -121.131945826913,48.42727247827271 L -121.12619499271801,48.42887712755861 L -121.12031315224705,48.42990228410428 L -121.11435854714671,48.43033779686849 L -121.10839013957472,48.430179353428855 L -121.10246702836014,48.42942852268322 L -121.09664786381008,48.42809273931451 L -121.09202588736544,48.426534398812265 L -121.08869323031904,48.42597185680815 L -121.08265522288464,48.42462931704451 L -121.07678752054208,48.42267227437401 L -121.07115248384126,48.42012152778231 L -121.06581000061666,48.41700418599965 L -121.06453337812043,48.41607076705357 L -121.05784391438107,48.414250330050294 L -121.05355527287911,48.41290885461898 L -121.04937905464772,48.41125017937985 L -121.03808024674527,48.40626502461851 L -121.03284913659621,48.40364524265407 L -121.02789803903649,48.40052844880671 L -121.02327402768643,48.39694427666139 L -121.01902106632416,48.39292680350232 L -121.0151795908906,48.3885142263159 L -121.01178612503607,48.38374849862467 L -121.00887293286388,48.37867493160523 L -121.00646771217248,48.37334176328251 L -121.00459333111266,48.36779969989642 L -121.00326761076374,48.36210143380144 L -121.00324733671309,48.361947604747336 L -121.00254217401447,48.36153142981447 L -120.99953686671364,48.35932232376434 L -120.98975232667713,48.3573625208009 L -120.98397277491686,48.35590290489763 L -120.97836658077713,48.35387700154295 L -120.97298908003704,48.35130480735802 L -120.96789335116395,48.34821171111205 L -120.96312969140413,48.344628243123154 L -120.95874512032512,48.34058977390972 L -120.95478291571045,48.33613616506674 L -120.95128218638759,48.33131137581302 L -120.94827748620526,48.32616302909305 L -120.94579847297076,48.320741941516005 L -120.9438696157134,48.31510162177192 L -120.94250995316362,48.30929774247567 L -120.94173290583176,48.303387590652 L -120.94100782465117,48.29445902487577 L -120.94098025225611,48.293565938187946 L -120.93922140279304,48.291825951794706 L -120.93560624733307,48.28750659258125 L -120.93241206206642,48.282867261433665 L -120.93099337071175,48.280584230058096 L -120.92982528694499,48.27987502681154 L -120.92510754093095,48.27669507337121 L -120.92071219911321,48.27308256708874 L -120.9166787820018,48.269069989645146 L -120.91304355587766,48.2646934199425 L -120.90983920670624,48.25999220970225 L -120.90709454624293,48.25500862963599 L -120.90525826455371,48.25076693038523 L -120.90058457638422,48.24803123021574 L -120.89954672377874,48.24726770364778 L -120.89677788809712,48.2462301241274 L -120.89154419323273,48.24366494543487 L -120.88658408516451,48.240604058045605 L -120.88194437037063,48.23707634628163 L -120.87766883191111,48.23311509969257 L -120.87379781626527,48.228757698915985 L -120.87036785259873,48.22404526293185 L -120.867411308053,48.21902226103983 L -120.86513409530711,48.2141193600465 L -120.86468650490212,48.2136725249651 L -120.86090025573371,48.20905367903613 L -120.85759195320873,48.20408129983859 L -120.8547943765588,48.1988046545626 L -120.85288876677052,48.19414117724706 L -120.85224555884777,48.192939926147744 L -120.85001283296258,48.18758181996419 L -120.84830831754498,48.18203303685941 L -120.84714796608596,48.176345510925906 L -120.84654263894876,48.170572474826095 L -120.84649800172076,48.164767961558766 L -120.84666277222424,48.16072191325916 L -120.84717073617529,48.15500196895415 L -120.84822318854269,48.1493567821333 L -120.84981048892409,48.14383806237895 L -120.8519180977405,48.13849636084246 L -120.85452670941795,48.133380607198646 L -120.8576124292255,48.128537661452796 L -120.8611469921495,48.12401188470574 L -120.86509802179954,48.119844732808765 L -120.86942932697424,48.11607437663042 L -120.87410123317059,48.112735352413615 L -120.87907094600007,48.109858245425606 L -120.88429294318274,48.107469409798746 L -120.88720188847576,48.10646230814345 L -120.88752339936859,48.10380920954492 L -120.88877349545488,48.09814667200729 L -120.89056438712073,48.0926312591822 L -120.89287934590999,48.087314489731284 L -120.89569674813237,48.0822460268176 L -120.89899027684724,48.07747321420926 L -120.90272916768633,48.0730406340483 L -120.90687849621911,48.068989690415364 L -120.91139950417677,48.06535822258035 L -120.9162499614876,48.06218015155126 L -120.92097534458442,48.059400752633216 L -120.92277892064567,48.05816457453294 L -120.9275996187771,48.05518492729772 L -120.93267993773449,48.05267339640763 L -120.93797455341259,48.05065238850513 L -120.94078001847357,48.04987549550613 L -120.94307306197433,48.04776940069474 L -120.94789449051856,48.04416076919588 L -120.95305338161329,48.04105364235995 L -120.95849776173144,48.038479323104134 L -120.96417278117137,48.03646374654687 L -120.96651590414687,48.03588821953819 L -120.97034408828188,48.03287314237944 L -120.97500703982459,48.029854503851716 L -120.97992917217516,48.027280031328225 L -120.9908883532209,48.02217940644838 L -120.99621167400987,48.020005726641756 L -121.00171862204566,48.01835159501002 L -121.00735862116855,48.01723220320288 L -121.01307987326918,48.01665783178564 L -121.01882983400574,48.01663375582179 L -121.02080453140411,48.016720200012045 L -121.02105197191503,48.01659959822005 L -121.02643389581782,48.01429425273663 L -121.03201488709274,48.01252443379598 L -121.03774180273054,48.01130699387101 L -121.04356011020901,48.010653525601455 L -121.05198714180148,48.010121634688005 L -121.05365375874266,48.00908865225641 L -121.05751433781877,48.00718116851738 L -121.05719597928137,48.001465829980994 L -121.05746724039454,47.99543766183637 L -121.05771141932321,47.99288254584322 L -121.05851184501564,47.98729641418293 L -121.05983354215661,47.98181018917087 L -121.06166481903222,47.97647240187461 L -121.06398947619357,47.97133027028378 L -121.06678694975685,47.96642928162046 L -121.07003249331116,47.96181278995986 L -121.07369739682491,47.95752163272082 L -121.07774924061434,47.95359376941854 L -121.08215218212756,47.95006394587529 L -121.08458351612943,47.9484651443352 L -121.08791161279319,47.945183389688886 L -121.09341248063566,47.94038295109888 L -121.09441090531477,47.93970610208291 L -121.09580174740263,47.93832153158434 L -121.10031143565631,47.93462999019145 L -121.10515799451167,47.931393381895376 L -121.1102956983472,47.928642242985944 L -121.11567607468832,47.92640252951575 L -121.12124836152852,47.92469537241364 L -121.1269599862515,47.923536878121716 L -121.13275706163584,47.92293797663651 L -121.13624262345,47.92291784601293 L -121.14105751536366,47.92210906022096 L -121.14675372514901,47.921702756077764 L -121.15246276177129,47.92183983204571 L -121.15281343388752,47.92188183828701 L -121.15723004749111,47.920040632025724 L -121.16280310158784,47.91833025482635 L -121.16851583486813,47.917168789976536 L -121.17431431861866,47.916567201808405 L -121.18014381463395,47.91653116936808 L -121.18536593129264,47.91675281517685 L -121.18545393390718,47.9167336429821 L -121.19051133940448,47.915856545146326 L -121.19562520412283,47.91541491337654 L -121.20075810217087,47.91541197976108 L -121.2038203489873,47.91554133144286 L -121.20719280824856,47.91324741995707 L -121.21985515025933,47.90571940719083 L -121.22512854874601,47.902929337904254 L -121.2306529612295,47.90067676494525 L -121.23637376781558,47.89898395951407 L -121.24223440686059,47.89786765838659 L -121.24817693419664,47.89733889843769 L -121.25414259602586,47.89740290751967 L -121.26007240981956,47.898059052774414 L -121.26590774747872,47.89930084689047 L -121.2715909149904,47.90111601224312 L -121.27717960282413,47.90321252881209 L -121.28280314305871,47.90565676129218 L -121.2881478526507,47.90866218248796 L -121.2931579113385,47.91219740371603 L -121.29718516282027,47.915706354417054 L -121.29929082397848,47.91491407279355 L -121.29992921823548,47.914751482019604 L -121.30340888324739,47.91262945462142 L -121.30985546046733,47.909677738864886 L -121.31307924890471,47.908426376053995 L -121.31858609746715,47.906590034665086 L -121.32424462953513,47.90529444768542 L -121.33000187884238,47.90455174236003 L -121.33580395508731,47.90436887072658 L -121.3415965483691,47.904747544541316 L -121.34319723499611,47.90500925717747 L -121.34412066943138,47.90467006773271 L -121.35022382883908,47.903130764510294 L -121.35250956387551,47.902680294827185 L -121.35276885632304,47.9025927967086 L -121.35346777813227,47.902370158074845 L -121.35369801769974,47.90225836916162 L -121.35953694630753,47.900137415599744 L -121.36556388093805,47.898631563595764 L -121.37171421363675,47.89775695568709 L -121.37792201363783,47.89752296755666 L -121.38193880651289,47.897579675532256 L -121.38798848759927,47.89797138107063 L -121.39396776130569,47.898971561264126 L -121.39981558576754,47.90057000536358 L -121.40547226107368,47.90275039499773 L -121.41088003873739,47.90549047076595 L -121.4159837112455,47.90876225948249 L -121.41719416807358,47.90972351863241 L -121.41789948979236,47.90944993962564 L -121.42339400238707,47.907900671100755 L -121.4290108844718,47.906880603817726 L -121.43469928781472,47.90639897219112 L -121.44040771671995,47.90646013631185 L -121.44608449420512,47.9070635424764 L -121.45692074956715,47.90873995301528 L -121.46048696914008,47.90869298638422 L -121.46620521076919,47.9088905124362 L -121.47187863759157,47.909631816665964 L -121.47745565719117,47.91091015787854 L -121.48288555384983,47.91271391122818 L -121.48811894974062,47.91502667393113 L -121.49310825395482,47.917827414427556 L -121.49780809527901,47.92109066363616 L -121.50217573478722,47.924786746562226 L -121.50617145449561,47.92888205215302 L -121.50975891854578,47.93333933894644 L -121.51034181199813,47.93422458219956 L -121.5109018000079,47.93458353868884 L -121.51545732433104,47.93816809713275 L -121.51964568181431,47.94217553056188 L -121.52342777895592,47.9465684341925 L -121.52676831422734,47.95130580531971 L -121.52963610757237,47.95634342603059 L -121.53200439143727,47.961634275927516 L -121.53385106061494,47.96712897100957 L -121.53515887857172,47.972776224615394 L -121.53591563833056,47.978523326125185 L -121.53611427640918,47.984316632953785 L -121.53575293874948,47.99010207124247 L -121.53483499802307,47.995825640576314 L -121.5333690221513,48.00143391801595 L -121.5313686943335,48.00687455673928 L -121.52885268533008,48.012096774638906 L -121.52584447919243,48.0170518283147 L -121.52237215406635,48.02169346803739 L -121.52196941028338,48.022180850620366 L -121.52052381448651,48.024739843249804 L -121.52055859871729,48.02547593234203 L -121.52032278381226,48.030906131992886 L -121.52000196667856,48.03451314776029 L -121.5197009559279,48.03667950558872 L -121.52327096024844,48.038529272392104 L -121.53865540320066,48.047442606467506 L -121.54354545568788,48.050602220165345 L -121.54810625984963,48.05422089912947 L -121.55229487346975,48.05826457171619 L -121.55501191089371,48.06145179284744 L -121.56615603066815,48.06035334538889 L -121.5720289447639,48.0600639872181 L -121.57790197998133,48.060350876503364 L -121.57858125148157,48.060451350561955 L -121.58238230410545,48.059339786364205 L -121.5878324431868,48.058280676572934 L -121.59092981873926,48.05782632326335 L -121.59157915781871,48.05761848168065 L -121.59754640186488,48.05582000603398 L -121.60379337066615,48.05429706489609 L -121.61016744023837,48.05345136636696 L -121.61504938413108,48.05333080305145 L -121.61636848147224,48.052660098073325 L -121.62152508587612,48.050613088429984 L -121.62684872453934,48.049051138097376 L -121.63229387914248,48.047987602105515 L -121.63781399237723,48.047431573928634 L -121.64336186602199,48.0473878077339 L -121.64510950948771,48.04745488294581 L -121.64855537182389,48.04740401452487 L -121.65173696979743,48.047441422788054 L -121.65903079455582,48.04772073334361 L -121.66482121727425,48.048224199402206 L -121.670535757175,48.04928556976787 L -121.67612078889923,48.05089488451947 L -121.68152390239547,48.0530370418146 L -121.68669439473662,48.05569193960556 L -121.69158374591723,48.05883466427741 L -121.69614607416622,48.06243572443781 L -121.70033856650238,48.06646132766486 L -121.70412188049222,48.07087369761587 L -121.7074605134402,48.07563142852137 L -121.71032313554672,48.0806898737376 L -121.71268288390762,48.08600156471162 L -121.71285333313908,48.086513924999785 L -121.71601818001326,48.08973553319379 L -121.71903641250313,48.09350498192102 L -121.71968710629935,48.093969876103166 L -121.72416839712358,48.09789810422052 L -121.7282379095878,48.10225149507099 L -121.73185549914572,48.10698710373131 L -121.73498547933794,48.11205821481171 L -121.7375969738283,48.11741480328947 L -121.73966422099005,48.123004027991136 L -121.74116682803651,48.128770752855154 L -121.74208997219013,48.134658090832964 L -121.74242454690472,48.14060796506304 L -121.7421672516989,48.14656168178207 L -121.74205930994844,48.14731376006673 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7fafedca49e8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx, miny, maxx, maxy = region.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojShapefile(sourcepath, outpath=None, \n",
    "                    newprojdictionary={'proj': 'longlat', 'ellps': 'WGS84', 'datum': 'WGS84'}):\n",
    "    \"\"\"\n",
    "    Convert a shapefile into a new projection\n",
    "    \n",
    "    sourcepath: (dir) the path to the .shp file\n",
    "    newprojdictionary: (dict) the new projection definitions (default is longlat projection with WGS84 datum)\n",
    "    outpath: (dir) the output path for the new shapefile\n",
    "    \"\"\"\n",
    "    # if outpath is none, treat the reprojection as a file replacement\n",
    "    if isinstance(outpath, type(None)):\n",
    "        outpath = sourcepath\n",
    "        \n",
    "    shpfile = gpd.GeoDataFrame.from_file(sourcepath)\n",
    "    shpfile = shpfile.to_crs(newprojdictionary)\n",
    "    shpfile.to_file(outpath)\n",
    "    \n",
    "    \n",
    "def getFullShape(shapefile):\n",
    "    \"\"\"\n",
    "    Generate a MultiPolygon to represent each shape/polygon within the shapefile\n",
    "    \n",
    "    shapefile: (dir) a path to the ESRI .shp shapefile\n",
    "    \"\"\"\n",
    "    shp = fiona.open(shapefile)\n",
    "    mp = [shape(pol['geometry']) for pol in shp]\n",
    "    mp = MultiPolygon(mp)\n",
    "    shp.close()\n",
    "    return(mp)\n",
    "\n",
    "\n",
    "def readShapefileTable(shapefile):\n",
    "    \"\"\"\n",
    "    Read in the datatable captured within the shapefile properties\n",
    "    \n",
    "    shapefile: (dir) a path to the ESRI .shp shapefile\n",
    "    \"\"\"\n",
    "    shp = fiona.open(shapefile)\n",
    "    centroid = [eachpol['properties'] for eachpol in shp]\n",
    "    cent_df = pd.DataFrame.from_dict(centroid, orient='columns')\n",
    "    shp.close()\n",
    "    return(cent_df)\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.geometry import MultiPolygon, Polygon, box, point, shape\n",
    "import dask as da\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 4)\n",
      "   FID       LAT      LONG_    ELEV\n",
      "0    0  48.53125 -121.59375  1113.0\n",
      "1    1  48.46875 -121.46875   646.0\n",
      "2    2  48.46875 -121.53125   321.0\n",
      "3    3  48.46875 -121.59375   164.0\n",
      "4    4  48.46875 -121.65625   369.0\n",
      "/home/jovyan/work/notebooks/data/70b977e22af544f8a7e5a803935c329c/70b977e22af544f8a7e5a803935c329c/data/contents/Sauk_mappingfile.csv\n",
      "CPU times: user 7.77 s, sys: 162 ms, total: 7.93 s\n",
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mappingfile1=treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                          mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))\n",
    "print(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 4)\n",
      "   FID       LAT      LONG_    ELEV\n",
      "0    0  48.53125 -121.59375  1113.0\n",
      "1    1  48.46875 -121.46875   646.0\n",
      "2    2  48.46875 -121.53125   321.0\n",
      "3    3  48.46875 -121.59375   164.0\n",
      "4    4  48.46875 -121.65625   369.0\n",
      "/home/jovyan/work/notebooks/data/70b977e22af544f8a7e5a803935c329c/70b977e22af544f8a7e5a803935c329c/data/contents/Sauk_mappingfile.csv\n",
      "CPU times: user 19.5 s, sys: 440 ms, total: 19.9 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# map the mappingfiles from usecase1\n",
    "mappingfile1=ogh.treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                              mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))\n",
    "print(mappingfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Watershed</th>\n",
       "      <th>Sauk-Suiattle river</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median elevation in meters [range](Number of gridded cells)</th>\n",
       "      <th>1171[164-2216] (n=99)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(Sauk-Suiattle river, 1171[164-2216] (n=99))]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Compare Hydrometeorology \n",
    "\n",
    "This section performs computations and generates plots of the Livneh 2013 and Salathe 2014 mean temperature and mean total monthly precipitation in order to compare them with each other. The generated plots are automatically downloaded and saved as .png files within the \"homedir\" directory.\n",
    "\n",
    "Let's compare the Livneh 2013 and Salathe 2014 using the period of overlapping history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function getDailyWRF_salathe2014 in module ogh.ogh:\n",
      "\n",
      "getDailyWRF_salathe2014(homedir, mappingfile, subdir='salathe2014/WWA_1950_2010/raw', catalog_label='dailywrf_salathe2014')\n",
      "    Get the Salathe el al., 2014 raw Daily WRF files of interest using the reference mapping file\n",
      "    \n",
      "    homedir: (dir) the home directory to be used for establishing subdirectories\n",
      "    mappingfile: (dir) the file path to the mappingfile, which contains LAT, LONG_, and ELEV coordinates of interest\n",
      "    subdir: (dir) the subdirectory to be established under homedir\n",
      "    catalog_label: (str) the preferred name for the series of catalogged filepaths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ogh.getDailyWRF_salathe2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_x_dailywrf_Salathe2014 in module ogh_xarray_landlab:\n",
      "\n",
      "get_x_dailywrf_Salathe2014(homedir, spatialbounds, subdir='salathe2014/Daily_WRF_1970_1999/noBC', nworkers=4, start_date='1970-01-01', end_date='1989-12-31', rename_timelatlong_names={'LAT': 'LAT', 'LON': 'LON'}, file_prefix='sp_', replace_file=True)\n",
      "    get Daily WRF data from Salathe et al. (2014) using xarray on netcdf files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(oxl.get_x_dailywrf_Salathe2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF retrieval and clipping to a spatial extent\n",
    "\n",
    "The function get_x_dailywrf_salathe2014 retrieves and clips NetCDF files archived within the UW Rocinante NNRP repository. This archive contains daily data from January 1970 through December 1999 (30 years). Each netcdf file is comprised of meteorologic and VIC hydrologic outputs for a calendar month. The expected number of files would be 360 files (12 months for 30 years). \n",
    "\n",
    "In the code chunk below, 20 parallel workers will be initialized to distribute file retrieval and spatial clipping tasks. For each worker, they will wget the requested file, clip the netcdf file to gridded cell centroids within the the provided bounding box, then return the location of the spatially clipped output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gridded data files:99\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1151.040404040404m\n",
      "Maximum elevation: 2216.0m\n",
      "[########################################] | 100% Completed | 14min  6.7s\n",
      "CPU times: user 10min 54s, sys: 3min 39s, total: 14min 34s\n",
      "Wall time: 14min 7s\n"
     ]
    }
   ],
   "source": [
    "maptable, nstations = ogh.mappingfileToDF(mappingfile1)\n",
    "spatialbounds = {'minx':maptable.LONG_.min(), 'maxx':maptable.LONG_.max(),\n",
    "                 'miny':maptable.LAT.min(), 'maxy':maptable.LAT.max()}\n",
    "\n",
    "outputfiles = oxl.get_x_dailywrf_Salathe2014(homedir=homedir,\n",
    "                                             subdir='salathe2014/Daily_WRF_1970_1999/noBC_netcdf',\n",
    "                                             spatialbounds=spatialbounds,\n",
    "                                             nworkers=20,\n",
    "                                             start_date='1970-01-01', end_date='1999-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outfiledict = oxl.netcdf_to_ascii(homedir=homedir, \n",
    "                                  subdir='salathe2014/Daily_WRF_1970_1999/noBC_ascii', \n",
    "                                  mappingfile=mappingfile1,\n",
    "                                  netcdfs=outputfiles,\n",
    "                                  catalog_label='sp_WRF_NNRP_noBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(outfiledict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vars = [ds_var for ds_var in dict(ds_mf.variables).keys() \n",
    "           if ds_var not in ['YEAR','MONTH','DAY','TIME','LAT','LON']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ogh.mappingfileSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ogh.getDailyMET_livneh2013(homedir, mappingfile1)\n",
    "ogh.getDailyWRF_salathe2014(homedir, mappingfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of climate variables for the long-term mean (ltm).\n",
    "#### INPUT: gridded meteorology ASCII files located from the Sauk-Suiattle Mapping file. The inputs to gridclim_dict() include the folder location and name of the hydrometeorology data, the file start and end, the analysis start and end, and the elevation band to be included in the analsyis (max and min elevation). <br/>OUTPUT: dictionary of dataframes where rows are temporal summaries and columns are spatial summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Livneh et al., 2013\n",
    "dr1 = meta_file['dailymet_livneh2013']\n",
    "\n",
    "# Salathe et al., 2014\n",
    "dr2 = meta_file['dailywrf_salathe2014']\n",
    "\n",
    "# define overlapping time window\n",
    "dr = ogh.overlappingDates(date_set1=tuple([dr1['start_date'], dr1['end_date']]), \n",
    "                          date_set2=tuple([dr2['start_date'], dr2['end_date']]))\n",
    "dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        variable_list=['PRECIP'],\n",
    "                        dataset='dailymet_livneh2013',\n",
    "                        subset_start_date=dr[0],\n",
    "                        subset_end_date=dr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total monthly and yearly precipitation, as well as the mean values across time and across stations\n",
    "#### INPUT: daily precipitation for each station from the long-term mean dictionary (ltm) <br/>OUTPUT: Append the computed dataframes and values into the ltm dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='PRECIP_dailymet_livneh2013',\n",
    "                                   start_date=dr[0],\n",
    "                                   end_date=dr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat spatial-temporal computations with Salathe et al 2014 WRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        variable_list=['PRECIP'],\n",
    "                        dataset='dailywrf_salathe2014',\n",
    "                        subset_start_date=dr[0],\n",
    "                        subset_end_date=dr[1],\n",
    "                        df_dict=ltm)\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='PRECIP_dailywrf_salathe2014',\n",
    "                                   start_date=dr[0],\n",
    "                                   end_date=dr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the Livneh et al. 2013 raw MET and Salathe et al. 2014 raw WRF dataframes and values within ltm\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of outputs\n",
    "files=[]\n",
    "\n",
    "# create the destination path for the dictionary of dataframes\n",
    "ltm_sauk=os.path.join(homedir, 'ltm_sauk.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=ltm, outfilepath=ltm_sauk)\n",
    "files.append(ltm_sauk)\n",
    "\n",
    "# append the mapping file for Sauk-Suiattle gridded cell centroids\n",
    "files.append(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitations\"\n",
    "\n",
    "#### INPUT: dataframe with each month as a row and each station as a column. <br/>OUTPUT: A png file that represents the distribution across stations (in Wateryear order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two lowest elevation locations\n",
    "lowE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=164)\n",
    "\n",
    "# one highest elevation location\n",
    "highE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=2216)\n",
    "\n",
    "# combine references together\n",
    "reference_lines = highE_ref + lowE_ref\n",
    "reference_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider value range when comparing Livneh to Salathe \n",
    "vr = ogh.valueRange([ltm['meanbymonthsum_PRECIP_dailymet_livneh2013'], \n",
    "                     ltm['meanbymonthsum_PRECIP_dailywrf_salathe2014']])\n",
    "\n",
    "print(vr.min(), vr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp, nstation = ogh.mappingfileToDF(mappingfile1)\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# #Higher resolution children gridded cells \n",
    "# #get data from Lower resolution parent grid cells to the children\n",
    "# \"\"\"\n",
    "# import landlab as L2\n",
    "\n",
    "# watershed_dem_sc = os.path.join(homedir, 'DEM_10m.asc')\n",
    "# (rmg_sc, z_sc) = L2.io.read_esri_ascii(watershed_dem_sc, name='topographic__elevation')\n",
    "# rmg_sc.set_watershed_boundary_condition(z_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test0=pd.read_table(watershed_dem_sc, nrows=5, sep='\\s+', header=None).set_index(0)[1].to_dict()\n",
    "# print(test0)\n",
    "\n",
    "# test1 = pd.read_table(watershed_dem_sc, \n",
    "#                       skiprows=6, \n",
    "#                       nrows=test0['nrows'],\n",
    "#                       sep='\\s+',\n",
    "#                       header=None)\n",
    "# print(test1.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.unstack().as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use('tableau-colorblind10')\n",
    "\n",
    "import shapely.ops\n",
    "from shapely.geometry import MultiPolygon, shape, point, box, Polygon\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculateUTMbounds(mappingfile, mappingfile_crs={'init':'epsg:4326'}, spatial_resolution=0.06250):\n",
    "    # read in the mappingfile\n",
    "    map_df, nstation = ogh.mappingfileToDF(mappingfile)\n",
    "\n",
    "    # loop though each LAT/LONG_ +/-0.06250 centroid into gridded cells\n",
    "    geom=[]\n",
    "    midpt = spatial_resolution\n",
    "    for ind in map_df.index:\n",
    "        mid = map_df.loc[ind]\n",
    "        geom.append(box(mid.LONG_- midpt, mid.LAT - midpt, mid.LONG_ + midpt, mid.LAT + midpt, ccw=True))\n",
    "\n",
    "    # generate the GeoDataFrame\n",
    "    test = gpd.GeoDataFrame(map_df, crs=mappingfile_crs, geometry=geom)\n",
    "\n",
    "    # compile gridded cells to extract bounding box\n",
    "    test['shapeName'] = 1\n",
    "\n",
    "    # dissolve shape into new shapefile\n",
    "    newShape = test.dissolve(by='shapeName').reset_index()\n",
    "    newShape.bounds\n",
    "\n",
    "    # take the minx and miny, and centroid_x and centroid_y\n",
    "    minx, miny, maxx, maxy = newShape.bounds.loc[0]\n",
    "    lon0, lat0 = np.array(newShape.centroid[0])\n",
    "\n",
    "    # generate the basemap raster\n",
    "    fig = plt.figure(figsize=(10,10), dpi=500)\n",
    "    ax1 = plt.subplot2grid((1,1),(0,0))\n",
    "    m = Basemap(projection='tmerc', resolution='h', ax=ax1, lat_0=lat0, lon_0=lon0,\n",
    "                llcrnrlon=minx, llcrnrlat=miny, urcrnrlon=maxx, urcrnrlat=maxy)\n",
    "\n",
    "    # transform each polygon to the utm basemap projection\n",
    "    for ind in newShape.index:\n",
    "        eachpol = newShape.loc[ind]\n",
    "        newShape.loc[ind,'g2'] = shapely.ops.transform(m, eachpol['geometry'])\n",
    "    \n",
    "    # remove the plot\n",
    "    plt.gcf().clear()\n",
    "\n",
    "    # establish the UTM basemap bounding box dimensions\n",
    "    minx2, miny2, maxx2, maxy2 = newShape['g2'].iloc[0].bounds\n",
    "    return(minx2, miny2, maxx2, maxy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx2, miny2, maxx2, maxy2 = calculateUTMbounds(mappingfile=mappingfile1,\n",
    "                                                mappingfile_crs={'init':'epsg:4326'},\n",
    "                                                spatial_resolution=0.06250)\n",
    "\n",
    "minx2, miny2, maxx2, maxy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # def mappingfileToUTM (mappingfile, spatial_resolution, starting_shape_projection={'init':'epsg:4326'}):\n",
    "\n",
    "# # read in the mappingfile\n",
    "# #map_df, nstation = ogh.mappingfileToDF(mappingfile)\n",
    "# map_df, nstation = ogh.mappingfileToDF(mappingfile1)\n",
    "\n",
    "# # turn mapping file coordinates into LatLong gridded cells\n",
    "# #midpt = spatial_resolution\n",
    "# midpt = 0.06250\n",
    "# geom=[]\n",
    "# for ind in map_df.index:\n",
    "#     mid = map_df.loc[ind]\n",
    "#     geom.append(box(mid.LONG_- midpt, mid.LAT - midpt, mid.LONG_ + midpt, mid.LAT + midpt, ccw=True))\n",
    "\n",
    "# # assemble geodataframe for each cell\n",
    "# test = gpd.GeoDataFrame(map_df, crs={'init':'epsg:4326'}, geometry=geom)\n",
    "\n",
    "# # dissolve into a single shape polygon\n",
    "# test['shapeName'] = 1\n",
    "# newShape = test.dissolve(by='shapeName').reset_index()\n",
    "# del test['shapeName']\n",
    "\n",
    "# # take the bounding box and centroids coordinates\n",
    "# minx, miny, maxx, maxy = newShape.bounds.iloc[0]\n",
    "# lon0, lat0 = np.array(newShape.centroid[0])\n",
    "# print(minx, miny, maxx, maxy)\n",
    "\n",
    "# # generate the basemap raster\n",
    "# fig = plt.figure(figsize=(10,10), dpi=500)\n",
    "# ax1 = plt.subplot2grid((1,1),(0,0))\n",
    "# m = Basemap(projection='tmerc', resolution='h', ax=ax1, lat_0=lat0, lon_0=lon0,\n",
    "#             llcrnrlon=minx, llcrnrlat=miny, urcrnrlon=maxx, urcrnrlat=maxy)\n",
    "    \n",
    "# # transform each polygon to the utm basemap projection\n",
    "# for ind in newShape.index:\n",
    "#     eachpol = newShape.loc[ind]\n",
    "#     newShape.loc[ind,'geometry'] = shapely.ops.transform(m, eachpol['geometry'])\n",
    "\n",
    "# # print the bounds by the new UTM projection\n",
    "# minx, miny, maxx, maxy = newShape.bounds.iloc[0]\n",
    "# print(minx, miny, maxx, maxy)\n",
    "\n",
    "# # transform each polygon to the utm basemap projection\n",
    "# for ind in test.index:\n",
    "#     eachpol = test.loc[ind]\n",
    "#     test.loc[ind,'geometry'] = shapely.ops.transform(m, eachpol['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = gpd.GeoDataFrame(test, crs={'init':m.projection}, geometry=test['geometry'])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test3 = tmp.plot(column='ELEV', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx2, miny2, maxx2, maxy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(maxx-minx)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = r.RasterModelGrid((len(row_list), len(col_list)), spacing=(dy, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import landlab.grid.raster as r\n",
    "\n",
    "def rasterDimensions (maxx, maxy, minx=0, miny=0, dy=100, dx=100):\n",
    "    # construct the range\n",
    "    x = pd.Series(range(int(minx),int(maxx)+1,1))\n",
    "    y = pd.Series(range(int(miny),int(maxy)+1,1))\n",
    "    \n",
    "    # filter for values that meet the increment or is the last value\n",
    "    cols = pd.Series(x.index).apply(lambda x1: x[x1] if x1 % dx == 0 or x1==x[0] or x1==x.index[-1] else None)\n",
    "    rows = pd.Series(y.index).apply(lambda y1: y[y1] if y1 % dy == 0 or y1==y[0] or y1==y.index[-1] else None)\n",
    "    \n",
    "    # construct the indices\n",
    "    row_list = np.array(rows.loc[pd.notnull(rows)])\n",
    "    col_list = np.array(cols.loc[pd.notnull(cols)])\n",
    "    \n",
    "    # construct the raster\n",
    "    raster = r.RasterModelGrid((len(row_list), len(col_list)), spacing=(dy, dx))\n",
    "    raster.add_zeros\n",
    "    return(raster, row_list, col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a raster\n",
    "raster, t1, t2 = rasterDimensions (minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=100, dy=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.boundary_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappingfileToRaster(mappingfile, spatial_resolution=0.01250, approx_distance_m_x=6000):\n",
    "    # assess raster dimensions from mappingfile\n",
    "    mf, nstations = mappingfileToDF(mappingfile, colvar=None)\n",
    "    ncol = int((mf.LONG_.max()-mf.LONG_.min())/spatial_resolution +1)\n",
    "    nrow = int((mf.LAT.max()-mf.LAT.min())/spatial_resolution +1)\n",
    "    \n",
    "    # dimensions of the raster\n",
    "    row_list = [mf.LAT.min() + spatial_resolution*(station) for station in range(0,nrow,1)]    \n",
    "    col_list = [mf.LONG_.min() + spatial_resolution*(station) for station in range(0,ncol,1)]\n",
    "    \n",
    "    # initialize RasterModelGrid\n",
    "    raster = r.RasterModelGrid(nrow, ncol, dx=approx_distance_m_x)\n",
    "    raster.add_zeros\n",
    "\n",
    "    # initialize node list\n",
    "    df_list=[]\n",
    "\n",
    "    # loop through the raster nodes (bottom to top arrays)\n",
    "    for row_index, nodelist in enumerate(raster.nodes):\n",
    "        \n",
    "        # index bottom to top arrays with ordered Latitude\n",
    "        lat = row_list[row_index]\n",
    "        \n",
    "        # index left to right with ordered Longitude\n",
    "        for nodeid, long_ in zip(nodelist, col_list):\n",
    "            df_list.append([nodeid, lat, long_])\n",
    "\n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame.from_records(df_list).rename(columns={0:'nodeid',1:'LAT',2:'LONG_'})\n",
    "    \n",
    "    # identify raster nodeid and equivalent mappingfile FID\n",
    "    df = df.merge(mf[['FID','LAT','LONG_','ELEV']], how='outer', on=['LAT','LONG_'])\n",
    "    return(df, raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeXmap, raster = ogh.mappingfileToRaster(mappingfile=mappingfile1,\n",
    "                                           spatial_resolution=0.06250,\n",
    "                                           approx_distance_m_x=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PRECIP_dailymet_livneh2013'],\n",
    "                          vardf_dateindex=3,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PRECIP_dailywrf_salathe2014'],\n",
    "                          vardf_dateindex=3,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total files and image to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Computed spatial-temporal summaries of two gridded data product data sets for Sauk-Suiattle'\n",
    "abstract = 'This resource contains the computed summaries for the Meteorology data from Livneh et al. 2013 and the WRF data from Salathe et al. 2014.'\n",
    "keywords = ['Sauk-Suiattle', 'Livneh 2013', 'Salathe 2014','climate','hydromet','watershed', 'visualizations and summaries'] \n",
    "rtype = 'genericresource'\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_bc45\n",
    "models=[model for model in df.columns if model not in ['Date','Year','Month','Day']]\n",
    "time=time1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t=pd.DataFrame({'test':['terrific']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=pd.DataFrame({'test':['terrific']})\n",
    "t2=pd.DataFrame({'test':['terrific']})\n",
    "t3=pd.DataFrame({'test':['terrific']})\n",
    "t4=pd.DataFrame({'test':['terrific']})\n",
    "\n",
    "for somedf in (t1, t2, t3, t4):\n",
    "    if somedf in locals():\n",
    "        print(locals())\n",
    "    list(somedf.to_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [t1,t2,t2,t4]:\n",
    "    if each in globals().keys():\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = eval(t)\n",
    "str(y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
