{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve NetCDF and model gridded climate time-series for a watershed\n",
    "\n",
    "### Case study:  the Sauk-Suiattle Watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. HydroShare setup and preparation\n",
    "    2. Re-establish the paths to the mapping file\n",
    "    3. Compute daily, monthly, and annual temperature and precipitation statistics\n",
    "    4. Visualize precipitation results relative to the forcing data\n",
    "    5. Visualize the time-series trends\n",
    "    6. Save results back into HydroShare\n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ogh/ogh.py:12: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-13f5b2ce818e>\", line 4, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/__init__.py\", line 6, in <module>\n",
      "    from .rcmod import *\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/rcmod.py\", line 8, in <module>\n",
      "    from . import palettes, _orig_rc_params\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/palettes.py\", line 12, in <module>\n",
      "    from .utils import desaturate, set_hls_values, get_color_cycle\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/seaborn/utils.py\", line 11, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  mpl.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import seaborn as sns\n",
    "\n",
    "# data migration library\n",
    "from utilities import hydroshare\n",
    "import ogh\n",
    "import ogh_xarray_landlab as oxl\n",
    "\n",
    "# plotting and shape libraries\n",
    "%matplotlib inline\n",
    "\n",
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(meta_file['dailymet_livneh2013'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of gridded climate points for the watershed\n",
    "\n",
    "For visualization purposes, we will also remap the study site shapefile, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. Since the shapefile was previously migrated, we can select 'N' for no overwriting.\n",
    "\n",
    "In the usecase1 notebook, the treatgeoself function identified the gridded cell centroid coordinates that overlap with our study site. These coordinates were documented within the mapping file, which will be remapped here. In the usecase2 notebook, the downloaded files were cataloged within the mapping file, so we will use the mappingfileSummary function to characterize the files available for Sauk-Suiattle for each gridded data product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# map the mappingfiles from usecase1\n",
    "mappingfile1=ogh.treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                              mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Compare Hydrometeorology \n",
    "\n",
    "This section performs computations and generates plots of the Livneh 2013 and Salathe 2014 mean temperature and mean total monthly precipitation in order to compare them with each other. The generated plots are automatically downloaded and saved as .png files within the \"homedir\" directory.\n",
    "\n",
    "Let's compare the Livneh 2013 and Salathe 2014 using the period of overlapping history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ogh.getDailyWRF_salathe2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.get_x_dailywrf_Salathe2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF retrieval and clipping to a spatial extent\n",
    "\n",
    "The function get_x_dailywrf_salathe2014 retrieves and clips NetCDF files archived within the UW Rocinante NNRP repository. This archive contains daily data from January 1970 through December 1999 (30 years). Each netcdf file is comprised of meteorologic and VIC hydrologic outputs for a calendar month. The expected number of files would be 360 files (12 months for 30 years). \n",
    "\n",
    "In the code chunk below, 20 parallel workers will be initialized to distribute file retrieval and spatial clipping tasks. For each worker, they will wget the requested file, clip the netcdf file to gridded cell centroids within the the provided bounding box, then return the location of the spatially clipped output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maptable, nstations = ogh.mappingfileToDF(mappingfile1)\n",
    "spatialbounds = {'minx':maptable.LONG_.min(), 'maxx':maptable.LONG_.max(),\n",
    "                 'miny':maptable.LAT.min(), 'maxy':maptable.LAT.max()}\n",
    "\n",
    "outputfiles = oxl.get_x_dailywrf_Salathe2014(homedir=homedir,\n",
    "                                             subdir='salathe2014/Daily_WRF_1970_1999/noBC_netcdf',\n",
    "                                             spatialbounds=spatialbounds,\n",
    "                                             nworkers=40,\n",
    "                                             start_date='1970-01-01', end_date='1999-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outfiledict = oxl.netcdf_to_ascii(homedir=homedir, \n",
    "                                  subdir='salathe2014/Daily_WRF_1970_1999/noBC_ascii', \n",
    "                                  mappingfile=mappingfile1,\n",
    "                                  netcdfs=outputfiles,\n",
    "                                  meta_file=meta_file,\n",
    "                                  catalog_label='sp_WRF_NNRP_noBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(outfiledict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ogh.getDailyMET_livneh2013(homedir, mappingfile1)\n",
    "ogh.getDailyWRF_salathe2014(homedir, mappingfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of climate variables for the long-term mean (ltm).\n",
    "#### INPUT: gridded meteorology ASCII files located from the Sauk-Suiattle Mapping file. The inputs to gridclim_dict() include the folder location and name of the hydrometeorology data, the file start and end, the analysis start and end, and the elevation band to be included in the analsyis (max and min elevation). <br/>OUTPUT: dictionary of dataframes where rows are temporal summaries and columns are spatial summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Livneh et al., 2013\n",
    "dr1 = meta_file['dailymet_livneh2013']\n",
    "\n",
    "# Salathe et al., 2014\n",
    "dr2 = meta_file['dailywrf_salathe2014']\n",
    "\n",
    "# define overlapping time window\n",
    "dr = ogh.overlappingDates(date_set1=tuple([dr1['start_date'], dr1['end_date']]), \n",
    "                          date_set2=tuple([dr2['start_date'], dr2['end_date']]))\n",
    "dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        variable_list=['PRECIP'],\n",
    "                        dataset='dailymet_livneh2013',\n",
    "                        subset_start_date=dr[0],\n",
    "                        subset_end_date=dr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total monthly and yearly precipitation, as well as the mean values across time and across stations\n",
    "#### INPUT: daily precipitation for each station from the long-term mean dictionary (ltm) <br/>OUTPUT: Append the computed dataframes and values into the ltm dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='PRECIP_dailymet_livneh2013',\n",
    "                                   start_date=dr[0],\n",
    "                                   end_date=dr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat spatial-temporal computations with Salathe et al 2014 WRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        variable_list=['PRECIP'],\n",
    "                        dataset='dailywrf_salathe2014',\n",
    "                        subset_start_date=dr[0],\n",
    "                        subset_end_date=dr[1],\n",
    "                        df_dict=ltm)\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='PRECIP_dailywrf_salathe2014',\n",
    "                                   start_date=dr[0],\n",
    "                                   end_date=dr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the Livneh et al. 2013 raw MET and Salathe et al. 2014 raw WRF dataframes and values within ltm\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of outputs\n",
    "files=[]\n",
    "\n",
    "# create the destination path for the dictionary of dataframes\n",
    "ltm_sauk=os.path.join(homedir, 'ltm_sauk.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=ltm, outfilepath=ltm_sauk)\n",
    "files.append(ltm_sauk)\n",
    "\n",
    "# append the mapping file for Sauk-Suiattle gridded cell centroids\n",
    "files.append(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitations\"\n",
    "\n",
    "#### INPUT: dataframe with each month as a row and each station as a column. <br/>OUTPUT: A png file that represents the distribution across stations (in Wateryear order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two lowest elevation locations\n",
    "lowE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=164)\n",
    "\n",
    "# one highest elevation location\n",
    "highE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=2216)\n",
    "\n",
    "# combine references together\n",
    "reference_lines = highE_ref + lowE_ref\n",
    "reference_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider value range when comparing Livneh to Salathe \n",
    "vr = ogh.valueRange([ltm['meanbymonthsum_PRECIP_dailymet_livneh2013'], \n",
    "                     ltm['meanbymonthsum_PRECIP_dailywrf_salathe2014']])\n",
    "\n",
    "print(vr.min(), vr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp, nstation = ogh.mappingfileToDF(mappingfile1)\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# #Higher resolution children gridded cells \n",
    "# #get data from Lower resolution parent grid cells to the children\n",
    "# \"\"\"\n",
    "# import landlab as L2\n",
    "\n",
    "# watershed_dem_sc = os.path.join(homedir, 'DEM_10m.asc')\n",
    "# (rmg_sc, z_sc) = L2.io.read_esri_ascii(watershed_dem_sc, name='topographic__elevation')\n",
    "# rmg_sc.set_watershed_boundary_condition(z_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test0=pd.read_table(watershed_dem_sc, nrows=5, sep='\\s+', header=None).set_index(0)[1].to_dict()\n",
    "# print(test0)\n",
    "\n",
    "# test1 = pd.read_table(watershed_dem_sc, \n",
    "#                       skiprows=6, \n",
    "#                       nrows=test0['nrows'],\n",
    "#                       sep='\\s+',\n",
    "#                       header=None)\n",
    "# print(test1.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.unstack().as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use('tableau-colorblind10')\n",
    "\n",
    "import shapely.ops\n",
    "from shapely.geometry import MultiPolygon, shape, point, box, Polygon\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculateUTMbounds(mappingfile, mappingfile_crs={'init':'epsg:4326'}, spatial_resolution=0.06250):\n",
    "    # read in the mappingfile\n",
    "    map_df, nstation = ogh.mappingfileToDF(mappingfile)\n",
    "\n",
    "    # loop though each LAT/LONG_ +/-0.06250 centroid into gridded cells\n",
    "    geom=[]\n",
    "    midpt = spatial_resolution\n",
    "    for ind in map_df.index:\n",
    "        mid = map_df.loc[ind]\n",
    "        geom.append(box(mid.LONG_- midpt, mid.LAT - midpt, mid.LONG_ + midpt, mid.LAT + midpt, ccw=True))\n",
    "\n",
    "    # generate the GeoDataFrame\n",
    "    test = gpd.GeoDataFrame(map_df, crs=mappingfile_crs, geometry=geom)\n",
    "\n",
    "    # compile gridded cells to extract bounding box\n",
    "    test['shapeName'] = 1\n",
    "\n",
    "    # dissolve shape into new shapefile\n",
    "    newShape = test.dissolve(by='shapeName').reset_index()\n",
    "    newShape.bounds\n",
    "\n",
    "    # take the minx and miny, and centroid_x and centroid_y\n",
    "    minx, miny, maxx, maxy = newShape.bounds.loc[0]\n",
    "    lon0, lat0 = np.array(newShape.centroid[0])\n",
    "\n",
    "    # generate the basemap raster\n",
    "    fig = plt.figure(figsize=(10,10), dpi=500)\n",
    "    ax1 = plt.subplot2grid((1,1),(0,0))\n",
    "    m = Basemap(projection='tmerc', resolution='h', ax=ax1, lat_0=lat0, lon_0=lon0,\n",
    "                llcrnrlon=minx, llcrnrlat=miny, urcrnrlon=maxx, urcrnrlat=maxy)\n",
    "\n",
    "    # transform each polygon to the utm basemap projection\n",
    "    for ind in newShape.index:\n",
    "        eachpol = newShape.loc[ind]\n",
    "        newShape.loc[ind,'g2'] = shapely.ops.transform(m, eachpol['geometry'])\n",
    "    \n",
    "    # remove the plot\n",
    "    plt.gcf().clear()\n",
    "\n",
    "    # establish the UTM basemap bounding box dimensions\n",
    "    minx2, miny2, maxx2, maxy2 = newShape['g2'].iloc[0].bounds\n",
    "    return(minx2, miny2, maxx2, maxy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx2, miny2, maxx2, maxy2 = calculateUTMbounds(mappingfile=mappingfile1,\n",
    "                                                mappingfile_crs={'init':'epsg:4326'},\n",
    "                                                spatial_resolution=0.06250)\n",
    "\n",
    "minx2, miny2, maxx2, maxy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # def mappingfileToUTM (mappingfile, spatial_resolution, starting_shape_projection={'init':'epsg:4326'}):\n",
    "\n",
    "# # read in the mappingfile\n",
    "# #map_df, nstation = ogh.mappingfileToDF(mappingfile)\n",
    "# map_df, nstation = ogh.mappingfileToDF(mappingfile1)\n",
    "\n",
    "# # turn mapping file coordinates into LatLong gridded cells\n",
    "# #midpt = spatial_resolution\n",
    "# midpt = 0.06250\n",
    "# geom=[]\n",
    "# for ind in map_df.index:\n",
    "#     mid = map_df.loc[ind]\n",
    "#     geom.append(box(mid.LONG_- midpt, mid.LAT - midpt, mid.LONG_ + midpt, mid.LAT + midpt, ccw=True))\n",
    "\n",
    "# # assemble geodataframe for each cell\n",
    "# test = gpd.GeoDataFrame(map_df, crs={'init':'epsg:4326'}, geometry=geom)\n",
    "\n",
    "# # dissolve into a single shape polygon\n",
    "# test['shapeName'] = 1\n",
    "# newShape = test.dissolve(by='shapeName').reset_index()\n",
    "# del test['shapeName']\n",
    "\n",
    "# # take the bounding box and centroids coordinates\n",
    "# minx, miny, maxx, maxy = newShape.bounds.iloc[0]\n",
    "# lon0, lat0 = np.array(newShape.centroid[0])\n",
    "# print(minx, miny, maxx, maxy)\n",
    "\n",
    "# # generate the basemap raster\n",
    "# fig = plt.figure(figsize=(10,10), dpi=500)\n",
    "# ax1 = plt.subplot2grid((1,1),(0,0))\n",
    "# m = Basemap(projection='tmerc', resolution='h', ax=ax1, lat_0=lat0, lon_0=lon0,\n",
    "#             llcrnrlon=minx, llcrnrlat=miny, urcrnrlon=maxx, urcrnrlat=maxy)\n",
    "    \n",
    "# # transform each polygon to the utm basemap projection\n",
    "# for ind in newShape.index:\n",
    "#     eachpol = newShape.loc[ind]\n",
    "#     newShape.loc[ind,'geometry'] = shapely.ops.transform(m, eachpol['geometry'])\n",
    "\n",
    "# # print the bounds by the new UTM projection\n",
    "# minx, miny, maxx, maxy = newShape.bounds.iloc[0]\n",
    "# print(minx, miny, maxx, maxy)\n",
    "\n",
    "# # transform each polygon to the utm basemap projection\n",
    "# for ind in test.index:\n",
    "#     eachpol = test.loc[ind]\n",
    "#     test.loc[ind,'geometry'] = shapely.ops.transform(m, eachpol['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = gpd.GeoDataFrame(test, crs={'init':m.projection}, geometry=test['geometry'])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test3 = tmp.plot(column='ELEV', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx2, miny2, maxx2, maxy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(maxx-minx)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = r.RasterModelGrid((len(row_list), len(col_list)), spacing=(dy, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import landlab.grid.raster as r\n",
    "\n",
    "def rasterDimensions (maxx, maxy, minx=0, miny=0, dy=100, dx=100):\n",
    "    # construct the range\n",
    "    x = pd.Series(range(int(minx),int(maxx)+1,1))\n",
    "    y = pd.Series(range(int(miny),int(maxy)+1,1))\n",
    "    \n",
    "    # filter for values that meet the increment or is the last value\n",
    "    cols = pd.Series(x.index).apply(lambda x1: x[x1] if x1 % dx == 0 or x1==x[0] or x1==x.index[-1] else None)\n",
    "    rows = pd.Series(y.index).apply(lambda y1: y[y1] if y1 % dy == 0 or y1==y[0] or y1==y.index[-1] else None)\n",
    "    \n",
    "    # construct the indices\n",
    "    row_list = np.array(rows.loc[pd.notnull(rows)])\n",
    "    col_list = np.array(cols.loc[pd.notnull(cols)])\n",
    "    \n",
    "    # construct the raster\n",
    "    raster = r.RasterModelGrid((len(row_list), len(col_list)), spacing=(dy, dx))\n",
    "    raster.add_zeros\n",
    "    return(raster, row_list, col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a raster\n",
    "raster, t1, t2 = rasterDimensions (minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=100, dy=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.boundary_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappingfileToRaster(mappingfile, spatial_resolution=0.01250, approx_distance_m_x=6000):\n",
    "    # assess raster dimensions from mappingfile\n",
    "    mf, nstations = mappingfileToDF(mappingfile, colvar=None)\n",
    "    ncol = int((mf.LONG_.max()-mf.LONG_.min())/spatial_resolution +1)\n",
    "    nrow = int((mf.LAT.max()-mf.LAT.min())/spatial_resolution +1)\n",
    "    \n",
    "    # dimensions of the raster\n",
    "    row_list = [mf.LAT.min() + spatial_resolution*(station) for station in range(0,nrow,1)]    \n",
    "    col_list = [mf.LONG_.min() + spatial_resolution*(station) for station in range(0,ncol,1)]\n",
    "    \n",
    "    # initialize RasterModelGrid\n",
    "    raster = r.RasterModelGrid(nrow, ncol, dx=approx_distance_m_x)\n",
    "    raster.add_zeros\n",
    "\n",
    "    # initialize node list\n",
    "    df_list=[]\n",
    "\n",
    "    # loop through the raster nodes (bottom to top arrays)\n",
    "    for row_index, nodelist in enumerate(raster.nodes):\n",
    "        \n",
    "        # index bottom to top arrays with ordered Latitude\n",
    "        lat = row_list[row_index]\n",
    "        \n",
    "        # index left to right with ordered Longitude\n",
    "        for nodeid, long_ in zip(nodelist, col_list):\n",
    "            df_list.append([nodeid, lat, long_])\n",
    "\n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame.from_records(df_list).rename(columns={0:'nodeid',1:'LAT',2:'LONG_'})\n",
    "    \n",
    "    # identify raster nodeid and equivalent mappingfile FID\n",
    "    df = df.merge(mf[['FID','LAT','LONG_','ELEV']], how='outer', on=['LAT','LONG_'])\n",
    "    return(df, raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeXmap, raster = ogh.mappingfileToRaster(mappingfile=mappingfile1,\n",
    "                                           spatial_resolution=0.06250,\n",
    "                                           approx_distance_m_x=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PRECIP_dailymet_livneh2013'],\n",
    "                          vardf_dateindex=3,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PRECIP_dailywrf_salathe2014'],\n",
    "                          vardf_dateindex=3,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total files and image to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Computed spatial-temporal summaries of two gridded data product data sets for Sauk-Suiattle'\n",
    "abstract = 'This resource contains the computed summaries for the Meteorology data from Livneh et al. 2013 and the WRF data from Salathe et al. 2014.'\n",
    "keywords = ['Sauk-Suiattle', 'Livneh 2013', 'Salathe 2014','climate','hydromet','watershed', 'visualizations and summaries'] \n",
    "rtype = 'genericresource'\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_bc45\n",
    "models=[model for model in df.columns if model not in ['Date','Year','Month','Day']]\n",
    "time=time1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t=pd.DataFrame({'test':['terrific']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=pd.DataFrame({'test':['terrific']})\n",
    "t2=pd.DataFrame({'test':['terrific']})\n",
    "t3=pd.DataFrame({'test':['terrific']})\n",
    "t4=pd.DataFrame({'test':['terrific']})\n",
    "\n",
    "for somedf in (t1, t2, t3, t4):\n",
    "    if somedf in locals():\n",
    "        print(locals())\n",
    "    list(somedf.to_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [t1,t2,t2,t4]:\n",
    "    if each in globals().keys():\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = eval(t)\n",
    "str(y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
