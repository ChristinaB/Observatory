{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve NetCDF and model gridded climate time-series for a watershed\n",
    "\n",
    "### Case study:  the Sauk-Suiattle Watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. HydroShare setup and preparation\n",
    "    2. Re-establish the paths to the mapping file\n",
    "    3. Compute daily, monthly, and annual temperature and precipitation statistics\n",
    "    4. Visualize precipitation results relative to the forcing data\n",
    "    5. Visualize the time-series trends\n",
    "    6. Save results back into HydroShare\n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import seaborn as sns\n",
    "\n",
    "# data migration library\n",
    "from utilities import hydroshare\n",
    "import ogh\n",
    "import ogh_xarray_landlab as oxl\n",
    "\n",
    "# plotting and shape libraries\n",
    "%matplotlib inline\n",
    "\n",
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dailymet_bclivneh2013',\n",
       " 'dailymet_livneh2013',\n",
       " 'dailymet_livneh2015',\n",
       " 'dailyvic_livneh2013',\n",
       " 'dailyvic_livneh2015',\n",
       " 'dailywrf_bcsalathe2014',\n",
       " 'dailywrf_salathe2014']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_steps',\n",
       " 'delimiter',\n",
       " 'domain',\n",
       " 'end_date',\n",
       " 'file_format',\n",
       " 'filename_structure',\n",
       " 'reference',\n",
       " 'spatial_resolution',\n",
       " 'start_date',\n",
       " 'subdomain',\n",
       " 'temporal_resolution',\n",
       " 'variable_info',\n",
       " 'variable_list',\n",
       " 'web_protocol']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(meta_file['dailymet_livneh2013'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following system variables:\n",
      "   HS_USR_NAME = jphuong\n",
      "   HS_RES_ID = 70b977e22af544f8a7e5a803935c329c\n",
      "   HS_RES_TYPE = genericresource\n",
      "   JUPYTER_HUB_IP = jupyter.cuahsi.org\n",
      "\n",
      "These can be accessed using the following command: \n",
      "   os.environ[key]\n",
      "\n",
      "   (e.g.)\n",
      "   os.environ[\"HS_USR_NAME\"]  => jphuong\n",
      "Successfully established a connection with HydroShare\n"
     ]
    }
   ],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of gridded climate points for the watershed\n",
    "\n",
    "For visualization purposes, we will also remap the study site shapefile, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. Since the shapefile was previously migrated, we can select 'N' for no overwriting.\n",
    "\n",
    "In the usecase1 notebook, the treatgeoself function identified the gridded cell centroid coordinates that overlap with our study site. These coordinates were documented within the mapping file, which will be remapped here. In the usecase2 notebook, the downloaded files were cataloged within the mapping file, so we will use the mappingfileSummary function to characterize the files available for Sauk-Suiattle for each gridded data product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|-- ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- NAmer_dem_list.cpg\n",
      "|   |   |   |-- NAmer_dem_list.dbf\n",
      "|   |   |   |-- NAmer_dem_list.prj\n",
      "|   |   |   |-- NAmer_dem_list.sbn\n",
      "|   |   |   |-- NAmer_dem_list.sbx\n",
      "|   |   |   |-- NAmer_dem_list.shp\n",
      "|   |   |   |-- NAmer_dem_list.shx\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "NAmer_dem_list.cpg<br>NAmer_dem_list.dbf<br>NAmer_dem_list.prj<br>NAmer_dem_list.sbn<br>NAmer_dem_list.sbx<br>NAmer_dem_list.shp<br>NAmer_dem_list.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "c532e0578e974201a0bc40a37ef2d284/\n",
      "|-- c532e0578e974201a0bc40a37ef2d284/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.cpg\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shp\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shx\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.dbf\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.prj\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wbdhub12_17110006_WGS84_Basin.cpg<br>wbdhub12_17110006_WGS84_Basin.shp<br>wbdhub12_17110006_WGS84_Basin.shx<br>wbdhub12_17110006_WGS84_Basin.dbf<br>wbdhub12_17110006_WGS84_Basin.prj"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 4)\n",
      "   FID       LAT      LONG_    ELEV\n",
      "0    0  48.53125 -121.59375  1113.0\n",
      "1    1  48.46875 -121.46875   646.0\n",
      "2    2  48.46875 -121.53125   321.0\n",
      "3    3  48.46875 -121.59375   164.0\n",
      "4    4  48.46875 -121.65625   369.0\n",
      "CPU times: user 16.8 s, sys: 445 ms, total: 17.3 s\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# map the mappingfiles from usecase1\n",
    "mappingfile1=ogh.treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                              mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Watershed</th>\n",
       "      <th>Sauk-Suiattle river</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median elevation in meters [range](Number of gridded cells)</th>\n",
       "      <th>1171[164-2216] (n=99)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(Sauk-Suiattle river, 1171[164-2216] (n=99))]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Compare Hydrometeorology \n",
    "\n",
    "This section performs computations and generates plots of the Livneh 2013 and Salathe 2014 mean temperature and mean total monthly precipitation in order to compare them with each other. The generated plots are automatically downloaded and saved as .png files within the \"homedir\" directory.\n",
    "\n",
    "Let's compare the Livneh 2013 and Salathe 2014 using the period of overlapping history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function getDailyWRF_salathe2014 in module ogh.ogh:\n",
      "\n",
      "getDailyWRF_salathe2014(homedir, mappingfile, subdir='salathe2014/WWA_1950_2010/raw', catalog_label='dailywrf_salathe2014')\n",
      "    Get the Salathe el al., 2014 raw Daily WRF files of interest using the reference mapping file\n",
      "    \n",
      "    homedir: (dir) the home directory to be used for establishing subdirectories\n",
      "    mappingfile: (dir) the file path to the mappingfile, which contains LAT, LONG_, and ELEV coordinates of interest\n",
      "    subdir: (dir) the subdirectory to be established under homedir\n",
      "    catalog_label: (str) the preferred name for the series of catalogged filepaths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ogh.getDailyWRF_salathe2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_x_dailywrf_Salathe2014 in module ogh_xarray_landlab:\n",
      "\n",
      "get_x_dailywrf_Salathe2014(homedir, spatialbounds, subdir='salathe2014/Daily_WRF_1970_1999/noBC', nworkers=4, start_date='1970-01-01', end_date='1989-12-31', rename_timelatlong_names={'LAT': 'LAT', 'LON': 'LON'}, file_prefix='sp_', replace_file=True)\n",
      "    get Daily WRF data from Salathe et al. (2014) using xarray on netcdf files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(oxl.get_x_dailywrf_Salathe2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF retrieval and clipping to a spatial extent\n",
    "\n",
    "The function get_x_dailywrf_salathe2014 retrieves and clips NetCDF files archived within the UW Rocinante NNRP repository. This archive contains daily data from January 1970 through December 1999 (30 years). Each netcdf file is comprised of meteorologic and VIC hydrologic outputs for a calendar month. The expected number of files would be 360 files (12 months for 30 years). \n",
    "\n",
    "In the code chunk below, 20 parallel workers will be initialized to distribute file retrieval and spatial clipping tasks. For each worker, they will wget the requested file, clip the netcdf file to gridded cell centroids within the the provided bounding box, then return the location of the spatially clipped output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gridded data files:99\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1151.040404040404m\n",
      "Maximum elevation: 2216.0m\n",
      "[########################################] | 100% Completed | 14min 38.3s\n"
     ]
    }
   ],
   "source": [
    "maptable, nstations = ogh.mappingfileToDF(mappingfile1)\n",
    "spatialbounds = {'minx':maptable.LONG_.min(), 'maxx':maptable.LONG_.max(),\n",
    "                 'miny':maptable.LAT.min(), 'maxy':maptable.LAT.max()}\n",
    "\n",
    "outputfiles = oxl.get_x_dailywrf_Salathe2014(homedir=homedir,\n",
    "                                             subdir='salathe2014/Daily_WRF_1970_1999/noBC_netcdf',\n",
    "                                             spatialbounds=spatialbounds,\n",
    "                                             nworkers=40,\n",
    "                                             start_date='1970-01-01', end_date='1999-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Observatory-1/tutorials/ogh_xarray_landlab.py\u001b[0m in \u001b[0;36mnetcdf_to_ascii\u001b[0;34m(homedir, subdir, netcdfs, mappingfile, catalog_label, meta_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# connect with collection of netcdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mds_mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_mfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetcdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'netcdf4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# generate list of variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, lock, data_vars, coords, autoclose, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mgetattr_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m     \u001b[0mfile_objs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_file_obj'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mgetattr_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m     \u001b[0mfile_objs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_file_obj'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                                                    \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                                                    \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                                                    **backend_kwargs)\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             store = backends.ScipyDataStore(filename_or_obj,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, writer, clobber, diskless, persist, autoclose, lock)\u001b[0m\n\u001b[1;32m    330\u001b[0m                                    \u001b[0mdiskless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiskless\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                                    format=format)\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         return cls(ds, mode=mode, writer=writer, opener=opener,\n\u001b[1;32m    334\u001b[0m                    autoclose=autoclose, lock=lock)\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_open_netcdf4_group\u001b[0;34m(filename, mode, group, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._get_vars\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/netCDF4/utils.py\u001b[0m in \u001b[0;36m_find_dim\u001b[0;34m(grp, dimname)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_find_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# find Dimension instance given group and name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# look in current group, and parents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outfilelist = oxl.netcdf_to_ascii(homedir=homedir, \n",
    "                                  subdir='salathe2014/Daily_WRF_1970_1999/noBC_ascii', \n",
    "                                  mappingfile=mappingfile1,\n",
    "                                  netcdfs=outputfiles,\n",
    "                                  meta_file=meta_file,\n",
    "                                  catalog_label='sp_WRF_NNRP_noBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'projection': 'Geographic',\n",
       " 'contributors': 'Eric P. Salathe, A.F. Hamlet, C.F. Mass, S-Y. Lee, G.S. Mauger, M. Stumbaugh, R. Steed, B. Dotson',\n",
       " 'keywords': 'climate change, climate impacts, dynamical downscaling, hydrologic change, extremes, Western U.S., Western United States, PNW, Pacific Northwest',\n",
       " 'contributor_role': 'Principal Investigator',\n",
       " 'contributor': 'Eric P. Salathe Jr.',\n",
       " 'contact_name': 'Eric P. Salathe Jr.',\n",
       " 'contributor_email': 'salathe@uw.edu',\n",
       " 'lat_min': '38.09375',\n",
       " 'lon_resolution': '0.0625',\n",
       " 'lat_max': '52.40625',\n",
       " 'title': 'Dynamically Downscaled Hydroclimate Projections: WRF model',\n",
       " 'lat_resolution': '0.0625',\n",
       " 'lon_min': '-124.65625',\n",
       " 'creator_email': 'bdotson@uw.edu',\n",
       " 'surfsgnconvention': 'Traditional',\n",
       " 'contact_email': 'salathe@uw.edu',\n",
       " 'institution': 'University of Washington, Climate Impacts Group',\n",
       " 'creator_name': 'Bri Dotson',\n",
       " 'rights': 'freely available',\n",
       " 'acknowledgement': ' ',\n",
       " 'summary': 'Dynamically downscaled NCEP-NCAR Reanalysis (NNRP) data and VIC model output using WRF for the period 1970-1999 at 1/16 degree resolution over the Pacific Northwestern United States. This dataset includes an observational run as well as two bias-correction methods. ',\n",
       " 'cdm_data_type': 'grid',\n",
       " 'sources': 'http://cses.washington.edu/cig/data/wrf.shtml',\n",
       " 'history': ' ',\n",
       " 'lon_max': '-105.96875',\n",
       " 'file_name': '/home/disk/picea/mauger/WRF/DATA/wrf_nnrp_vic_16d/WRF_NNRP_noBC/netcdf_daily/WRF_NNRP_noBC.197001.nc',\n",
       " 'variable_list': array(['MASK', 'PREC', 'TAVG', 'TMAX', 'TMIN', 'OLR', 'ISR', 'RH', 'VPD',\n",
       "        'ET', 'RUNOFF', 'BASEFLOW', 'SOILM1', 'SOILM2', 'SOILM3', 'SWE',\n",
       "        'SNODEP', 'PET1', 'PET2', 'PET3', 'PET4', 'PET5'],\n",
       "       dtype='<U8')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_file['sp_WRF_NNRP_noBC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        variable_list=['PREC'],\n",
    "                        dataset='sp_WRF_NNRP__noBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sp_WRF_NNRP__noBC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-ad6dee2e259a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extract metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sp_WRF_NNRP__noBC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compute sums and mean monthly an yearly sums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sp_WRF_NNRP__noBC'"
     ]
    }
   ],
   "source": [
    "# extract metadata\n",
    "dr = meta_file['sp_WRF_NNRP__noBC']\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='sp_WRF_NNRP__noBC',\n",
    "                                   start_date=dr['start_date'],\n",
    "                                   end_date=dr['end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(VegType_low, yrs_low, debug_low) = run_ecohydrology_model(grid_low, \n",
    "                                                           input_data=input_data_low,\n",
    "                                                           input_file=InputFile,\n",
    "                                                           synthetic_storms=False,\n",
    "                                                           number_of_storms=n,\n",
    "                                                           pet_method='PriestleyTaylor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of climate variables for the long-term mean (ltm).\n",
    "#### INPUT: gridded meteorology ASCII files located from the Sauk-Suiattle Mapping file. The inputs to gridclim_dict() include the folder location and name of the hydrometeorology data, the file start and end, the analysis start and end, and the elevation band to be included in the analsyis (max and min elevation). <br/>OUTPUT: dictionary of dataframes where rows are temporal summaries and columns are spatial summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total monthly and yearly precipitation, as well as the mean values across time and across stations\n",
    "#### INPUT: daily precipitation for each station from the long-term mean dictionary (ltm) <br/>OUTPUT: Append the computed dataframes and values into the ltm dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        variable_list=['PRECIP'],\n",
    "                        dataset='dailywrf_salathe2014',\n",
    "                        subset_start_date=dr[0],\n",
    "                        subset_end_date=dr[1],\n",
    "                        df_dict=ltm)\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='PRECIP_dailywrf_salathe2014',\n",
    "                                   start_date=dr[0],\n",
    "                                   end_date=dr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the Livneh et al. 2013 raw MET and Salathe et al. 2014 raw WRF dataframes and values within ltm\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of outputs\n",
    "files=[]\n",
    "\n",
    "# create the destination path for the dictionary of dataframes\n",
    "ltm_sauk=os.path.join(homedir, 'ltm_sauk.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=ltm, outfilepath=ltm_sauk)\n",
    "files.append(ltm_sauk)\n",
    "\n",
    "# append the mapping file for Sauk-Suiattle gridded cell centroids\n",
    "files.append(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitations\"\n",
    "\n",
    "#### INPUT: dataframe with each month as a row and each station as a column. <br/>OUTPUT: A png file that represents the distribution across stations (in Wateryear order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two lowest elevation locations\n",
    "lowE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=164)\n",
    "\n",
    "# one highest elevation location\n",
    "highE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=2216)\n",
    "\n",
    "# combine references together\n",
    "reference_lines = highE_ref + lowE_ref\n",
    "reference_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider value range when comparing Livneh to Salathe \n",
    "vr = ogh.valueRange([ltm['meanbymonthsum_PRECIP_dailymet_livneh2013'], \n",
    "                     ltm['meanbymonthsum_PRECIP_dailywrf_salathe2014']])\n",
    "\n",
    "print(vr.min(), vr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp, nstation = ogh.mappingfileToDF(mappingfile1)\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# #Higher resolution children gridded cells \n",
    "# #get data from Lower resolution parent grid cells to the children\n",
    "# \"\"\"\n",
    "# import landlab as L2\n",
    "\n",
    "# watershed_dem_sc = os.path.join(homedir, 'DEM_10m.asc')\n",
    "# (rmg_sc, z_sc) = L2.io.read_esri_ascii(watershed_dem_sc, name='topographic__elevation')\n",
    "# rmg_sc.set_watershed_boundary_condition(z_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test0=pd.read_table(watershed_dem_sc, nrows=5, sep='\\s+', header=None).set_index(0)[1].to_dict()\n",
    "# print(test0)\n",
    "\n",
    "# test1 = pd.read_table(watershed_dem_sc, \n",
    "#                       skiprows=6, \n",
    "#                       nrows=test0['nrows'],\n",
    "#                       sep='\\s+',\n",
    "#                       header=None)\n",
    "# print(test1.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.unstack().as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use('tableau-colorblind10')\n",
    "\n",
    "import shapely.ops\n",
    "from shapely.geometry import MultiPolygon, shape, point, box, Polygon\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculateUTMbounds(mappingfile, mappingfile_crs={'init':'epsg:4326'}, spatial_resolution=0.06250):\n",
    "    # read in the mappingfile\n",
    "    map_df, nstation = ogh.mappingfileToDF(mappingfile)\n",
    "\n",
    "    # loop though each LAT/LONG_ +/-0.06250 centroid into gridded cells\n",
    "    geom=[]\n",
    "    midpt = spatial_resolution\n",
    "    for ind in map_df.index:\n",
    "        mid = map_df.loc[ind]\n",
    "        geom.append(box(mid.LONG_- midpt, mid.LAT - midpt, mid.LONG_ + midpt, mid.LAT + midpt, ccw=True))\n",
    "\n",
    "    # generate the GeoDataFrame\n",
    "    test = gpd.GeoDataFrame(map_df, crs=mappingfile_crs, geometry=geom)\n",
    "\n",
    "    # compile gridded cells to extract bounding box\n",
    "    test['shapeName'] = 1\n",
    "\n",
    "    # dissolve shape into new shapefile\n",
    "    newShape = test.dissolve(by='shapeName').reset_index()\n",
    "    newShape.bounds\n",
    "\n",
    "    # take the minx and miny, and centroid_x and centroid_y\n",
    "    minx, miny, maxx, maxy = newShape.bounds.loc[0]\n",
    "    lon0, lat0 = np.array(newShape.centroid[0])\n",
    "\n",
    "    # generate the basemap raster\n",
    "    fig = plt.figure(figsize=(10,10), dpi=500)\n",
    "    ax1 = plt.subplot2grid((1,1),(0,0))\n",
    "    m = Basemap(projection='tmerc', resolution='h', ax=ax1, lat_0=lat0, lon_0=lon0,\n",
    "                llcrnrlon=minx, llcrnrlat=miny, urcrnrlon=maxx, urcrnrlat=maxy)\n",
    "\n",
    "    # transform each polygon to the utm basemap projection\n",
    "    for ind in newShape.index:\n",
    "        eachpol = newShape.loc[ind]\n",
    "        newShape.loc[ind,'g2'] = shapely.ops.transform(m, eachpol['geometry'])\n",
    "    \n",
    "    # remove the plot\n",
    "    plt.gcf().clear()\n",
    "\n",
    "    # establish the UTM basemap bounding box dimensions\n",
    "    minx2, miny2, maxx2, maxy2 = newShape['g2'].iloc[0].bounds\n",
    "    return(minx2, miny2, maxx2, maxy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gridded data files:99\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1151.040404040404m\n",
      "Maximum elevation: 2216.0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(122.60987004411436, -99.61975587982306, 70041.535561452, 83348.40050629438)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5000x5000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minx2, miny2, maxx2, maxy2 = calculateUTMbounds(mappingfile=mappingfile1,\n",
    "                                                mappingfile_crs={'init':'epsg:4326'},\n",
    "                                                spatial_resolution=0.06250)\n",
    "\n",
    "minx2, miny2, maxx2, maxy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-5914d69969c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeometry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = gpd.GeoDataFrame(test, crs={'init':m.projection}, geometry=test['geometry'])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-07ce872d010c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ELEV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "test3 = tmp.plot(column='ELEV', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122.60987004411436, -99.61975587982306, 70041.535561452, 83348.40050629438)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minx2, miny2, maxx2, maxy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maxx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-b764a16dc581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mmaxx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mminx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'maxx' is not defined"
     ]
    }
   ],
   "source": [
    "(maxx-minx)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import landlab.grid.raster as r\n",
    "\n",
    "def rasterDimensions (maxx, maxy, minx=0, miny=0, dy=100, dx=100):\n",
    "    # construct the range\n",
    "    x = pd.Series(range(int(minx),int(maxx)+1,1))\n",
    "    y = pd.Series(range(int(miny),int(maxy)+1,1))\n",
    "    \n",
    "    # filter for values that meet the increment or is the last value\n",
    "    cols = pd.Series(x.index).apply(lambda x1: x[x1] if x1 % dx == 0 or x1==x[0] or x1==x.index[-1] else None)\n",
    "    rows = pd.Series(y.index).apply(lambda y1: y[y1] if y1 % dy == 0 or y1==y[0] or y1==y.index[-1] else None)\n",
    "    \n",
    "    # construct the indices\n",
    "    row_list = np.array(rows.loc[pd.notnull(rows)])\n",
    "    col_list = np.array(cols.loc[pd.notnull(cols)])\n",
    "    \n",
    "    # construct the raster\n",
    "    raster = r.RasterModelGrid((len(row_list), len(col_list)), spacing=(dy, dx))\n",
    "    raster.add_zeros\n",
    "    return(raster, row_list, col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a raster\n",
    "raster, t1, t2 = oxl.rasterDimensions (minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=100, dy=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(836, 702)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappingfileToRaster(mappingfile, spatial_resolution=0.01250, approx_distance_m_x=6000):\n",
    "    # assess raster dimensions from mappingfile\n",
    "    mf, nstations = ogh.mappingfileToDF(mappingfile, colvar=None)\n",
    "    ncol = int((mf.LONG_.max()-mf.LONG_.min())/spatial_resolution +1)\n",
    "    nrow = int((mf.LAT.max()-mf.LAT.min())/spatial_resolution +1)\n",
    "    \n",
    "    # dimensions of the raster\n",
    "    row_list = [mf.LAT.min() + spatial_resolution*(station) for station in range(0,nrow,1)]    \n",
    "    col_list = [mf.LONG_.min() + spatial_resolution*(station) for station in range(0,ncol,1)]\n",
    "    \n",
    "    # initialize RasterModelGrid\n",
    "    raster = r.RasterModelGrid(nrow, ncol, dx=approx_distance_m_x)\n",
    "    raster.add_zeros\n",
    "\n",
    "    # initialize node list\n",
    "    df_list=[]\n",
    "\n",
    "    # loop through the raster nodes (bottom to top arrays)\n",
    "    for row_index, nodelist in enumerate(raster.nodes):\n",
    "        \n",
    "        # index bottom to top arrays with ordered Latitude\n",
    "        lat = row_list[row_index]\n",
    "        \n",
    "        # index left to right with ordered Longitude\n",
    "        for nodeid, long_ in zip(nodelist, col_list):\n",
    "            df_list.append([nodeid, lat, long_])\n",
    "\n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame.from_records(df_list).rename(columns={0:'nodeid',1:'LAT',2:'LONG_'})\n",
    "    \n",
    "    # identify raster nodeid and equivalent mappingfile FID\n",
    "    df = df.merge(mf[['FID','LAT','LONG_','ELEV']], how='outer', on=['LAT','LONG_'])\n",
    "    return(df, raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gridded data files:99\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1151.040404040404m\n",
      "Maximum elevation: 2216.0m\n"
     ]
    }
   ],
   "source": [
    "nodeXmap, raster = mappingfileToRaster(mappingfile=mappingfile1,\n",
    "                                           spatial_resolution=0.06250,\n",
    "                                           approx_distance_m_x=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 14)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ltm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-8b5946e30e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PRECIP_dailymet_livneh2013'],\n\u001b[0m\u001b[1;32m      2\u001b[0m                           \u001b[0mvardf_dateindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           \u001b[0mcrossmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodeXmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                           nodata=-9999)\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ltm' is not defined"
     ]
    }
   ],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PRECIP_dailymet_livneh2013'],\n",
    "                          vardf_dateindex=3,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_PRECIP_dailywrf_salathe2014'],\n",
    "                          vardf_dateindex=3,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total files and image to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Computed spatial-temporal summaries of two gridded data product data sets for Sauk-Suiattle'\n",
    "abstract = 'This resource contains the computed summaries for the Meteorology data from Livneh et al. 2013 and the WRF data from Salathe et al. 2014.'\n",
    "keywords = ['Sauk-Suiattle', 'Livneh 2013', 'Salathe 2014','climate','hydromet','watershed', 'visualizations and summaries'] \n",
    "rtype = 'genericresource'\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_bc45\n",
    "models=[model for model in df.columns if model not in ['Date','Year','Month','Day']]\n",
    "time=time1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t=pd.DataFrame({'test':['terrific']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=pd.DataFrame({'test':['terrific']})\n",
    "t2=pd.DataFrame({'test':['terrific']})\n",
    "t3=pd.DataFrame({'test':['terrific']})\n",
    "t4=pd.DataFrame({'test':['terrific']})\n",
    "\n",
    "for somedf in (t1, t2, t3, t4):\n",
    "    if somedf in locals():\n",
    "        print(locals())\n",
    "    list(somedf.to_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [t1,t2,t2,t4]:\n",
    "    if each in globals().keys():\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = eval(t)\n",
    "str(y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
