{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve NetCDF and model gridded climate time-series for a watershed\n",
    "\n",
    "### Case study:  the Sauk-Suiattle Watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. HydroShare setup and preparation\n",
    "    2. Re-establish the paths to the mapping file\n",
    "    3. Compute daily, monthly, and annual temperature and precipitation statistics\n",
    "    4. Visualize precipitation results relative to the forcing data\n",
    "    5. Visualize the time-series trends\n",
    "    6. Save results back into HydroShare\n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data processing\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget\n",
    "import urllib\n",
    "import dask as da\n",
    "import xarray as xray\n",
    "import sys\n",
    "\n",
    "# data migration library\n",
    "#import ogh\n",
    "from utilities import hydroshare\n",
    "import ogh_xarray_landlab as oxl\n",
    "\n",
    "# plotting and shape libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# supplementary libraries\n",
    "from dask.diagnostics import ProgressBar\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following system variables:\n",
      "   HS_USR_NAME = jphuong\n",
      "   HS_RES_ID = 87dc5742cf164126a11ff45c3307fd9d\n",
      "   HS_RES_TYPE = compositeresource\n",
      "   JUPYTER_HUB_IP = jupyter.cuahsi.org\n",
      "\n",
      "These can be accessed using the following command: \n",
      "   os.environ[key]\n",
      "\n",
      "   (e.g.)\n",
      "   os.environ[\"HS_USR_NAME\"]  => jphuong\n",
      "\n",
      "The hs_utils library requires a secure connection to your HydroShare account.\n",
      "Enter the HydroShare password for user 'jphuong': ········\n",
      "Successfully established a connection with HydroShare\n"
     ]
    }
   ],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of gridded climate points for the watershed\n",
    "\n",
    "For visualization purposes, we will also remap the study site shapefile, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. Since the shapefile was previously migrated, we can select 'N' for no overwriting.\n",
    "\n",
    "In the usecase1 notebook, the treatgeoself function identified the gridded cell centroid coordinates that overlap with our study site. These coordinates were documented within the mapping file, which will be remapped here. In the usecase2 notebook, the downloaded files were cataloged within the mapping file, so we will use the mappingfileSummary function to characterize the files available for Sauk-Suiattle for each gridded data product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|-- ef2d82bf960144b4bfb1bae6242bcc7f/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- NAmer_dem_list.cpg\n",
      "|   |   |   |-- NAmer_dem_list.dbf\n",
      "|   |   |   |-- NAmer_dem_list.prj\n",
      "|   |   |   |-- NAmer_dem_list.sbn\n",
      "|   |   |   |-- NAmer_dem_list.sbx\n",
      "|   |   |   |-- NAmer_dem_list.shp\n",
      "|   |   |   |-- NAmer_dem_list.shx\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "NAmer_dem_list.cpg<br>NAmer_dem_list.dbf<br>NAmer_dem_list.prj<br>NAmer_dem_list.sbn<br>NAmer_dem_list.sbx<br>NAmer_dem_list.shp<br>NAmer_dem_list.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "c532e0578e974201a0bc40a37ef2d284/\n",
      "|-- c532e0578e974201a0bc40a37ef2d284/\n",
      "|   |-- bagit.txt\n",
      "|   |-- manifest-md5.txt\n",
      "|   |-- readme.txt\n",
      "|   |-- tagmanifest-md5.txt\n",
      "|   |-- data/\n",
      "|   |   |-- resourcemap.xml\n",
      "|   |   |-- resourcemetadata.xml\n",
      "|   |   |-- contents/\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.cpg\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.dbf\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.prj\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.sbn\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.sbx\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shp\n",
      "|   |   |   |-- wbdhub12_17110006_WGS84_Basin.shx\n",
      "\n",
      "Do you want to overwrite these data [Y/n]? n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wbdhub12_17110006_WGS84_Basin.cpg<br>wbdhub12_17110006_WGS84_Basin.dbf<br>wbdhub12_17110006_WGS84_Basin.prj<br>wbdhub12_17110006_WGS84_Basin.sbn<br>wbdhub12_17110006_WGS84_Basin.sbx<br>wbdhub12_17110006_WGS84_Basin.shp<br>wbdhub12_17110006_WGS84_Basin.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mappingfile1 = os.path.join(homedir, 'Sauk_mappingfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# map the mappingfiles from usecase1\n",
    "mappingfile1=ogh.treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                              mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Compare Hydrometeorology \n",
    "\n",
    "This section performs computations and generates plots of the Livneh 2013 and Salathe 2014 mean temperature and mean total monthly precipitation in order to compare them with each other. The generated plots are automatically downloaded and saved as .png files within the \"homedir\" directory.\n",
    "\n",
    "Let's compare the Livneh 2013 and Salathe 2014 using the period of overlapping history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_netcdfmap_hourlywrf_PNNL2018 in module ogh_xarray_landlab:\n",
      "\n",
      "get_netcdfmap_hourlywrf_PNNL2018(spatialbounds, homedir, subdir='PNNL2018/Hourly_WRF_1981_2015/noBC', start_date='1970-01-01', end_date='1970-02-28', nworkers=20)\n",
      "    get hourly WRF data from a 2018 PNNL WRF run using xarray on netcdf files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(oxl.get_netcdfmap_hourlywrf_PNNL2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF retrieval and clipping to a spatial extent\n",
    "\n",
    "The function get_x_dailywrf_salathe2014 retrieves and clips NetCDF files archived within the UW Rocinante NNRP repository. This archive contains daily data from January 1970 through December 1979 (10 years). Each netcdf file is comprised of meteorologic and VIC hydrologic outputs for a calendar month. The expected number of files would be 360 files (12 months for 30 years). \n",
    "\n",
    "In the code chunk below, 40 parallel workers will be initialized to distribute file retrieval and spatial clipping tasks. For each worker, they will wget the requested file, clip the netcdf file to gridded cell centroids within the the provided bounding box, then return the location of the spatially clipped output files.\n",
    "\n",
    "Provide the home and subdirectory where the cropped NetCDF files will be stored. Also provide the spatial bounds (in WGS84) to crop the NetCDF files upon download. Finally, provide the number of workers to carry out the download tasks, and the start and end date of the files of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (south_north: 88, west_east: 75)\n",
       "Dimensions without coordinates: south_north, west_east\n",
       "Data variables:\n",
       "    LAT      (south_north, west_east) float32 ...\n",
       "    LON      (south_north, west_east) float32 ...\n",
       "    Z        (south_north, west_east) float32 ..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the coordinate map\n",
    "domain = 'http://cses.washington.edu'\n",
    "subdomain = 'rocinante/WRF/PNNL_NARR_6km'\n",
    "netcdfmap = 'data_LatLonGht.nc'\n",
    "\n",
    "# retrieve the map\n",
    "if not os.path.exists(netcdfmap):\n",
    "    wget.download('{0}/{1}/{2}'.format(domain, subdomain, netcdfmap))\n",
    "\n",
    "# open the map\n",
    "pnnlxy=xray.open_dataset(netcdfmap)\n",
    "pnnlxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a few sample NetCDF data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-01.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-02.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-03.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-04.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-05.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-06.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-07.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-08.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-09.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-10.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-11.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-12.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-13.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-14.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-15.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-16.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-17.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-18.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-19.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-20.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-21.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-22.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-23.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-24.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-25.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-26.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-27.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-28.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-29.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-30.nc',\n",
       " 'http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-31.nc']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date='1981-01-01'\n",
    "end_date='1981-01-31'\n",
    "\n",
    "dr = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "filelist = oxl.compile_x_wrfpnnl2018_raw_locations(dr,\n",
    "                                                   domain='http://cses.washington.edu',\n",
    "                                                   subdomain='rocinante/WRF/PNNL_NARR_6km')\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_x_wrfpnnl2018_raw_locations(time_increments,\n",
    "                                        domain='http://cses.washington.edu',\n",
    "                                        subdomain='rocinante/WRF/PNNL_NARR_6km'):\n",
    "    \"\"\"\n",
    "    Compile a list of file URLs for PNNL 2018 raw WRF data\n",
    "    time_increments: (list) a list of dates that identify each netcdf file\n",
    "    \"\"\"\n",
    "    locations=[]\n",
    "\n",
    "    for ind, ymd in enumerate(time_increments):\n",
    "        subfolder = '{0}'.format(ymd.strftime('%Y'))\n",
    "        basename='data.{0}.nc'.format(ymd.strftime('%Y-%m-%d'))\n",
    "        url='{0}/{1}/{2}/{3}'.format(domain, subdomain, subfolder, basename)\n",
    "        locations.append(url)\n",
    "    return(locations)\n",
    "\n",
    "\n",
    "def ensure_dir(f):\n",
    "    \"\"\"\n",
    "    Check if the folder directory exists, else create it, then set it as the working directory\n",
    "    \n",
    "    f: (dir) the directory to create and/or set as working directory\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f):\n",
    "        os.makedirs(f)\n",
    "    os.chdir(f)\n",
    "    \n",
    "    \n",
    "def wget_download_one(fileurl):\n",
    "    \"\"\"\n",
    "    Download a file from an http domain\n",
    "    \n",
    "    fileurl: (url) a url to request\n",
    "    \"\"\"\n",
    "    # check and download each location point, if it doesn't already exist in the download directory\n",
    "    basename=os.path.basename(fileurl)\n",
    "    \n",
    "    # if it exists, remove for new download (overwrite mode)\n",
    "    if os.path.isfile(basename):\n",
    "        os.remove(basename)\n",
    "        \n",
    "    try:\n",
    "        ping = urllib.request.urlopen(fileurl)\n",
    "        if ping.getcode()!=404:\n",
    "            wget.download(fileurl)\n",
    "            print('downloaded: ' + basename)\n",
    "    except:\n",
    "        print('File does not exist at this URL: ' + basename)\n",
    "        \n",
    "        \n",
    "def wget_download_p(listofinterest, nworkers=20):\n",
    "    \"\"\"\n",
    "    Download files from an http domain in parallel\n",
    "    \n",
    "    listofinterest: (list) a list of urls to request\n",
    "    nworkers: (int) the number of processors to distribute tasks; default is 20\n",
    "    \"\"\"\n",
    "    # initialize parallel workers\n",
    "    #da.set_options(pool=ThreadPool(nworkers))\n",
    "    #ProgressBar().register()\n",
    "    #pool = dask.delayed(wget_download_one)(each for each in listofinterest)\n",
    "    #dask.compute(pool)\n",
    "    pool = Pool(int(nworkers))\n",
    "    pool.map(wget_download_one, listofinterest)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    \n",
    "    \n",
    "def wget_netcdfmap_spSubset(url,\n",
    "                            datetime,\n",
    "                            spatialbounds,\n",
    "                            netcdfmap=netcdfmap,\n",
    "                            nworkers=20,\n",
    "                            time_resolution='H',\n",
    "                            time_steps=24,\n",
    "                            file_prefix='sp_',\n",
    "                            rename_timelatlong_names={'LAT':'LAT','LON':'LON'},\n",
    "                            replace_file=True):\n",
    "    \n",
    "    # retrieve the file\n",
    "    #ogh.wget_download_one(url)\n",
    "    wget_download_one(url)\n",
    "    \n",
    "    # open the file\n",
    "    ds = xray.open_dataset(os.path.basename(url), engine='netcdf4')\n",
    "\n",
    "    #merge pnnl lat long data with climate data\n",
    "    ds_new = xr.merge([ds, netcdfmap], compat='no_conflicts')\n",
    "\n",
    "    #convert variables LAT, LON and Z to coordinates\n",
    "    ds_c = ds_new.set_coords({'LAT','LON','Z'}, inplace=False)\n",
    "\n",
    "    #create series of dates to add to dataset\n",
    "    time_inc = pd.date_range(start=ymd, periods=time_steps, freq=time_resolution)\n",
    "\n",
    "    #add coordinates using series of dates\n",
    "    ds_c.update({'time': ('time', time_inc)})\n",
    "    \n",
    "    # rename latlong if they are not LAT and LON, respectively\n",
    "    if not isinstance(rename_timelatlong_names, type(None)):\n",
    "        ds_c = ds_c.rename(rename_timelatlong_names)\n",
    "\n",
    "    # slice by the bounding box\n",
    "    spSubset = ds_c.sel(LON=slice(spatialbounds['minx'], spatialbounds['maxx']),\n",
    "                        LAT=slice(spatialbounds['miny'], spatialbounds['maxy']))\n",
    "\n",
    "    # print the spatial subset\n",
    "    spSubset.to_netcdf(file_prefix+os.path.basename(url))\n",
    "    \n",
    "    # remove the parent\n",
    "    ds.close()\n",
    "    os.remove(os.path.basename(url))\n",
    "    return(os.path.join(os.getcwd(), file_prefix+os.path.basename(url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://cses.washington.edu/rocinante/WRF/PNNL_NARR_6km/1981/data.1981-01-01.nc\n"
     ]
    }
   ],
   "source": [
    "print(filelist[0])\n",
    "ping = urllib.request.urlopen(filelist[0])\n",
    "if ping.getcode()!=404:\n",
    "    wget.download(filelist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_netcdfmap_hourlywrf_PNNL2018(spatialbounds,\n",
    "                                     homedir,\n",
    "                                     subdir='PNNL2018/Hourly_WRF_1981_2015/noBC',\n",
    "                                     start_date='1970-01-01',\n",
    "                                     end_date='1970-02-28',\n",
    "                                     nworkers=20):\n",
    "    \"\"\"\n",
    "    get hourly WRF data from a 2018 PNNL WRF run using xarray on netcdf files\n",
    "    \"\"\"\n",
    "    # retrieve the netcdf map\n",
    "    domain='http://cses.washington.edu'\n",
    "    subdomain='rocinante/WRF/PNNL_NARR_6km'\n",
    "    netcdfmap = 'data_LatLonGht.nc'\n",
    "    if not os.path.exists(netcdfmap):\n",
    "        wget.download('{0}/{1}/{2}'.format(domain, subdomain, netcdfmap))\n",
    "    pnnlxy=xray.open_dataset(netcdfmap)\n",
    "    \n",
    "    # check and generate data directory\n",
    "    filedir=os.path.join(homedir, subdir)\n",
    "    #ogh.ensure_dir(filedir)\n",
    "    ensure_dir(filedir)\n",
    "\n",
    "    # initialize parallel workers\n",
    "    da.set_options(pool=ThreadPool(nworkers))\n",
    "    ProgressBar().register()\n",
    "\n",
    "    # map to the data files\n",
    "    locations=[]\n",
    "    dr = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    locations = compile_x_wrfpnnl2018_raw_locations(time_increments=dr,\n",
    "                                                    domain=domain,\n",
    "                                                    subdomain=domain)\n",
    "    \n",
    "    # retrieve and subset each netcdf datafile\n",
    "    pnnl_files=[]\n",
    "    for eachurl, ymd in zip(locations, dr):\n",
    "        pnnl_files.append(da.delayed(wget_netcdfmap_spSubset)(url=eachurl,\n",
    "                                                              datetime=ymd,\n",
    "                                                              spatialbounds=spatialbounds,\n",
    "                                                              netcdfmap=netcdfmap,\n",
    "                                                              nworkers=nworkers,\n",
    "                                                              time_resolution='H',\n",
    "                                                              file_prefix='sp_',\n",
    "                                                              rename_timelatlong_names={'LAT':'LAT','LON':'LON'},\n",
    "                                                              replace_file=True))\n",
    "    # execute retrieval and spSubsetting\n",
    "    #pnnl_spSubset = da.compute(pnnl_files)[0]\n",
    "    #return(pnnl_spSubset)\n",
    "    return(pnnl_files)\n",
    "\n",
    "\n",
    "def compareonvar(map_df, colvar='all'):\n",
    "    \"\"\"\n",
    "    subsetting a dataframe based on some columns of interest\n",
    "    \n",
    "    map_df: (dataframe) the dataframe of the mappingfile table\n",
    "    colvar: (str or list) the column(s) to use for subsetting; 'None' returns an outerjoin, 'all' returns an innerjoin\n",
    "    \"\"\"\n",
    "    # apply row-wise inclusion based on a subset of columns\n",
    "    if isinstance(colvar, type(None)):\n",
    "        return(map_df)\n",
    "    \n",
    "    if colvar is 'all':\n",
    "        # compare on all columns except the station info\n",
    "        return(map_df.dropna())\n",
    "    else:\n",
    "        # compare on only the listed columns\n",
    "        return(map_df.dropna(subset=colvar))\n",
    "\n",
    "    \n",
    "def mappingfileToDF(mappingfile, colvar='all', summary=True):\n",
    "    \"\"\"\n",
    "    read in a dataframe and subset based on columns of interest\n",
    "    \n",
    "    mappingfile: (dir) the file path to the mappingfile, which contains LAT, LONG_, and ELEV coordinates of interest\n",
    "    colvar: (str or list) the column(s) to use for subsetting; 'None' returns an outerjoin, 'all' returns an innerjoin\n",
    "    \"\"\"\n",
    "    # Read in the mappingfile as a data frame\n",
    "    map_df = pd.read_csv(mappingfile)\n",
    "    \n",
    "    # select rows (datafiles) based on the colvar(s) chosen, default is \n",
    "    map_df = compareonvar(map_df=map_df, colvar=colvar)\n",
    "    \n",
    "    if summary:\n",
    "        # compile summaries\n",
    "        print('Number of gridded data files:'+ str(len(map_df)))\n",
    "        print('Minimum elevation: ' + str(np.min(map_df.ELEV))+ 'm')\n",
    "        print('Mean elevation: '+ str(np.mean(map_df.ELEV))+ 'm')\n",
    "        print('Maximum elevation: '+ str(np.max(map_df.ELEV))+ 'm')\n",
    "        \n",
    "    return(map_df, len(map_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gridded data files:99\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1151.040404040404m\n",
      "Maximum elevation: 2216.0m\n"
     ]
    }
   ],
   "source": [
    "#maptable, nstations = ogh.mappingfileToDF(mappingfile1)\n",
    "maptable, nstations = mappingfileToDF(mappingfile1)\n",
    "spatialbounds = {'minx':maptable.LONG_.min(), 'maxx':maptable.LONG_.max(),\n",
    "                 'miny':maptable.LAT.min(), 'maxy':maptable.LAT.max()}\n",
    "\n",
    "outputfiles = get_netcdfmap_hourlywrf_PNNL2018(homedir=homedir,\n",
    "                                               subdir='PNNL2018/Hourly_WRF_1970_1970/raw_netcdf',\n",
    "                                               spatialbounds=spatialbounds,\n",
    "                                               nworkers=5,\n",
    "                                               start_date=start_date, \n",
    "                                               end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks/data/87dc5742cf164126a11ff45c3307fd9d/87dc5742cf164126a11ff45c3307fd9d/data/contents/PNNL2018/Hourly_WRF_1970_1970/raw_netcdf/data_LatLonGht.nc'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#netcdfmap\n",
    "os.path.join(os.getcwd(),netcdfmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded: data.1981-01-01.nc\n",
      "<xarray.Dataset>\n",
      "Dimensions:      (south_north: 88, time: 24, west_east: 75)\n",
      "Coordinates:\n",
      "    LAT          (south_north, west_east) float32 ...\n",
      "    LON          (south_north, west_east) float32 ...\n",
      "    Z            (south_north, west_east) float32 ...\n",
      "  * time         (time) datetime64[ns] 1981-01-01 ... 1981-01-01T23:00:00\n",
      "Dimensions without coordinates: south_north, west_east\n",
      "Data variables:\n",
      "    T2           (time, south_north, west_east) float32 ...\n",
      "    Q2           (time, south_north, west_east) float32 ...\n",
      "    PSFC         (time, south_north, west_east) float32 ...\n",
      "    GLW          (time, south_north, west_east) float32 ...\n",
      "    SWDOWN       (time, south_north, west_east) float32 ...\n",
      "    U10          (time, south_north, west_east) float32 ...\n",
      "    V10          (time, south_north, west_east) float32 ...\n",
      "    PREC_ACC_NC  (time, south_north, west_east) float32 ...\n",
      "    SNOW_ACC_NC  (time, south_north, west_east) float32 ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimensions or multi-index levels ['LON', 'LAT'] do not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-d85fc6ba3780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# slice by the bounding box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     spSubset = ds_c.sel(LON=slice(spatialbounds['minx'], spatialbounds['maxx']),\n\u001b[0;32m---> 41\u001b[0;31m                         LAT=slice(spatialbounds['miny'], spatialbounds['maxy']))\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# print the spatial subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36msel\u001b[0;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meither_dict_or_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         pos_indexers, new_indexes = remap_label_indexers(\n\u001b[0;32m-> 1621\u001b[0;31m             self, indexers=indexers, method=method, tolerance=tolerance)\n\u001b[0m\u001b[1;32m   1622\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/coordinates.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     pos_indexers, new_indexes = indexing.remap_label_indexers(\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m     \u001b[0;31m# attach indexer's coordinate to pos_indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(data_obj, indexers, method, tolerance)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mnew_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mdim_indexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dim_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_indexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mget_dim_indexers\u001b[0;34m(data_obj, indexers)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"dimensions or multi-index levels %r do not exist\"\n\u001b[0;32m--> 205\u001b[0;31m                          % invalid)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mlevel_indexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimensions or multi-index levels ['LON', 'LAT'] do not exist"
     ]
    }
   ],
   "source": [
    "#perform = da.compute(outputfiles[0])[0]\n",
    "\n",
    "spatialbounds\n",
    "pnnlxy=xray.open_dataset(os.path.join(os.getcwd(),netcdfmap), engine='netcdf4')\n",
    "\n",
    "nworkers=20\n",
    "time_resolution='H'\n",
    "time_steps=24\n",
    "file_prefix='sp_'\n",
    "rename_timelatlong_names={'LAT':'LAT','LON':'LON'}\n",
    "replace_file=True\n",
    "\n",
    "for url, ymd in zip(filelist[:3], dr[:3]):\n",
    "    \n",
    "    # retrieve the file\n",
    "    #ogh.wget_download_one(url)\n",
    "    wget_download_one(url)\n",
    "    \n",
    "    # open the file\n",
    "    ds = xray.open_dataset(os.path.basename(url), engine='netcdf4')\n",
    "\n",
    "    #merge pnnl lat long data with climate data\n",
    "    ds_new = xray.merge([ds, pnnlxy], compat='no_conflicts')\n",
    "\n",
    "    #convert variables LAT, LON and Z to coordinates\n",
    "    ds_c = ds_new.set_coords({'LAT','LON','Z'}, inplace=False)\n",
    "\n",
    "    #create series of dates to add to dataset\n",
    "    time_inc = pd.date_range(start=ymd, periods=time_steps, freq=time_resolution)\n",
    "\n",
    "    #add coordinates using series of dates\n",
    "    ds_c.update({'time': ('time', time_inc)})\n",
    "    \n",
    "    # rename latlong if they are not LAT and LON, respectively\n",
    "    if not isinstance(rename_timelatlong_names, type(None)):\n",
    "        ds_c = ds_c.rename(rename_timelatlong_names)\n",
    "\n",
    "    print(ds_c)\n",
    "    # slice by the bounding box\n",
    "    spSubset = ds_c.sel(LON=slice(spatialbounds['minx'], spatialbounds['maxx']),\n",
    "                        LAT=slice(spatialbounds['miny'], spatialbounds['maxy']))\n",
    "\n",
    "    # print the spatial subset\n",
    "    spSubset.to_netcdf(file_prefix+os.path.basename(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:      (south_north: 88, time: 4, west_east: 75)\n",
       "Coordinates:\n",
       "    LAT          (south_north, west_east) float32 45.0204 45.028 ... 50.1414\n",
       "    LON          (south_north, west_east) float32 ...\n",
       "    Z            (south_north, west_east) float32 0.0 0.0 ... 1382.39 1516.88\n",
       "  * time         (time) datetime64[ns] 1981-01-01T18:00:00 ... 1981-01-01T21:00:00\n",
       "Dimensions without coordinates: south_north, west_east\n",
       "Data variables:\n",
       "    T2           (time, south_north, west_east) float32 ...\n",
       "    Q2           (time, south_north, west_east) float32 ...\n",
       "    PSFC         (time, south_north, west_east) float32 ...\n",
       "    GLW          (time, south_north, west_east) float32 ...\n",
       "    SWDOWN       (time, south_north, west_east) float32 ...\n",
       "    U10          (time, south_north, west_east) float32 ...\n",
       "    V10          (time, south_north, west_east) float32 ...\n",
       "    PREC_ACC_NC  (time, south_north, west_east) float32 ...\n",
       "    SNOW_ACC_NC  (time, south_north, west_east) float32 ..."
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_c.sel(time=slice('1981-01-01T018:00:00.000000000', '1981-01-01T21:00:00.000000000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimensions or multi-index levels ['LON'] do not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-18599b063df6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLON\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatialbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatialbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36msel\u001b[0;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meither_dict_or_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         pos_indexers, new_indexes = remap_label_indexers(\n\u001b[0;32m-> 1621\u001b[0;31m             self, indexers=indexers, method=method, tolerance=tolerance)\n\u001b[0m\u001b[1;32m   1622\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/coordinates.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     pos_indexers, new_indexes = indexing.remap_label_indexers(\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m     \u001b[0;31m# attach indexer's coordinate to pos_indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(data_obj, indexers, method, tolerance)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mnew_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mdim_indexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dim_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_indexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mget_dim_indexers\u001b[0;34m(data_obj, indexers)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"dimensions or multi-index levels %r do not exist\"\n\u001b[0;32m--> 205\u001b[0;31m                          % invalid)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mlevel_indexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimensions or multi-index levels ['LON'] do not exist"
     ]
    }
   ],
   "source": [
    "ds_c.sel(LON=slice(spatialbounds['minx'], spatialbounds['maxx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'south_north' (south_north: 88)>\n",
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87])\n",
       "Dimensions without coordinates: south_north"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_c.T2.south_north"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:      (LATs: 6600, LONs: 6600, south_north: 88, time: 24, west_east: 75)\n",
       "Coordinates:\n",
       "    LAT          (south_north, west_east) float32 45.0204 45.028 ... 50.1414\n",
       "    LON          (south_north, west_east) float32 -124.985 -124.909 ... -119.851\n",
       "    Z            (south_north, west_east) float32 0.0 0.0 ... 1382.39 1516.88\n",
       "  * time         (time) datetime64[ns] 1981-01-01 ... 1981-01-01T23:00:00\n",
       "  * LATs         (LATs) float64 45.02 45.03 45.04 45.04 ... 50.13 50.14 50.14\n",
       "  * LONs         (LONs) float64 -125.0 -124.9 -124.8 ... -120.0 -119.9 -119.9\n",
       "Dimensions without coordinates: south_north, west_east\n",
       "Data variables:\n",
       "    T2           (time, south_north, west_east) float32 ...\n",
       "    Q2           (time, south_north, west_east) float32 ...\n",
       "    PSFC         (time, south_north, west_east) float32 ...\n",
       "    GLW          (time, south_north, west_east) float32 ...\n",
       "    SWDOWN       (time, south_north, west_east) float32 ...\n",
       "    U10          (time, south_north, west_east) float32 ...\n",
       "    V10          (time, south_north, west_east) float32 ...\n",
       "    PREC_ACC_NC  (time, south_north, west_east) float32 ...\n",
       "    SNOW_ACC_NC  (time, south_north, west_east) float32 ..."
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LATs=np.array([])\n",
    "for ind, eachline in enumerate(ds_c.LAT.data):\n",
    "    LATs=np.concatenate([LATs,eachline])\n",
    "\n",
    "LONs=np.array([])\n",
    "for ind, eachline in enumerate(ds_c.LON.data):\n",
    "    LONs=np.concatenate([LONs,eachline])\n",
    "\n",
    "#add coordinates using series of dates\n",
    "ds_c.update({'LATs': ('LATs', LATs)})\n",
    "ds_c.update({'LONs': ('LONs', LONs)})\n",
    "\n",
    "#convert variables LAT, LON and Z to coordinates\n",
    "ds_c = ds_new.set_coords({'LAT','LON','Z'}, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add coordinates using series of dates\n",
    "ds_c.update({'LATs': ('LATs', LATs)})\n",
    "ds_c.update({'LONs': ('LONs', LONs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'T2' (time: 24, south_north: 88, west_east: 75)>\n",
       "[158400 values with dtype=float32]\n",
       "Coordinates:\n",
       "    LAT      (south_north, west_east) float32 45.0204 45.028 ... 50.137 50.1414\n",
       "    LON      (south_north, west_east) float32 -124.985 -124.909 ... -119.851\n",
       "    Z        (south_north, west_east) float32 0.0 0.0 0.0 ... 1382.39 1516.88\n",
       "  * time     (time) datetime64[ns] 1981-01-01 ... 1981-01-01T23:00:00\n",
       "Dimensions without coordinates: south_north, west_east\n",
       "Attributes:\n",
       "    units:        K\n",
       "    description:  Air Temperature at 2m "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_c.T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'T2' (time: 4, south_north: 88, west_east: 75)>\n",
       "array([[[ 283.06781 ,  282.995697, ...,  279.703918,  279.567993],\n",
       "        [ 283.088135,  282.92807 , ...,  280.379639,  280.331268],\n",
       "        ..., \n",
       "        [ 282.743896,  281.649963, ...,  275.711853,  274.920471],\n",
       "        [ 283.677765,  281.619598, ...,  276.210236,  275.094177]],\n",
       "\n",
       "       [[ 283.027313,  283.578583, ...,  280.052124,  279.990326],\n",
       "        [ 283.065063,  283.437988, ...,  280.683777,  280.657776],\n",
       "        ..., \n",
       "        [ 283.939545,  282.839264, ...,  276.178528,  275.262543],\n",
       "        [ 285.08432 ,  282.696991, ...,  276.938965,  275.865784]],\n",
       "\n",
       "       [[ 284.048615,  284.502838, ...,  280.264496,  280.316132],\n",
       "        [ 284.031311,  284.436493, ...,  280.890442,  280.883484],\n",
       "        ..., \n",
       "        [ 284.705597,  283.593445, ...,  276.522308,  275.484924],\n",
       "        [ 286.000061,  283.3479  , ...,  277.37558 ,  276.232941]],\n",
       "\n",
       "       [[ 284.727783,  284.86673 , ...,  280.600555,  280.891388],\n",
       "        [ 284.651337,  284.800873, ...,  281.2229  ,  281.15332 ],\n",
       "        ..., \n",
       "        [ 285.253571,  284.057739, ...,  276.706635,  275.570709],\n",
       "        [ 286.467316,  283.693512, ...,  277.445282,  276.30368 ]]], dtype=float32)\n",
       "Coordinates:\n",
       "    LAT      (south_north, west_east) float32 45.0204 45.028 ... 50.137 50.1414\n",
       "    LON      (south_north, west_east) float32 -124.985 -124.909 ... -119.851\n",
       "    Z        (south_north, west_east) float32 0.0 0.0 0.0 ... 1382.39 1516.88\n",
       "  * time     (time) datetime64[ns] 1981-01-01T18:00:00 ... 1981-01-01T21:00:00\n",
       "Dimensions without coordinates: south_north, west_east\n",
       "Attributes:\n",
       "    units:        K\n",
       "    description:  Air Temperature at 2m "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ds_c.T2.sel(south_north=slice(spatialbounds['minx'], spatialbounds['maxx']))\n",
    "ds_c.T2.sel(time=slice('1981-01-01T018:00:00.000000000', '1981-01-01T21:00:00.000000000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-121.71875",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m   3590\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_searchsorted_monotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_searchsorted_monotonic\u001b[0;34m(self, label, side)\u001b[0m\n\u001b[1;32m   3549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3550\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index must be monotonic increasing or decreasing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: index must be monotonic increasing or decreasing",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-72ce969de858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m spSubset = ds_c.sel(LONs=slice(spatialbounds['minx'], spatialbounds['maxx']),\n\u001b[0;32m----> 2\u001b[0;31m                     LATs=slice(spatialbounds['miny'], spatialbounds['maxy']))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspSubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36msel\u001b[0;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meither_dict_or_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         pos_indexers, new_indexes = remap_label_indexers(\n\u001b[0;32m-> 1621\u001b[0;31m             self, indexers=indexers, method=method, tolerance=tolerance)\n\u001b[0m\u001b[1;32m   1622\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/coordinates.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     pos_indexers, new_indexes = indexing.remap_label_indexers(\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m     \u001b[0;31m# attach indexer's coordinate to pos_indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[0;34m(data_obj, indexers, method, tolerance)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             idxr, new_idx = convert_label_indexer(index, label,\n\u001b[0;32m--> 250\u001b[0;31m                                                   dim, method, tolerance)\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mpos_indexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36mconvert_label_indexer\u001b[0;34m(index, label, index_name, method, tolerance)\u001b[0m\n\u001b[1;32m    136\u001b[0m         indexer = index.slice_indexer(_sanitize_slice_element(label.start),\n\u001b[1;32m    137\u001b[0m                                       \u001b[0m_sanitize_slice_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                                       _sanitize_slice_element(label.step))\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# unlike pandas, in xarray we never want to silently convert a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_indexer\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   3455\u001b[0m         \"\"\"\n\u001b[1;32m   3456\u001b[0m         start_slice, end_slice = self.slice_locs(start, end, step=step,\n\u001b[0;32m-> 3457\u001b[0;31m                                                  kind=kind)\n\u001b[0m\u001b[1;32m   3458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3459\u001b[0m         \u001b[0;31m# return a slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_locs\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3658\u001b[0;31m             \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_slice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3660\u001b[0m             \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m   3592\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m                 \u001b[0;31m# raise the original KeyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3594\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m   3586\u001b[0m         \u001b[0;31m# we need to look up the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3587\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3588\u001b[0;31m             \u001b[0mslc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc_only_exact_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3589\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_loc_only_exact_matches\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3555\u001b[0m         \u001b[0mget_slice_bound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m         \"\"\"\n\u001b[0;32m-> 3557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         return super(Float64Index, self).get_loc(key, method=method,\n\u001b[0;32m--> 404\u001b[0;31m                                                  tolerance=tolerance)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Float64Engine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -121.71875"
     ]
    }
   ],
   "source": [
    "spSubset = ds_c.sel(south_north=slice(spatialbounds['minx'], spatialbounds['maxx']),\n",
    "                    LATs=slice(spatialbounds['miny'], spatialbounds['maxy']))\n",
    "\n",
    "spSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_LatLonGht.nc']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/home/jovyan/work/notebooks/data/87dc5742cf164126a11ff45c3307fd9d/87dc5742cf164126a11ff45c3307fd9d/data/contents/PNNL2018/Hourly_WRF_1970_1970/raw_netcdf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/home/jovyan/work/notebooks/data/87dc5742cf164126a11ff45c3307fd9d/87dc5742cf164126a11ff45c3307fd9d/data/contents/PNNL2018/Hourly_WRF_1970_1970/raw_netcdf/data.1981-01-01.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9ffd21cb152b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jovyan/work/notebooks/data/87dc5742cf164126a11ff45c3307fd9d/87dc5742cf164126a11ff45c3307fd9d/data/contents/PNNL2018/Hourly_WRF_1970_1970/raw_netcdf/data.1981-01-01.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                                                    \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                                                    \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                                                    **backend_kwargs)\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             store = backends.ScipyDataStore(filename_or_obj,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, writer, clobber, diskless, persist, autoclose, lock)\u001b[0m\n\u001b[1;32m    329\u001b[0m                                    \u001b[0mdiskless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiskless\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                                    format=format)\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         return cls(ds, mode=mode, writer=writer, opener=opener,\n\u001b[1;32m    333\u001b[0m                    autoclose=autoclose, lock=lock)\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_open_netcdf4_group\u001b[0;34m(filename, mode, group, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/home/jovyan/work/notebooks/data/87dc5742cf164126a11ff45c3307fd9d/87dc5742cf164126a11ff45c3307fd9d/data/contents/PNNL2018/Hourly_WRF_1970_1970/raw_netcdf/data.1981-01-01.nc'"
     ]
    }
   ],
   "source": [
    "#tmp = xray.open_dataset('/home/jovyan/work/notebooks/data/87dc5742cf164126a11ff45c3307fd9d/87dc5742cf164126a11ff45c3307fd9d/data/contents/PNNL2018/Hourly_WRF_1970_1970/raw_netcdf/data.1981-01-01.nc')\n",
    "tmp = xray.open_dataset('data.1981-01-01.nc')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert collection of NetCDF files into a collection of ASCII files\n",
    "\n",
    "Provide the home and subdirectory where the ASCII files will be stored, the source_directory of netCDF files, and the mapping file to which the resulting ASCII files will be cataloged. Also, provide the Pandas Datetime code for the frequency of the time steps. Finally, provide the catalog label that will be used for the mapping file catalog and the metadata file label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# convert the netCDF files into daily ascii time-series files for each gridded location\n",
    "outfilelist = oxl.netcdf_to_ascii(homedir=homedir, \n",
    "                                  subdir='livneh2013/Daily_MET_1970_1970/raw_ascii', \n",
    "                                  source_directory=os.path.join(homedir, 'livneh2013/Daily_MET_1970_1970/raw_netcdf'),\n",
    "                                  mappingfile=mappingfile1,\n",
    "                                  temporal_resolution='D',\n",
    "                                  meta_file=meta_file,\n",
    "                                  catalog_label='sp_dailymet_livneh_1970_1970')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metadata\n",
    "ogh.saveDictOfDf(dictionaryObject=meta_file, outfilepath='test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of climate variables for the long-term mean (ltm).\n",
    "#### INPUT: gridded meteorology ASCII files located from the Sauk-Suiattle Mapping file. The inputs to gridclim_dict() include the folder location and name of the hydrometeorology data, the file start and end, the analysis start and end, and the elevation band to be included in the analsyis (max and min elevation). <br/>OUTPUT: dictionary of dataframes where rows are temporal summaries and columns are spatial summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file['sp_dailymet_livneh_1970_1970']['variable_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        dataset='sp_dailymet_livneh_1970_1970',\n",
    "                        variable_list=['Prec','Tmax','Tmin'])\n",
    "\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total monthly and yearly precipitation, as well as the mean values across time and across stations\n",
    "#### INPUT: daily precipitation for each station from the long-term mean dictionary (ltm) <br/>OUTPUT: Append the computed dataframes and values into the ltm dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata\n",
    "dr = meta_file['sp_dailymet_livneh_1970_1970']\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='Prec_sp_dailymet_livneh_1970_1970',\n",
    "                                   start_date=dr['start_date'],\n",
    "                                   end_date=dr['end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the analytical dataframes and values within ltm\n",
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of outputs\n",
    "files=[]\n",
    "\n",
    "# create the destination path for the dictionary of dataframes\n",
    "ltm_sauk=os.path.join(homedir, 'ltm_1970_1970_sauk.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=ltm, outfilepath=ltm_sauk)\n",
    "files.append(ltm_sauk)\n",
    "\n",
    "# append the mapping file for Sauk-Suiattle gridded cell centroids\n",
    "files.append(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitations\"\n",
    "\n",
    "#### INPUT: dataframe with each month as a row and each station as a column. <br/>OUTPUT: A png file that represents the distribution across stations (in Wateryear order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # two lowest elevation locations\n",
    "lowE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=164)\n",
    "\n",
    "# one highest elevation location\n",
    "highE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=2216)\n",
    "\n",
    "# combine references together\n",
    "reference_lines = highE_ref + lowE_ref\n",
    "reference_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogh.renderValueInBoxplot(ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'],\n",
    "                         outfilepath='totalMonthlyRainfall.png', \n",
    "                         plottitle='Total monthly rainfall',\n",
    "                         time_steps='month',\n",
    "                         wateryear=True,\n",
    "                         reference_lines=reference_lines,\n",
    "                         ref_legend=True,\n",
    "                         value_name='Total daily precipitation (mm)',\n",
    "                         cmap='seismic_r',\n",
    "                         figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogh.renderValuesInPoints(ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'], \n",
    "                         vardf_dateindex=12, \n",
    "                         shapefile=sauk, \n",
    "                         cmap='seismic_r',\n",
    "                         outfilepath='test.png', \n",
    "                         plottitle='December total rainfall',\n",
    "                         colorbar_label='Total monthly rainfall (mm)', \n",
    "                         figsize=(1.5,1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minx2, miny2, maxx2, maxy2 = oxl.calculateUTMbounds(mappingfile=mappingfile1,\n",
    "                                                    mappingfile_crs={'init':'epsg:4326'},\n",
    "                                                    spatial_resolution=0.06250)\n",
    "\n",
    "print(minx2, miny2, maxx2, maxy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate a raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.rasterDimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a raster\n",
    "raster, row_list, col_list = oxl.rasterDimensions(minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=1000, dy=1000)\n",
    "raster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher resolution children of gridded cells \n",
    "### get data from Lower resolution parent grid cells to the children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.mappingfileToRaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# landlab raster node crossmap to gridded cell id\n",
    "nodeXmap, raster, m = oxl.mappingfileToRaster(mappingfile=mappingfile1, spatial_resolution=0.06250, \n",
    "                                           minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=1000, dy=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the raster dimensions\n",
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nodeXmap.plot(column='ELEV', figsize=(10,10), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vector array of December monthly precipitation\n",
    "prec_vector = ogh.rasterVector(vardf=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'],\n",
    "                          vardf_dateindex=12,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "\n",
    "# close-off areas without data\n",
    "raster.status_at_node[prec_vector==-9999] = CLOSED_BOUNDARY\n",
    "\n",
    "fig =plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster, \n",
    "            prec_vector,\n",
    "            var_name='Daily precipitation',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Prec'].attrs['units'],\n",
    "            color_for_closed='black', \n",
    "            cmap='seismic_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_vector = ogh.rasterVector(vardf=ltm['meanbymonth_Tmax_sp_dailymet_livneh_1970_1970'],\n",
    "                          vardf_dateindex=12,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster, \n",
    "            tmax_vector,\n",
    "            var_name='Daily maximum temperature',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Tmax'].attrs['units'],\n",
    "            color_for_closed='black', symmetric_cbar=False, cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_vector = ogh.rasterVector(vardf=ltm['meanbymonth_Tmin_sp_dailymet_livneh_1970_1970'],\n",
    "                          vardf_dateindex=12,\n",
    "                          crossmap=nodeXmap,\n",
    "                          nodata=-9999)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster, \n",
    "            tmin_vector,\n",
    "            var_name='Daily minimum temperature',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Tmin'].attrs['units'],\n",
    "            color_for_closed='black', symmetric_cbar=False, cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a raster vector back to geospatial presentation\n",
    "t4, t5 = rasterVectorToWGS(prec_vector, nodeXmap=nodeXmap, UTM_transformer=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4.plot(column='value', figsize=(10,10), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is one decade\n",
    "inputvectors = {'precip_met': np.tile(ltm['meandaily_Prec_sp_dailymet_livneh_1970_1970'], 15000),\n",
    "                'Tmax_met': np.tile(ltm['meandaily_Tmax_sp_dailymet_livneh_1970_1970'], 15000),\n",
    "                'Tmin_met': np.tile(ltm['meandaily_Tmin_sp_dailymet_livneh_1970_1970'], 15000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(VegType_low, yrs_low, debug_low) = run_ecohydrology_model(raster,\n",
    "                                                           input_data=inputvectors,\n",
    "                                                           input_file=InputFile,\n",
    "                                                           synthetic_storms=False,\n",
    "                                                           number_of_storms=100000,\n",
    "                                                           pet_method='PriestleyTaylor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(raster, VegType_low, yrs_low, yr_step=yrs_low-1)\n",
    "plt.show()\n",
    "plt.savefig('grid_low.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the \"average monthly total precipitation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total files and image to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Computed spatial-temporal summaries of two gridded data product data sets for Sauk-Suiattle'\n",
    "abstract = 'This resource contains the computed summaries for the Meteorology data from Livneh et al. 2013 and the WRF data from Salathe et al. 2014.'\n",
    "keywords = ['Sauk-Suiattle', 'Livneh 2013', 'Salathe 2014','climate','hydromet','watershed', 'visualizations and summaries'] \n",
    "rtype = 'genericresource'\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
