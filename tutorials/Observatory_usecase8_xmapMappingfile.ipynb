{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to retrieve gridded climate time-series data sets\n",
    "\n",
    "### Case study: the Sauk-Suiattle river watershed, the Elwha river watershed, the Upper Rio Salado watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. HydroShare setup and preparation\n",
    "    2. Re-establish the paths to the mapping file\n",
    "    3. Download climate data\n",
    "    4. Summarize the file availability from each watershed mapping file\n",
    "    5. Save results back into HydroShare\n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import ogh\n",
    "import geopandas as gpd\n",
    "import ogh_xarray_landlab as oxl\n",
    "import xarray as xray\n",
    "\n",
    "# data migration library\n",
    "from utilities import hydroshare\n",
    "\n",
    "# plotting and shape libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)\n",
    "print('Data will be loaded from and save to:'+homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "\n",
    "# retrieve the netcdf map\n",
    "domain='http://cses.washington.edu'\n",
    "subdomain='rocinante/WRF/PNNL_NARR_6km'\n",
    "netcdfmap = 'data_LatLonGht.nc'\n",
    "\n",
    "if not os.path.exists(netcdfmap):\n",
    "    wget.download('{0}/{1}/{2}'.format(domain, subdomain, netcdfmap))\n",
    "\n",
    "pnnlxy=xray.open_dataset(netcdfmap)\n",
    "pnnlxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnnlxy_df = pnnlxy.to_dataframe().reset_index().rename(columns={'south_north':'SN','west_east':'WE','Z':'ELEV'})\n",
    "pnnlxy_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Re-establish the paths to the mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the mapping files generated for Sauk-Suiattle, Elwha, and Upper Rio Salado from usecase1\n",
    "mappingfile1 = os.path.join(homedir,'Sauk_mappingfile_i.csv')\n",
    "\n",
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingfile1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download climate data \n",
    "\n",
    "### Call each get Climate data function\n",
    "\n",
    "Each function reads in the mapping file table, generates the destination folder, downloads and unzips the files, then catalogs the downloaded files within the mapping file.\n",
    "\n",
    "Meteorology - MET; Weather Research and Forecasting (WRF); Variable Infiltration Capacity - VIC\n",
    "\n",
    "    1. getDailyMET_livneh2013\n",
    "    2. getDailyMET_bcLivneh2013\n",
    "    3. getDailyMET_livneh2015\n",
    "    4. getDailyVIC_livneh2013\n",
    "    5. getDailyVIC_livneh2015\n",
    "    6. getDailyWRF_salathe2014\n",
    "    7. getDailyWRF_bcsalathe2014\n",
    "\n",
    "### View data extent:\n",
    "\n",
    "    1. Continental United States (CONUS)\n",
    "    Livneh, B. (2017). Gridded climatology locations (1/16th degree): Continental United States extent, HydroShare, http://www.hydroshare.org/resource/14f0a6619c6b45cc90d1f8cabc4129af\n",
    "\n",
    "    2. North America (NAmer)\n",
    "    Livneh, B. (2017). Gridded climatology locations (1/16th degree): North American extent, HydroShare, http://www.hydroshare.org/resource/ef2d82bf960144b4bfb1bae6242bcc7f\n",
    "\n",
    "    3. Pacific Northwest - Columbia River Basin\n",
    "    Bandaragoda, C. (2017). Sauk Suiattle HydroMeteorology (WRF), HydroShare, http://www.hydroshare.org/resource/0db969e4cfb54cb18b4e1a2014a26c82\n",
    "\n",
    "\n",
    "\n",
    "### Please cite:\n",
    "\n",
    "    1. Livneh B., E.A. Rosenberg, C. Lin, B. Nijssen, V. Mishra, K.M. Andreadis, E.P. Maurer, and D.P. Lettenmaier, 2013: A Long-Term Hydrologically Based Dataset of Land Surface Fluxes and States for the Conterminous United States: Update and Extensions, Journal of Climate, 26, 9384–9392.\n",
    "\n",
    "    2. Livneh B., T.J. Bohn, D.S. Pierce, F. Munoz-Ariola, B. Nijssen, R. Vose, D. Cayan, and L.D. Brekke, 2015: A spatially comprehensive, hydrometeorological data set for Mexico, the U.S., and southern Canada 1950-2013, Nature Scientific Data, 5:150042, doi:10.1038/sdata.2015.42.\n",
    "\n",
    "    3. Salathé, EP, AF Hamlet, CF Mass, M Stumbaugh, S-Y Lee, R Steed: 2017. Estimates of 21st Century Flood Risk in the Pacific Northwest Based on Regional Scale Climate Model Simulations.  J. Hydrometeorology. DOI: 10.1175/JHM-D-13-0137.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all available datasets for Sauk-Suiattle Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_df = pd.read_csv(mappingfile1)\n",
    "# map_df\n",
    "# len(map_df)\n",
    "# map_df.dropna()\n",
    "\n",
    "# ds_pan.loc['48.12695']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maptable, nstations = ogh.mappingfileToDF_PNNL(mappingfile1)\n",
    "maptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialbounds = {'minx':maptable.WE.min().astype(np.int64), 'maxx':maptable.WE.max().astype(np.int64),\n",
    "                 'miny':maptable.SN.min().astype(np.int64), 'maxy':maptable.SN.max().astype(np.int64)}\n",
    "spatialbounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputfiles2 = oxl.get_x_hourlywrf_PNNL2018(homedir=homedir,\n",
    "                                            spatialbounds=spatialbounds,\n",
    "                                            subdir='PNNL2018/Hourly_WRF_2005_2007/noBC',\n",
    "                                            nworkers=40,\n",
    "                                            start_date='2005-10-01',\n",
    "                                            end_date='2007-09-30',\n",
    "                                            file_prefix='sp_',\n",
    "                                            replace_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputfiles2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshoot get_x_hourlywrf_PNNL2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#troublshoot \n",
    "# generate the list of files to download\n",
    "        # modify each month between start_date and end_date to year-month\n",
    "start_date='2005-10-01'\n",
    "end_date='2007-09-30'\n",
    "dates = [x.strftime('%Y%m%d') for x in pd.date_range(start=start_date, end=end_date, freq='D')]\n",
    "filelist = oxl.compile_x_wrfpnnl2018_raw_locations(dates)\n",
    "fileurl = filelist[1]\n",
    "basename = os.path.basename(fileurl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l PNNL2018/Hourly_WRF_2005_2007/noBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xr.open_dataset('PNNL2018/Hourly_WRF_2005_2007/noBC/'+'sp_'+os.path.basename(basename))\n",
    "test2 = xr.open_mfdataset(outputfiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be included into the variable_info meta_file section\n",
    "#dict(test2.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of variables\n",
    "ds_vars = [ds_var for ds_var in dict(test2.variables).keys() \n",
    "           if ds_var not in ['YEAR','MONTH','DAY','TIME','LAT','LON', 'SN','WE']]\n",
    "\n",
    "# convert netcdfs to pandas.Panel API\n",
    "ds_pan = test2.to_dataframe()#[ds_vars]\n",
    "ds_pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ind, eachrow in maptable.iterrows():\n",
    "    \n",
    "    dummy = test2.sel(SN=eachrow['SN'], WE=eachrow['WE']).to_dataframe()\n",
    "    \n",
    "    print(dummy)\n",
    "    \n",
    "    if ind==5:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wget_x_download_spSubset_PNNL(fileurl,\n",
    "#                                   filedate,\n",
    "#                                   spatialbounds,\n",
    "#                                   time_resolution='H',\n",
    "#                                   time_steps=24,\n",
    "#                                   file_prefix='sp_',\n",
    "#                                   rename_timelatlong_names={'south_north':'SN','west_east':'WE'},\n",
    "#                                   replace_file=True):\n",
    "#         \"\"\"\n",
    "#         Download files from an http domain\n",
    "\n",
    "#         fileurl: (str) a urls to request a netcdf file\n",
    "#         spatialbounds: (dict) dict providing the minx, miny, maxx, and maxy of the spatial region\n",
    "#         file_prefix: (str) a string to mark the output file as a spatial subset\n",
    "#         rename_latlong_names: (dict) a dict to standardize latitude/longitude synonyms to LAT/LON, respectively\n",
    "#         replace_file: (logic) If True, the existing file will be replaced; if False, the file download is skipped\n",
    "#         \"\"\"\n",
    "\n",
    "#         # check if the file path already exists; if so, apply replace_file logic\n",
    "#         basename = os.path.basename(fileurl)\n",
    "#         if os.path.isfile(basename):\n",
    "#             os.remove(basename)\n",
    "\n",
    "#         if os.path.isfile(file_prefix+basename) and replace_file:\n",
    "#             os.remove(file_prefix+basename)\n",
    "#         elif os.path.isfile(file_prefix+basename) and not replace_file:\n",
    "#             # replace_file is False; return file path and skip\n",
    "#             return(os.path.join(os.getcwd(), file_prefix+basename))\n",
    "\n",
    "#         # try the file connection\n",
    "#         #print('connecting to: '+basename)\n",
    "#         try:\n",
    "#             ping = urllib.request.urlopen(fileurl)\n",
    "\n",
    "#             # if the file exists, download it\n",
    "#             if ping.getcode() != 404:\n",
    "#                 ping.close()\n",
    "#                 wget.download(fileurl)\n",
    "\n",
    "#                 # open the parent netcdf file\n",
    "#                 ds = xray.open_dataset(basename, engine = 'netcdf4')\n",
    "#                 #print('file read in')\n",
    "#                 # rename latlong if they are not LAT and LON, respectively\n",
    "#                 if not isinstance(rename_timelatlong_names, type(None)):\n",
    "#                     ds = ds.rename(rename_timelatlong_names)\n",
    "#                     #print('renamed columns')\n",
    "\n",
    "#                 # slice by the bounding box NOTE:dataframe slice includes last index\n",
    "#                 ds=ds.assign_coords(SN=ds.SN, WE=ds.WE)\n",
    "#                 spSubset = ds.sel(WE=slice(spatialbounds['minx'], spatialbounds['maxx']),\n",
    "#                                   SN=slice(spatialbounds['miny'], spatialbounds['maxy']))\n",
    "#                 print('cropped')\n",
    "                \n",
    "#                 # change time to datetimeindex\n",
    "#                 hour = [x.strftime('%Y-%m-%d %H:%M:%S') for x in pd.date_range(start=filedate,\n",
    "#                                                                                periods=time_steps,\n",
    "#                                                                                freq=time_resolution)]\n",
    "#                 spSubset['TIME']=pd.DatetimeIndex(hour)\n",
    "                \n",
    "#                 # print the spatial subset\n",
    "#                 spSubset.to_netcdf(file_prefix+basename)\n",
    "#                 print('downloaded: spatial subset of ' + basename)\n",
    "\n",
    "#                 # remove the parent\n",
    "#                 ds.close()\n",
    "#                 os.remove(basename)\n",
    "#                 #print('closed')\n",
    "#                 return(os.path.join(os.getcwd(), file_prefix+basename))\n",
    "\n",
    "#             else:\n",
    "#                 ping.close()\n",
    "#         except:\n",
    "#             print('File does not exist at this URL: ' + basename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_x_hourlywrf_PNNL2018(homedir,\n",
    "#                              spatialbounds,\n",
    "#                              subdir='PNNL2018/Hourly_WRF_1981_2015/SaukSpatialBounds',\n",
    "#                              nworkers=4,\n",
    "#                              start_date='2005-01-01',\n",
    "#                              end_date='2007-12-31',\n",
    "#                              time_resolution='H',\n",
    "#                              time_steps=24,\n",
    "#                              file_prefix='sp_',\n",
    "#                              rename_timelatlong_names={'south_north':'SN','west_east':'WE', 'time':'TIME'},\n",
    "#                              replace_file=True):\n",
    "#     \"\"\"\n",
    "#     get hourly WRF data from a 2018 PNNL WRF run using xarray on netcdf files\n",
    "#     \"\"\"\n",
    "#     # check and generate data directory\n",
    "#     filedir=os.path.join(homedir, subdir)\n",
    "#     ogh.ensure_dir(filedir)\n",
    "    \n",
    "#     # modify each month between start_date and end_date to year-month\n",
    "#     dates = [x.strftime('%Y%m%d') for x in pd.date_range(start=start_date, end=end_date, freq='D')]\n",
    "    \n",
    "#     # initialize parallel workers\n",
    "#     da.set_options(pool=ThreadPool(nworkers))\n",
    "#     ProgressBar().register()\n",
    "    \n",
    "#     # generate the list of files to download\n",
    "#     filelist = compile_x_wrfpnnl2018_raw_locations(dates)\n",
    "    \n",
    "#     # download files of interest\n",
    "#     NetCDFs=[]\n",
    "#     for url, date in zip(filelist, dates):\n",
    "#         NetCDFs.append(da.delayed(wget_x_download_spSubset_PNNL)(fileurl=url,\n",
    "#                                                                  filedate=date,\n",
    "#                                                                  time_resolution=time_resolution,\n",
    "#                                                                  time_steps=time_steps,\n",
    "#                                                                  spatialbounds=spatialbounds,\n",
    "#                                                                  file_prefix=file_prefix,\n",
    "#                                                                  rename_timelatlong_names=rename_timelatlong_names,\n",
    "#                                                                  replace_file=replace_file))\n",
    "    \n",
    "#     # run operations\n",
    "#     outputfiles = da.compute(NetCDFs)[0]\n",
    "    \n",
    "#     # reset working directory\n",
    "#     os.chdir(homedir)\n",
    "#     return(outputfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as da\n",
    "import xarray as xray\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def netcdf_to_ascii(homedir, subdir, netcdfs, mappingfile, catalog_label, meta_file):\n",
    "    # initialize list of dataframe outputs\n",
    "    outfiledict = {}\n",
    "    \n",
    "    # generate destination folder\n",
    "    filedir=os.path.join(homedir, subdir)\n",
    "    ogh.ensure_dir(filedir)\n",
    "\n",
    "    # connect with collection of netcdfs\n",
    "    ds_mf = xray.open_mfdataset(netcdfs, engine = 'netcdf4')\n",
    "\n",
    "    # convert netcdfs to pandas.Panel API\n",
    "    ds_pan = ds_mf.to_dataframe().reset_index('TIME')\n",
    "    \n",
    "    # generate list of variables\n",
    "    ds_vars = [ds_var for ds_var in ds_pan.columns\n",
    "               if ds_var not in ['YEAR','MONTH','DAY','TIME','LAT','LON']]\n",
    "\n",
    "    # read in gridded cells of interest\n",
    "    maptable, nstation = ogh.mappingfileToDF(mappingfile, colvar=None)\n",
    "\n",
    "    # at each latlong of interest\n",
    "    for ind, eachrow in maptable.iterrows():\n",
    "\n",
    "        # generate ASCII time-series\n",
    "        ds_df = ds_pan.loc[eachrow['SN'],eachrow['WE'],:].reset_index(drop=True).loc[:,ds_vars]\n",
    "\n",
    "        # create file name\n",
    "        outfilename = os.path.join(filedir,'data_{0}_{1}'.format(eachrow['LAT'],eachrow['LONG_']))\n",
    "\n",
    "        # save ds_df\n",
    "        outfiledict[outfilename] = da.delayed(ds_df.to_csv)(path_or_buf=outfilename, sep='\\t', header=False, index=False)\n",
    "\n",
    "    # compute ASCII time-series files\n",
    "    ProgressBar().register()\n",
    "    outfiledict = da.compute(outfiledict)[0]\n",
    "    \n",
    "    # update metadata file\n",
    "    meta_file[catalog_label]['variable_info'].update(dict(ds_mf.attrs))\n",
    "    meta_file[catalog_label]['variable_info'].update(dict(ds_mf.variables))\n",
    "    meta_file[catalog_label]['variable_list']=np.array(ds_vars)\n",
    "    \n",
    "    # catalog the output files\n",
    "    ogh.addCatalogToMap(outfilepath=mappingfile, maptable=maptable, folderpath=filedir, catalog_label=catalog_label)\n",
    "    os.chdir(homedir)\n",
    "    return(list(outfiledict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pan = test2.to_dataframe().reset_index('TIME')\n",
    "ds_pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vars = [ds_var for ds_var in ds_pan.columns\n",
    "           if ds_var not in ['YEAR','MONTH','DAY','TIME','LAT','LON']]\n",
    "ds_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(start=meta_file['hourlywrf_pnnl']['start_date'],\n",
    "              end='2015-12-31 23:00:00',\n",
    "              freq=meta_file['hourlywrf_pnnl']['temporal_resolution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file['hourlywrf_pnnl']['variable_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert netcdfs to pandas.Panel API\n",
    "# ds_pan = test2.to_dataframe()[ds_vars].reset_index('TIME')\n",
    "# print(ds_pan)\n",
    "\n",
    "# for ind, eachrow in maptable.iterrows():\n",
    "\n",
    "#         # generate ASCII time-series\n",
    "#         ds_df = ds_pan.loc[eachrow['SN'],eachrow['WE'],:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "outputlist = netcdf_to_ascii(homedir=homedir,\n",
    "                             subdir='PNNL2018/Hourly_WRF_2005_2007/ASCII',\n",
    "                             netcdfs=outputfiles2,\n",
    "                             mappingfile=mappingfile1,\n",
    "                             catalog_label='hourlywrf_pnnl',\n",
    "                             meta_file=meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file['hourlywrf_pnnl']['variable_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maptable, nstation = ogh.mappingfileToDF(mappingfile1,colvar=None)\n",
    "maptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_table(maptable['hourlywrf_pnnl'][0], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('PNNL2018/Hourly_WRF_2005_2007/noBC/'+'sp_'+os.path.basename(basename), engine = 'netcdf4')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour = [x.strftime('%Y-%m-%d %H:%M:%S') for x in pd.date_range(start='2005-10-02', periods=24, freq='H')]\n",
    "# ds['TIME'] = pd.DatetimeIndex(hour)\n",
    "# ds.TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END TROUBLESHOOTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Concatenate netcdf files into one netCDF file and one dataframe with lat,long,time indices : Use xarray function open_mfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir='/PNNL2018/Hourly_WRF_2005_2007/noBC'\n",
    "\n",
    "os.chdir(homedir+subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download Lat and Lon netcdf file, convert to dataframe\n",
    "LtLnEl = xr.open_dataset(('data_LatLonGht.nc'), engine = 'netcdf4')\n",
    "LtLnEl_df = LtLnEl.to_dataframe()\n",
    "LtLnEl_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NetCDF files(range of dates) to concatenate into a single dataframe\n",
    "start_date = '2005-10-01'\n",
    "end_date = '2005-12-30'\n",
    "dates = [x.strftime('%Y-%m-%d') for x in pd.date_range(start=start_date, end=end_date, freq='D')]\n",
    "  \n",
    "#for each day (file) open pnnl netcdf file as xarray dataset, convert all \n",
    "#variables to single dataframe, change indexes to corresponding LAT,LON,TIME values, \n",
    "#append to dataframe\n",
    "ds_pan = pd.DataFrame()\n",
    "ds_pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date=dates[0]\n",
    "#open netcdf file\n",
    "pnnl = xr.open_dataset('sp_data.' + date +'.nc')\n",
    "pnnl_df = pnnl.to_dataframe() #index: SN, TIME, WE\n",
    "pnnl_df = pnnl_df.reset_index(['TIME'])#.sort_values(['SN','WE','TIME'])\n",
    "\n",
    "pnnl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialbounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnnl=pnnl.assign_coords(SN=pnnl.SN, WE=pnnl.WE)\n",
    "pnnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnnl.sel(SN=slice(4,6), WE=slice(3,5)).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pnnl.sel(SN=slice(4,6),WE=slice(4,6)).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download Lat and Lon netcdf file, convert to dataframe\n",
    "LtLnEl = xr.open_dataset(('data_LatLonGht.nc'), engine = 'netcdf4')\n",
    "LtLnEl_df = LtLnEl.to_dataframe()#\n",
    "LtLnEl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of dates to add to dataset as TIME index\n",
    "time = pd.date_range(start=date, periods=24, freq='H')\n",
    "tdf = pd.DataFrame(time, columns = ['TIME'])\n",
    "\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Lat and Lon netcdf file, convert to dataframe\n",
    "LtLnEl = xr.open_dataset(('data_LatLonGht.nc'), engine = 'netcdf4')\n",
    "LtLnEl_df = LtLnEl.to_dataframe()#\n",
    "\n",
    "#create LAT and LONG index values from spatialbounds\n",
    "#spatialbounds = {'minx': 49, 'maxx': 57, 'miny': 49, 'maxy': 58} #from notebook usercase 2\n",
    "\n",
    "#NetCDF files(range of dates) to concatenate into a single dataframe\n",
    "start_date = '2005-10-01'\n",
    "end_date = '2005-12-30'\n",
    "dates = [x.strftime('%Y-%m-%d') for x in pd.date_range(start=start_date, end=end_date, freq='D')]\n",
    "  \n",
    "#for each day (file) open pnnl netcdf file as xarray dataset, convert all \n",
    "#variables to single dataframe, change indexes to corresponding LAT,LON,TIME values, \n",
    "#append to dataframe\n",
    "ds_pan = pd.DataFrame()\n",
    "for ini, date in enumerate(dates):\n",
    "    #for each file\n",
    "\n",
    "    #open netcdf file\n",
    "    pnnl = xr.open_dataset('sp_data.' + date +'.nc')\n",
    "    pnnl_df = pnnl.to_dataframe() #index: SN, TIME, WE\n",
    "          \n",
    "    #create series of dates to add to dataset as TIME index\n",
    "    time = pd.date_range(start=date, periods=24, freq='H')\n",
    "    tdf = pd.DataFrame(time, columns = ['TIME'])\n",
    "    \n",
    "\n",
    "\n",
    "    #(1) set index of pnnl dataframe to indices of lat, long only (remove time indice)\n",
    "    # =============================================================================\n",
    "    pnnl_df['index1'] = (pnnl_df.index)\n",
    "    lst1 = []\n",
    "    lst2 = []\n",
    "    lst3 = []\n",
    "    for ind in list(pnnl_df['index1']):\n",
    "        lst1.append(ind[0]+ spatialbounds['miny'])#SN index\n",
    "        lst2.append(ind[2]+ spatialbounds['minx'])#WE index\n",
    "        lst3.append(ind[1])#TIME index\n",
    "    pnnl_df['ind1'] = lst1\n",
    "    pnnl_df['ind2'] = lst2\n",
    "    pnnl_df['ind3'] = lst3\n",
    "  \n",
    "    pnnl_df.set_index(['ind1','ind2'],inplace=True)\n",
    "    \n",
    "    #(2) set index of LtLnEl dataframe to indices lat and lon\n",
    "    # =============================================================================\n",
    "    \n",
    "    LtLnEl_df['index2'] = LtLnEl_df.index\n",
    "     \n",
    "    lst4 = []\n",
    "    lst5 = []\n",
    "    for ind in list(LtLnEl_df['index2']):\n",
    "        lst4.append(ind[0])#LAT index\n",
    "        lst5.append(ind[1])#LON index\n",
    "    # \n",
    "    LtLnEl_df['ind1'] = lst4\n",
    "    LtLnEl_df['ind2'] = lst5\n",
    "   \n",
    "    LtLnEl_df.set_index(['ind1','ind2'],inplace=True)\n",
    "\n",
    "    #(3) Join LAT and LON coordinates to index values to create df3\n",
    "    # ============================================================================\n",
    "    \n",
    "    df3 = pnnl_df.join(LtLnEl_df)\n",
    "    \n",
    "    #(4) set index of df3 to time index and join with dataframe containing values for time indices \n",
    "    # =============================================================================\n",
    "     \n",
    "    df3.set_index(['ind3'],inplace=True)\n",
    "     \n",
    "    final = df3.join(tdf)\n",
    "    \n",
    "    #(5) Append to final dataframe ds_pan \n",
    "    # =============================================================================    \n",
    "    \n",
    "    #set index as ind1, ind2 and ind3\n",
    "    #final.set_index(['LAT','LON','TIME'],inplace=True)\n",
    "    final.set_index(['LAT','LON'],inplace=True)\n",
    "\n",
    "    #append final to ds_pan \n",
    "    ds_pan = pd.concat([ds_pan, final])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lat lon to column data\n",
    "ds_pan['index3'] = ds_pan.index\n",
    "\n",
    "lst6 = []\n",
    "lst7 = []\n",
    "for ind in list(ds_pan['index3']):\n",
    "    lst6.append('%.5f'%(ind[0]))#LAT index\n",
    "    lst7.append('%.5f'%(ind[1]))#LON index\n",
    "# \n",
    "ds_pan['LAT'] = lst6\n",
    "ds_pan['LON'] = lst7\n",
    "\n",
    "# ds_pan.set_index(['LAT','LON','TIME'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_pan.set_index(['LAT','LON','TIME'],inplace=True)\n",
    "ds_pan.set_index(['LAT','LON'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_pan.loc['48.12695']\n",
    "ds_pan.loc['48.126949']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in gridded cells of interest\n",
    "maptable, nstation = ogh.mappingfileToDF(mappingfile1, colvar=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maptable.head()\n",
    "val = '%.5f'%(maptable['LAT'][0])\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pan.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at each latlong of interest\n",
    "for ind, eachrow in maptable.iterrows():\n",
    "\n",
    "    # generate ASCII time-series\n",
    "    ds_df = ds_pan.loc['%.5f'%(eachrow['LAT']),'%.5f'%(eachrow['LONG_']),:]#.reset_index(drop=True, level=[0,1])\n",
    "\n",
    "    # create file name\n",
    "    #outfilename = os.path.join(filedir,'data_{0}_{1}'.format(eachrow['LAT'],eachrow['LONG_']))\n",
    "\n",
    "    # save ds_df\n",
    "    #outfiledict[outfilename] = da.delayed(ds_df.to_csv)(path_or_buf=outfilename, sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive the downloaded data files for collaborative use\n",
    "\n",
    "Create list of files to save to HydroShare. Verify location and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !tar -zcf livneh2013.tar.gz livneh2013\n",
    "# !tar -zcf livneh2015.tar.gz livneh2015\n",
    "# !tar -zcf salathe2014.tar.gz salathe2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the downloaded tar files\n",
    "# climate2013_tar = os.path.join(homedir,'livneh2013.tar.gz')\n",
    "# climate2015_tar = os.path.join(homedir,'livneh2015.tar.gz')\n",
    "# wrf_tar = os.path.join(homedir,'salathe2014.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "# title = 'Downloaded data sets from each study site for each of seven gridded data products'\n",
    "# abstract = 'This resource contains the downloaded data files for each study site and the gridded cell centroids that intersected within the study area. The file availability is described within the watershed_table file, which summarizes each of the three mapping file catalogs.'\n",
    "# keywords = ['Sauk', 'Elwha','Rio Salado','climate','hydromet','watershed'] \n",
    "# rtype = 'genericresource'\n",
    "\n",
    "# ##PROBLEM WITH WRITING TO RESOURCE\n",
    "\n",
    "# # files to migrate ===> NO watershed_table\n",
    "# #files=[mappingfile1, # sauk\n",
    "# #       mappingfile2, # elwha\n",
    "# #       mappingfile3, # riosalado\n",
    "# #       watershed_table, # watershed summary table\n",
    "# #       climate2013_tar, # Livneh et al. 2013 raw MET, bc MET, and VIC\n",
    "# #       climate2015_tar, # Livneh et al. 2015 raw MET, and VIC\n",
    "# #       wrf_tar] # Salathe et al. 2014 raw WRF and bc WRF\n",
    "\n",
    "# # files to migrate\n",
    "# files=[mappingfile1, # sauk\n",
    "#        mappingfile2, # elwha\n",
    "#        mappingfile3, # riosalado\n",
    "#        climate2013_tar, # Livneh et al. 2013 raw MET, bc MET, and VIC\n",
    "#        climate2015_tar, # Livneh et al. 2015 raw MET, and VIC\n",
    "#        wrf_tar] # Salathe et al. 2014 raw WRF and bc WRF\n",
    "\n",
    "# # files to migrate\n",
    "# #files=[climate2013_tar, # Livneh et al. 2013 raw MET, bc MET, and VIC\n",
    "# #       climate2015_tar, # Livneh et al. 2015 raw MET, and VIC\n",
    "# #       wrf_tar] # Salathe et al. 2014 raw WRF and bc WRF\n",
    "\n",
    "\n",
    "# # create the new resource\n",
    "# resource_id = hs.createHydroShareResource(title=title, \n",
    "#                                           abstract=abstract,\n",
    "#                                           keywords=keywords, \n",
    "#                                           resource_type=rtype, \n",
    "#                                           content_files=files, \n",
    "#                                           public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
