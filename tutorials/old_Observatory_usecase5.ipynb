{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Notebook to TreatGeoSelf with gridded climate time-series data sets\n",
    "\n",
    "## (Case study:  the Sauk-Suiattle river watershed, the Elwha river watershed, the Upper Rio Salado watershed)\n",
    "\n",
    "<img src= \"http://www.sauk-suiattle.com/images/Elliott.jpg\"\n",
    "style=\"float:left;width:150px;padding:20px\">   \n",
    "This data is compiled to digitally observe the watersheds, powered by HydroShare. <br />\n",
    "<br />\n",
    "Use this Jupyter Notebook to: <br /> \n",
    "Generate a list of available gridded data points in your area of interest, <br /> \n",
    "Download Livneh daily 1/16 degree gridded climate data, <br /> \n",
    "Download WRF daily 1/16 degree gridded climate data, <br /> \n",
    "Summarize the elevation range and data availability within the watershed areas, <br /> \n",
    "Visualize the elevation gradient within the watershed areas. <br /> \n",
    "\n",
    "<br /> <br /> <br /> <img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\" style=\"float:right;width:120px;padding:20px\">  \n",
    "#### A Watershed Dynamics Model by the Watershed Dynamics Research Group in the Civil and Environmental Engineering Department at the University of Washington "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the python library basemap-data-hires is not installed, please uncomment and run the following lines in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge basemap-data-hires --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import geopandas as gpd\n",
    "from utilities import hydroshare\n",
    "import ogh\n",
    "\n",
    "# plotting and shape libraries\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # spatial plotting\n",
    "# import fiona\n",
    "# import shapely.ops\n",
    "# from shapely.geometry import MultiPolygon, shape, point, box, Polygon\n",
    "# from descartes import PolygonPatch\n",
    "# from matplotlib.collections import PatchCollection\n",
    "# from mpl_toolkits.basemap import Basemap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dailymet_bclivneh2013',\n",
       " 'dailymet_livneh2013',\n",
       " 'dailymet_livneh2015',\n",
       " 'dailyvic_livneh2013',\n",
       " 'dailyvic_livneh2015',\n",
       " 'dailywrf_bcsalathe2014',\n",
       " 'dailywrf_salathe2014']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_steps',\n",
       " 'delimiter',\n",
       " 'domain',\n",
       " 'end_date',\n",
       " 'file_format',\n",
       " 'filename_structure',\n",
       " 'reference',\n",
       " 'spatial_resolution',\n",
       " 'start_date',\n",
       " 'subdomain',\n",
       " 'temporal_resolution',\n",
       " 'variable_info',\n",
       " 'variable_list',\n",
       " 'web_protocol']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(meta_file['dailymet_livneh2013'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following system variables:\n",
      "   HS_USR_NAME = jphuong\n",
      "   HS_RES_ID = 7c3416535ab24d4f93b0b94741bb9572\n",
      "   HS_RES_TYPE = compositeresource\n",
      "   JUPYTER_HUB_IP = jupyter.cuahsi.org\n",
      "\n",
      "These can be accessed using the following command: \n",
      "   os.environ[key]\n",
      "\n",
      "   (e.g.)\n",
      "   os.environ[\"HS_USR_NAME\"]  => jphuong\n",
      "Successfully established a connection with HydroShare\n",
      "Data will be loaded from and save to:/home/jovyan/work/Observatory-1/ogh\n"
     ]
    }
   ],
   "source": [
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "\n",
    "print('Data will be loaded from and save to:'+homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of gridded climate points for the watershed\n",
    "\n",
    "This example uses a shapefile with the watershed boundary of the Sauk-Suiattle Basin, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. \n",
    "\n",
    "The data for our processing routines can be retrieved using the getResourceFromHydroShare function by passing in the global identifier from the url above.  In the next cell, we download this resource from HydroShare, and identify that the points in this resource are available for downloading gridded hydrometeorology data, based on the point shapefile at https://www.hydroshare.org/resource/ef2d82bf960144b4bfb1bae6242bcc7f/, which is for the extent of North America and includes the average elevation for each 1/16 degree grid cell.  The file must include columns with station numbers, latitude, longitude, and elevation. The header of these columns must be FID, LAT, LONG_, and ELEV or RASTERVALU, respectively. The station numbers will be used for the remainder of the code to uniquely reference data from each climate station, as well as to identify minimum, maximum, and average elevation of all of the climate stations.  The webserice is currently set to a URL for the smallest geographic overlapping extent - e.g. WRF for Columbia River Basin (to use a limit using data from a FTP service, treatgeoself() would need to be edited in observatory_gridded_hydrometeorology utility). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "Would you like to overwrite this data [Y/n]? y\n",
      "Download Finished                               \n",
      "Successfully downloaded resource ef2d82bf960144b4bfb1bae6242bcc7f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "NAmer_dem_list.cpg<br>NAmer_dem_list.dbf<br>NAmer_dem_list.prj<br>NAmer_dem_list.sbn<br>NAmer_dem_list.sbx<br>NAmer_dem_list.shp<br>NAmer_dem_list.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "Would you like to overwrite this data [Y/n]? y\n",
      "Download Finished                               \n",
      "Successfully downloaded resource c532e0578e974201a0bc40a37ef2d284\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wbdhuc12_17110006_WGS84.cpg<br>wbdhuc12_17110006_WGS84.dbf<br>wbdhuc12_17110006_WGS84.prj<br>wbdhuc12_17110006_WGS84.sbn<br>wbdhuc12_17110006_WGS84.sbx<br>wbdhuc12_17110006_WGS84.shp<br>wbdhuc12_17110006_WGS84.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "Would you like to overwrite this data [Y/n]? y\n",
      "Download Finished                               \n",
      "Successfully downloaded resource 4aff8b10bc424250b3d7bac2188391e8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "elwha_ws_bnd_wgs84.cpg<br>elwha_ws_bnd_wgs84.dbf<br>elwha_ws_bnd_wgs84.prj<br>elwha_ws_bnd_wgs84.sbn<br>elwha_ws_bnd_wgs84.sbx<br>elwha_ws_bnd_wgs84.shp<br>elwha_ws_bnd_wgs84.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resource already exists in your userspace.\n",
      "Would you like to overwrite this data [Y/n]? y\n",
      "Download Finished                               \n",
      "Successfully downloaded resource 5c041d95ceb64dce8eb85d2a7db88ed7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Found the following file(s) associated with this HydroShare resource.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "UpperRioSalado_delineatedBoundary.cpg<br>UpperRioSalado_delineatedBoundary.dbf<br>UpperRioSalado_delineatedBoundary.prj<br>UpperRioSalado_delineatedBoundary.sbn<br>UpperRioSalado_delineatedBoundary.sbx<br>UpperRioSalado_delineatedBoundary.shp<br>UpperRioSalado_delineatedBoundary.shp.xml<br>UpperRioSalado_delineatedBoundary.shx"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "These files are stored in a dictionary called <b>hs.content</b> for your convenience.  To access a file, simply issue the following command where MY_FILE is one of the files listed above: <pre>hs.content[\"MY_FILE\"] </pre> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhuc12_17110006_WGS84.shp']\n",
    "\n",
    "\"\"\"\n",
    "Elwha\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('4aff8b10bc424250b3d7bac2188391e8')\n",
    "elwha = hs.content[\"elwha_ws_bnd_wgs84.shp\"]\n",
    "\n",
    "\"\"\"\n",
    "Rio Salado\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('5c041d95ceb64dce8eb85d2a7db88ed7')\n",
    "riosalado = hs.content['UpperRioSalado_delineatedBoundary.shp']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingfile1 = os.path.join(homedir,'Sauk_mappingfile.csv')\n",
    "mappingfile2 = os.path.join(homedir,'Elwha_mappingfile.csv')\n",
    "mappingfile3 = os.path.join(homedir,'RioSalado_mappingfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Watershed</th>\n",
       "      <th>Sauk-Suiattle river</th>\n",
       "      <th>Elwha river</th>\n",
       "      <th>Upper Rio Salado</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median elevation in meters [range](No. gridded cells)</th>\n",
       "      <th>1182[164-2216] (n=98)</th>\n",
       "      <th>1120[36-1642] (n=55)</th>\n",
       "      <th>2308[1962-2669] (n=31)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dailymet_bclivneh2013</th>\n",
       "      <td>1182[164-2216] (n=98)</td>\n",
       "      <td>1120[36-1642] (n=55)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymet_livneh2013</th>\n",
       "      <td>1182[164-2216] (n=98)</td>\n",
       "      <td>1146[174-1642] (n=52)</td>\n",
       "      <td>2308[1962-2669] (n=31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymet_livneh2015</th>\n",
       "      <td>1182[164-2216] (n=98)</td>\n",
       "      <td>1120[36-1642] (n=55)</td>\n",
       "      <td>2308[1962-2669] (n=31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailyvic_livneh2013</th>\n",
       "      <td>1182[164-2216] (n=98)</td>\n",
       "      <td>1146[174-1642] (n=52)</td>\n",
       "      <td>2308[1962-2669] (n=31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailyvic_livneh2015</th>\n",
       "      <td>1182[164-2216] (n=98)</td>\n",
       "      <td>1120[36-1642] (n=55)</td>\n",
       "      <td>2308[1962-2669] (n=31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailywrf_bcsalathe2014</th>\n",
       "      <td>1182[164-2216] (n=98)</td>\n",
       "      <td>1142[97-1642] (n=53)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailywrf_salathe2014</th>\n",
       "      <td>1182[164-2216] (n=98)</td>\n",
       "      <td>1142[97-1642] (n=53)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Watershed                                                Sauk-Suiattle river  \\\n",
       "Median elevation in meters [range](No. gridded cells)  1182[164-2216] (n=98)   \n",
       "dailymet_bclivneh2013                                  1182[164-2216] (n=98)   \n",
       "dailymet_livneh2013                                    1182[164-2216] (n=98)   \n",
       "dailymet_livneh2015                                    1182[164-2216] (n=98)   \n",
       "dailyvic_livneh2013                                    1182[164-2216] (n=98)   \n",
       "dailyvic_livneh2015                                    1182[164-2216] (n=98)   \n",
       "dailywrf_bcsalathe2014                                 1182[164-2216] (n=98)   \n",
       "dailywrf_salathe2014                                   1182[164-2216] (n=98)   \n",
       "\n",
       "Watershed                                                        Elwha river  \\\n",
       "Median elevation in meters [range](No. gridded cells)   1120[36-1642] (n=55)   \n",
       "dailymet_bclivneh2013                                   1120[36-1642] (n=55)   \n",
       "dailymet_livneh2013                                    1146[174-1642] (n=52)   \n",
       "dailymet_livneh2015                                     1120[36-1642] (n=55)   \n",
       "dailyvic_livneh2013                                    1146[174-1642] (n=52)   \n",
       "dailyvic_livneh2015                                     1120[36-1642] (n=55)   \n",
       "dailywrf_bcsalathe2014                                  1142[97-1642] (n=53)   \n",
       "dailywrf_salathe2014                                    1142[97-1642] (n=53)   \n",
       "\n",
       "Watershed                                                    Upper Rio Salado  \n",
       "Median elevation in meters [range](No. gridded cells)  2308[1962-2669] (n=31)  \n",
       "dailymet_bclivneh2013                                                       0  \n",
       "dailymet_livneh2013                                    2308[1962-2669] (n=31)  \n",
       "dailymet_livneh2015                                    2308[1962-2669] (n=31)  \n",
       "dailyvic_livneh2013                                    2308[1962-2669] (n=31)  \n",
       "dailyvic_livneh2015                                    2308[1962-2669] (n=31)  \n",
       "dailywrf_bcsalathe2014                                                      0  \n",
       "dailywrf_salathe2014                                                        0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1, mappingfile2, mappingfile3], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river','Elwha river','Upper Rio Salado'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the subset time-period shared between two gridded data products of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1950-01-01', '2010-12-31')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Livneh et al., 2013\n",
    "dr1 = meta_file['dailymet_livneh2013']\n",
    "\n",
    "# Salathe et al., 2014\n",
    "dr2 = meta_file['dailywrf_salathe2014']\n",
    "\n",
    "# define overlapping time window\n",
    "dr = ogh.overlappingDates(date_set1=tuple([dr1['start_date'], dr1['end_date']]), \n",
    "                          date_set2=tuple([dr2['start_date'], dr2['end_date']]))\n",
    "dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read in time-series data files for Sauk-Suiattle, and compute Spatial-temporal statistics\n",
    "\n",
    "### Do this for Livneh et al., 2013 daily meteorology data and Salathe et al., 2014 WRF-NNRP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gridclim_dict in module ogh:\n",
      "\n",
      "gridclim_dict(mappingfile, dataset, gridclimname=None, metadata=None, min_elev=None, max_elev=None, file_start_date=None, file_end_date=None, file_time_step=None, file_colnames=None, file_delimiter=None, subset_start_date=None, subset_end_date=None, df_dict=None, colvar='all')\n",
      "    # pipelined operation for assimilating data, processing it, and standardizing the plotting\n",
      "    \n",
      "    mappingfile: (dir) the path directory to the mappingfile\n",
      "    dataset: (str) the name of the dataset within mappingfile to use\n",
      "    gridclimname: (str) the suffix for the dataset to be named; if None is provided, default to the dataset name\n",
      "    metadata: (str) the dictionary that contains the metadata explanations; default is None\n",
      "    min_elev: (float) the minimum elevation criteria; default is None\n",
      "    max_elev: (float) the maximum elevation criteria; default is None\n",
      "    file_start_date: (date) the start date of the files that will be read-in; default is None\n",
      "    file_end_date: (date) the end date for the files that will be read in; default is None\n",
      "    file_time_step: (str) the timedelta code that represents the difference between time points; default is 'D' (daily)    \n",
      "    file_colnames: (list) the list of shorthand variables; default is None\n",
      "    file_delimiter: (str) a file parsing character to be used for file reading\n",
      "    subset_start_date: (date) the start date of a date range of interest\n",
      "    subset_end_date: (date) the end date of a date range of interest\n",
      "    df_dict: (dict) an existing dictionary where new computations will be stored\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ogh.gridclim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FID       LAT      LONG_    ELEV  \\\n",
      "0    0  48.53125 -121.59375  1113.0   \n",
      "1    1  48.46875 -121.46875   646.0   \n",
      "2    2  48.46875 -121.53125   321.0   \n",
      "3    3  48.46875 -121.59375   164.0   \n",
      "4    4  48.46875 -121.65625   369.0   \n",
      "\n",
      "                                 dailymet_livneh2013  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "\n",
      "                               dailymet_bclivneh2013  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "\n",
      "                                 dailymet_livneh2015  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "\n",
      "                                 dailyvic_livneh2013  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "\n",
      "                                 dailyvic_livneh2015  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "\n",
      "                                dailywrf_salathe2014  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "\n",
      "                              dailywrf_bcsalathe2014  \n",
      "0  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "1  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "2  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "3  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "4  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "Number of gridded data files:98\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1162.030612244898m\n",
      "Maximum elevation: 2216.0m\n",
      "Number of data files within elevation range (164.0:2216.0): 98\n",
      "PRECIP dataframe reading to start: 0:00:00.178103\n",
      "PRECIP dataframe complete:0:00:01.327330\n",
      "TMAX dataframe reading to start: 0:00:01.504994\n",
      "TMAX dataframe complete:0:00:02.648546\n",
      "TMIN dataframe reading to start: 0:00:02.822284\n",
      "TMIN dataframe complete:0:00:03.980111\n",
      "WINDSPD dataframe reading to start: 0:00:04.173351\n",
      "WINDSPD dataframe complete:0:00:05.328856\n",
      "PRECIP_dailymet_livneh2013 calculations completed in 0:00:00.084295\n",
      "TMAX_dailymet_livneh2013 calculations completed in 0:00:00.079968\n",
      "TMIN_dailymet_livneh2013 calculations completed in 0:00:00.080613\n",
      "WINDSPD_dailymet_livneh2013 calculations completed in 0:00:00.078218\n",
      "   FID       LAT      LONG_    ELEV  \\\n",
      "0    0  48.53125 -121.59375  1113.0   \n",
      "1    1  48.46875 -121.46875   646.0   \n",
      "2    2  48.46875 -121.53125   321.0   \n",
      "3    3  48.46875 -121.59375   164.0   \n",
      "4    4  48.46875 -121.65625   369.0   \n",
      "\n",
      "                                 dailymet_livneh2013  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "\n",
      "                               dailymet_bclivneh2013  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "\n",
      "                                 dailymet_livneh2015  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "\n",
      "                                 dailyvic_livneh2013  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2013...   \n",
      "\n",
      "                                 dailyvic_livneh2015  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/livneh2015...   \n",
      "\n",
      "                                dailywrf_salathe2014  \\\n",
      "0  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "1  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "2  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "3  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "4  /home/jovyan/work/Observatory-1/ogh/salathe201...   \n",
      "\n",
      "                              dailywrf_bcsalathe2014  \n",
      "0  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "1  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "2  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "3  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "4  /home/jovyan/work/Observatory-1/ogh/salathe201...  \n",
      "Number of gridded data files:98\n",
      "Minimum elevation: 164.0m\n",
      "Mean elevation: 1162.030612244898m\n",
      "Maximum elevation: 2216.0m\n",
      "Number of data files within elevation range (164.0:2216.0): 98\n",
      "PRECIP dataframe reading to start: 0:00:00.179813\n",
      "PRECIP dataframe complete:0:00:01.414828\n",
      "TMAX dataframe reading to start: 0:00:01.590171\n",
      "TMAX dataframe complete:0:00:02.682059\n",
      "TMIN dataframe reading to start: 0:00:02.860118\n",
      "TMIN dataframe complete:0:00:03.981594\n",
      "WINDSPD dataframe reading to start: 0:00:04.158578\n",
      "WINDSPD dataframe complete:0:00:05.245323\n",
      "PRECIP_dailywrf_salathe2014 calculations completed in 0:00:00.086166\n",
      "TMAX_dailywrf_salathe2014 calculations completed in 0:00:00.084619\n",
      "TMIN_dailywrf_salathe2014 calculations completed in 0:00:00.087175\n",
      "WINDSPD_dailywrf_salathe2014 calculations completed in 0:00:00.084543\n"
     ]
    }
   ],
   "source": [
    "#initiate new dictionary with original data\n",
    "\n",
    "# Livneh et al. 2013 daily meteorology\n",
    "ltm_0to3000 = ogh.gridclim_dict(metadata=meta_file,\n",
    "                                mappingfile=mappingfile1,\n",
    "                                dataset='dailymet_livneh2013',\n",
    "                                file_start_date=dr1['start_date'], \n",
    "                                file_end_date=dr1['end_date'], \n",
    "                                subset_start_date=dr[0],\n",
    "                                subset_end_date=dr[1])\n",
    "\n",
    "# Salathe et al. 2014 daily WRF-NNRP\n",
    "ltm_0to3000 = ogh.gridclim_dict(metadata=meta_file,\n",
    "                                mappingfile=mappingfile1,\n",
    "                                dataset='dailywrf_salathe2014',\n",
    "                                file_start_date=dr2['start_date'], \n",
    "                                file_end_date=dr2['end_date'], \n",
    "                                subset_start_date=dr[0],\n",
    "                                subset_end_date=dr[1],\n",
    "                                df_dict=ltm_0to3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRECIP_dailymet_livneh2013',\n",
       " 'PRECIP_dailywrf_salathe2014',\n",
       " 'TMAX_dailymet_livneh2013',\n",
       " 'TMAX_dailywrf_salathe2014',\n",
       " 'TMIN_dailymet_livneh2013',\n",
       " 'TMIN_dailywrf_salathe2014',\n",
       " 'WINDSPD_dailymet_livneh2013',\n",
       " 'WINDSPD_dailywrf_salathe2014',\n",
       " 'anom_year_PRECIP_dailymet_livneh2013',\n",
       " 'anom_year_PRECIP_dailywrf_salathe2014',\n",
       " 'anom_year_TMAX_dailymet_livneh2013',\n",
       " 'anom_year_TMAX_dailywrf_salathe2014',\n",
       " 'anom_year_TMIN_dailymet_livneh2013',\n",
       " 'anom_year_TMIN_dailywrf_salathe2014',\n",
       " 'anom_year_WINDSPD_dailymet_livneh2013',\n",
       " 'anom_year_WINDSPD_dailywrf_salathe2014',\n",
       " 'meanallyear_PRECIP_dailymet_livneh2013',\n",
       " 'meanallyear_PRECIP_dailywrf_salathe2014',\n",
       " 'meanallyear_TMAX_dailymet_livneh2013',\n",
       " 'meanallyear_TMAX_dailywrf_salathe2014',\n",
       " 'meanallyear_TMIN_dailymet_livneh2013',\n",
       " 'meanallyear_TMIN_dailywrf_salathe2014',\n",
       " 'meanallyear_WINDSPD_dailymet_livneh2013',\n",
       " 'meanallyear_WINDSPD_dailywrf_salathe2014',\n",
       " 'meanmonth_PRECIP_dailymet_livneh2013',\n",
       " 'meanmonth_PRECIP_dailywrf_salathe2014',\n",
       " 'meanmonth_TMAX_dailymet_livneh2013',\n",
       " 'meanmonth_TMAX_dailywrf_salathe2014',\n",
       " 'meanmonth_TMIN_dailymet_livneh2013',\n",
       " 'meanmonth_TMIN_dailywrf_salathe2014',\n",
       " 'meanmonth_WINDSPD_dailymet_livneh2013',\n",
       " 'meanmonth_WINDSPD_dailywrf_salathe2014',\n",
       " 'meanyear_PRECIP_dailymet_livneh2013',\n",
       " 'meanyear_PRECIP_dailywrf_salathe2014',\n",
       " 'meanyear_TMAX_dailymet_livneh2013',\n",
       " 'meanyear_TMAX_dailywrf_salathe2014',\n",
       " 'meanyear_TMIN_dailymet_livneh2013',\n",
       " 'meanyear_TMIN_dailywrf_salathe2014',\n",
       " 'meanyear_WINDSPD_dailymet_livneh2013',\n",
       " 'meanyear_WINDSPD_dailywrf_salathe2014',\n",
       " 'month_PRECIP_dailymet_livneh2013',\n",
       " 'month_PRECIP_dailywrf_salathe2014',\n",
       " 'month_TMAX_dailymet_livneh2013',\n",
       " 'month_TMAX_dailywrf_salathe2014',\n",
       " 'month_TMIN_dailymet_livneh2013',\n",
       " 'month_TMIN_dailywrf_salathe2014',\n",
       " 'month_WINDSPD_dailymet_livneh2013',\n",
       " 'month_WINDSPD_dailywrf_salathe2014',\n",
       " 'year_PRECIP_dailymet_livneh2013',\n",
       " 'year_PRECIP_dailywrf_salathe2014',\n",
       " 'year_TMAX_dailymet_livneh2013',\n",
       " 'year_TMAX_dailywrf_salathe2014',\n",
       " 'year_TMIN_dailymet_livneh2013',\n",
       " 'year_TMIN_dailywrf_salathe2014',\n",
       " 'year_WINDSPD_dailymet_livneh2013',\n",
       " 'year_WINDSPD_dailywrf_salathe2014']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the data objects within the dictionary\n",
    "sorted(ltm_0to3000.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare gridded model to point observations\n",
    "\n",
    "### Read in  SNOTEL data - assess available data \n",
    "If you want to plot observed snotel point precipitation or temperature with the gridded climate data, set to 'Y' \n",
    "Give name of Snotel file and name to be used in figure legends. \n",
    "File format: Daily SNOTEL Data Report - Historic - By individual SNOTEL site, standard sensors (https://www.wcc.nrcs.usda.gov/snow/snotel-data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/jovyan/work/Observatory-1/ogh/ThunderBasinSNOTEL.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8afdb72f4790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                          \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSNOTEL_file_use_colsnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                          \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                          header=58)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# generate the start and stop date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/Observatory-1/ogh/ogh.py\u001b[0m in \u001b[0;36mread_daily_snotel\u001b[0;34m(file_name, file_colnames, usecols, delimiter, header)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;31m# read in the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m     \u001b[0mdaily_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;31m# reset the colnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/home/jovyan/work/Observatory-1/ogh/ThunderBasinSNOTEL.txt' does not exist"
     ]
    }
   ],
   "source": [
    "# Sauk\n",
    "SNOTEL_file = os.path.join(homedir,'ThunderBasinSNOTEL.txt')\n",
    "SNOTEL_station_name='Thunder Creek'\n",
    "SNOTEL_file_use_colsnames = ['Date','Air Temperature Maximum (degF)', 'Air Temperature Minimum (degF)','Air Temperature Average (degF)','Precipitation Increment (in)']\n",
    "SNOTEL_station_elev=int(4320/3.281) # meters\n",
    "\n",
    "SNOTEL_obs_daily = ogh.read_daily_snotel(file_name=SNOTEL_file, \n",
    "                                         usecols=SNOTEL_file_use_colsnames,\n",
    "                                         delimiter=',', \n",
    "                                         header=58)\n",
    "\n",
    "# generate the start and stop date\n",
    "SNOTEL_obs_start_date=SNOTEL_obs_daily.index[0]\n",
    "SNOTEL_obs_end_date=SNOTEL_obs_daily.index[-1]\n",
    "\n",
    "# peek\n",
    "SNOTEL_obs_daily.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in  COOP station data - assess available data\n",
    "https://www.ncdc.noaa.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "COOP_file=os.path.join(homedir, 'USC00455678.csv') # Sauk\n",
    "COOP_station_name='Mt Vernon'\n",
    "COOP_file_use_colsnames = ['DATE','PRCP','TMAX', 'TMIN','TOBS']\n",
    "COOP_station_elev=int(4.3) # meters\n",
    "\n",
    "COOP_obs_daily = ogh.read_daily_coop(file_name=COOP_file,\n",
    "                                     usecols=COOP_file_use_colsnames,\n",
    "                                     delimiter=',',\n",
    "                                     header=0)\n",
    "\n",
    "# generate the start and stop date\n",
    "COOP_obs_start_date=COOP_obs_daily.index[0]\n",
    "COOP_obs_end_date=COOP_obs_daily.index[-1]\n",
    "\n",
    "# peek\n",
    "COOP_obs_daily.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the mappingfile\n",
    "mappingfile = mappingfile1\n",
    "\n",
    "mapdf = pd.read_csv(mappingfile)\n",
    "\n",
    "# select station by first FID\n",
    "firstStation = ogh.findStationCode(mappingfile=mappingfile, colvar='FID', colvalue=0)\n",
    "\n",
    "# select station by elevation\n",
    "maxElevStation = ogh.findStationCode(mappingfile=mappingfile, colvar='ELEV', colvalue=mapdf.loc[:,'ELEV'].max())\n",
    "medElevStation = ogh.findStationCode(mappingfile=mappingfile, colvar='ELEV', colvalue=mapdf.loc[:,'ELEV'].median())\n",
    "minElevStation = ogh.findStationCode(mappingfile=mappingfile, colvar='ELEV', colvalue=mapdf.loc[:,'ELEV'].min())\n",
    "\n",
    "\n",
    "# print(firstStation, mapdf.iloc[0].ELEV)\n",
    "# print(maxElevStation, mapdf.loc[:,'ELEV'].max())\n",
    "# print(medElevStation, mapdf.loc[:,'ELEV'].median())\n",
    "# print(minElevStation, mapdf.loc[:,'ELEV'].min())\n",
    "\n",
    "# let's compare monthly averages for TMAX using livneh, salathe, and the salathe-corrected livneh\n",
    "comp = ['month_TMAX_dailymet_livneh2013',\n",
    "        'month_TMAX_dailywrf_salathe2014']\n",
    "\n",
    "obj = dict()\n",
    "for eachkey in ltm_0to3000.keys():\n",
    "    if eachkey in comp:\n",
    "        obj[eachkey] = ltm_0to3000[eachkey]\n",
    "\n",
    "panel_obj = pd.Panel.from_dict(obj)\n",
    "panel_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = ['meanmonth_TMAX_dailymet_livneh2013',\n",
    "        'meanmonth_TMAX_dailywrf_salathe2014']\n",
    "\n",
    "obj = dict()\n",
    "for eachkey in ltm_0to3000.keys():\n",
    "    if eachkey in comp:\n",
    "        obj[eachkey] = ltm_0to3000[eachkey]\n",
    "\n",
    "        df_obj = pd.DataFrame.from_dict(obj)\n",
    "df_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_res, var, dataset, pub = each.rsplit('_',3)\n",
    "\n",
    "print(t_res, var, dataset, pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylab_var = meta_file['_'.join([dataset, pub])]['variable_info'][var]['desc']\n",
    "ylab_unit = meta_file['_'.join([dataset, pub])]['variable_info'][var]['units']\n",
    "\n",
    "print('{0} {1} ({2})'.format(t_res, ylab_var, ylab_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comp = [['meanmonth_TMAX_dailymet_livneh2013','meanmonth_TMAX_dailywrf_salathe2014'],\n",
    "        ['meanmonth_PRECIP_dailymet_livneh2013','meanmonth_PRECIP_dailywrf_salathe2014']]\n",
    "wy_numbers=[10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "month_strings=[ 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,5), dpi=500)\n",
    "\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=1)\n",
    "ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=1)\n",
    "\n",
    "\n",
    "# monthly\n",
    "for eachsumm in df_obj.columns:\n",
    "    ax1.plot(df_obj[eachsumm])\n",
    "    \n",
    "\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj[each].index.apply(lambda x: x+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    panel_obj.xs(key=(minElevStation[0][0], minElevStation[0][1], minElevStation[0][2]), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)\n",
    "fig.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    panel_obj.xs(key=(maxElevStation[0][0], maxElevStation[0][1], maxElevStation[0][2]), \n",
    "                 axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up VIC dictionary (as an example)  to compare to available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vic_dr1 = meta_file['dailyvic_livneh2013']['date_range']\n",
    "vic_dr2 = meta_file['dailyvic_livneh2015']['date_range']\n",
    "vic_dr = ogh.overlappingDates(tuple([vic_dr1['start'], vic_dr1['end']]),\n",
    "                              tuple([vic_dr2['start'], vic_dr2['end']]))\n",
    "\n",
    "vic_ltm_3bands = ogh.gridclim_dict(mappingfile=mappingfile,\n",
    "                                   metadata=meta_file,\n",
    "                                   dataset='dailyvic_livneh2013',\n",
    "                                   file_start_date=vic_dr1['start'], \n",
    "                                   file_end_date=vic_dr1['end'],\n",
    "                                   file_time_step=vic_dr1['time_step'],\n",
    "                                   subset_start_date=vic_dr[0],\n",
    "                                   subset_end_date=vic_dr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vic_ltm_3bands.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16956\r\n",
      "-rwxrwx--- 1 root   users 3191412 Mar 26 23:35 Observatory_Sauk_TreatGeoSelf_usecase1.ipynb\r\n",
      "-rwxrwx--- 1 root   users 6241597 Mar 26 23:35 Observatory_Sauk_TreatGeoSelf_usecase2.ipynb\r\n",
      "-rwxrwx--- 1 root   users  395862 Mar 26 23:21 gcGradient_r.png\r\n",
      "-rwxrwx--- 1 root   users  842081 Mar 26 23:21 gcGradient_e.png\r\n",
      "-rwxrwx--- 1 root   users  774657 Mar 26 23:21 gcGradient_s.png\r\n",
      "-rwxrwx--- 1 root   users   16802 Mar 26 22:57 RioSalado_mappingfile.csv\r\n",
      "-rwxrwx--- 1 root   users   43565 Mar 26 22:56 Elwha_mappingfile.csv\r\n",
      "-rwxrwx--- 1 root   users   79503 Mar 26 22:55 Sauk_mappingfile.csv\r\n",
      "-rwxrwx--- 1 root   users  299095 Mar 26 22:49 statemap_annotated.png\r\n",
      "-rw-r--r-- 1 jovyan users  208936 Mar 26 22:40 allwatersheds.shp\r\n",
      "-rw-r--r-- 1 jovyan users     108 Mar 26 22:40 allwatersheds.shx\r\n",
      "-rw-r--r-- 1 jovyan users      78 Mar 26 22:40 allwatersheds.dbf\r\n",
      "-rw-r--r-- 1 jovyan users     143 Mar 26 22:40 allwatersheds.prj\r\n",
      "-rwxrwx--- 1 root   users      10 Mar 26 22:40 allwatersheds.cpg\r\n",
      "-rw-r--r-- 1 jovyan users  209040 Mar 26 22:40 eachwatershed.shp\r\n",
      "-rw-r--r-- 1 jovyan users     124 Mar 26 22:40 eachwatershed.shx\r\n",
      "-rw-r--r-- 1 jovyan users     309 Mar 26 22:40 eachwatershed.dbf\r\n",
      "-rw-r--r-- 1 jovyan users     143 Mar 26 22:40 eachwatershed.prj\r\n",
      "-rwxrwx--- 1 root   users      10 Mar 26 22:40 eachwatershed.cpg\r\n",
      "-rwxrwx--- 1 root   users    7836 Mar 26 22:35 Unit_test_dev.ipynb\r\n",
      "drwxrws--- 2 root   users    4096 Mar 26 22:31 __pycache__\r\n",
      "-rwxrwx--- 1 root   users  100857 Mar 26 22:28 ogh.py\r\n",
      "-rw-r--r-- 1 jovyan users  856025 Mar 26 22:25 Observatory_Sauk_TreatGeoSelf_usecase1-Copy1.ipynb\r\n",
      "-rwxrws--- 1 root   users     114 Mar 24 22:08 mydask.png\r\n",
      "-rwxrws--- 1 root   users  307267 Mar 24 21:58 statemap.png\r\n",
      "-rwxrws--- 1 root   users  642717 Mar 23 00:24 SaukPrecipDec.png\r\n",
      "-rwxrws--- 1 root   users  613426 Mar 23 00:24 SaukPrecipSep.png\r\n",
      "-rwxrws--- 1 root   users  620523 Mar 23 00:23 SaukPrecipJun.png\r\n",
      "-rwxrws--- 1 root   users   82853 Mar 23 00:23 SaukPrecipMar.png\r\n",
      "-rwxrws--- 1 root   users  640888 Mar 23 00:15 SaukPrecipJul.png\r\n",
      "-rwxrws--- 1 root   users  193359 Mar 22 23:18 SaukPrecipApr.png\r\n",
      "-rwxrws--- 1 root   users   54847 Mar 22 23:18 SaukPrecipAug.png\r\n",
      "-rwxrws--- 1 root   users    6247 Mar 22 23:18 SaukPrecipJan.png\r\n",
      "-rwxrws--- 1 root   users   17943 Mar 22 23:18 SaukPrecipNov.png\r\n",
      "-rwxrws--- 1 root   users    6247 Mar 22 23:18 SaukPrecipMay.png\r\n",
      "-rwxrws--- 1 root   users   20891 Mar 22 23:18 SaukPrecipOct.png\r\n",
      "-rwxrws--- 1 root   users  616142 Mar 22 23:18 SaukPrecipFeb.png\r\n",
      "drwxrws--- 4 root   users    4096 Mar 22 17:57 livneh2015\r\n",
      "drwxrws--- 4 root   users    4096 Mar 22 17:57 livneh2013\r\n",
      "drwxrws--- 3 root   users    4096 Mar 22 17:56 salathe2014\r\n",
      "-rwxrws--- 1 root   users   11135 Mar 18 22:19 ogh_meta.json\r\n",
      "-rwxrws--- 1 root   users   18441 Mar 18 22:19 _version.py\r\n",
      "-rwxrws--- 1 root   users     310 Mar 18 22:19 __init__.py\r\n"
     ]
    }
   ],
   "source": [
    "#execute this cell to list the content of the directory\n",
    "!ls -lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive the downloaded data files for collaborative use\n",
    "\n",
    "Create list of files to save to HydroShare. Verify location and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zcf {climate2013_tar} livneh2013\n",
    "!tar -zcf {climate2015_tar} livneh2015\n",
    "!tar -zcf {wrf_tar} salathe2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThisNotebook='Observatory_Sauk_TreatGeoSelf_usecase1.ipynb' #check name for consistency\n",
    "climate2013_tar = 'livneh2013.tar.gz'\n",
    "climate2015_tar = 'livneh2015.tar.gz'\n",
    "wrf_tar = 'salathe2014.tar.gz'\n",
    "mappingfile = 'Sauk_mappingfile.csv'\n",
    "\n",
    "files=[ThisNotebook, mappingfile, climate2013_tar, climate2015_tar, wrf_tar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Results from testing out the TreatGeoSelf utility'\n",
    "abstract = 'This the output from the TreatGeoSelf utility integration notebook.'\n",
    "keywords = ['Sauk', 'Elwha','Rio Salado','climate','hydromet','watershed'] \n",
    "rtype = 'genericresource'\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
